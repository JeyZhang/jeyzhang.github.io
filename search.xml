<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[北京的一天]]></title>
    <url>%2Fone-day-of-beijing.html</url>
    <content type="text"><![CDATA[今天的北京阳光灿烂，风清气爽，气温适宜，好像除了空气中飘着一些烦人的柳絮之外，一切都刚刚好。 索性去了趟玉渊潭公园，活动活动筋骨。通往公园的桥边上站着一群上了年纪的大爷们守着鱼竿，通往公园的路上各色的人群攒动，耳朵里是小贩的叫卖声，协警的指引声，人群里除了各种寒暄声还夹杂着孩童的哭闹声。这种浓烈喧闹的市民生活气息好像已经很久没有感受过了，工作以来的我一到周末就愿意宅家，躺着不想出门，像在学校读书的时候一样，仿佛在一个属于自己的小角落里与世隔离很久。所以那种喧闹咋一来仿佛还挺新鲜，把我从安静的独居拽入一个仿佛真实仿佛也不那么真实的群体环境里。也许人独处久了，就会希望将自己置身于一个群体中，哪怕这个群体对你而言很陌生，甚至无需与之交流。 回来的路上，我看着出租车外面那灿烂阳光照耀下的北京，一条条从未觉得如此干净的街道，旁边一棵棵长出嫩绿的小树，周边一幢幢整齐排列的居民楼，还有远处反光的高楼表面，这些画面组合的那一刻，我仿佛感受到了这座城市从所未有的美，似乎这座城市是带有温情的，是一个你愿意安定下来的地方。 当你感到生活乏味找不到意义时，我建议你挑个天气好的日子出去走走，其实只要自己尝试着放慢自己，细细地去发掘这座城市，你会发现生活其实没有那么无聊糟糕，所在的城市好像也没有那么冰冷无情。我们常在探索生命的意义而未果，而今天的我觉得生命的意义或许就像是树枝上刚长出来的新芽，我们都知道总将有一天它会老化脱落、不复存在，然后周而复始地开始重复着，但此时此刻你看，这些嫩芽就在阳光下闪烁着它的光辉，让人欢欣鼓舞。怀抱着希望并给予身边的人希望，或许就是生命的意义。 现在的我，则希望能凭借自身的努力在这片土地上找到一个归属的角落。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决国内Hexo+Next无法正常使用disqus评论系统的问题]]></title>
    <url>%2Fusing-disqus-in-china.html</url>
    <content type="text"><![CDATA[今天在家里没有连V的情况下，发现博客的评论竟然加载不出来了（我说这么最近半年博文的评论数直线下降（大雾））…搜了一下才知道原来半年前国内就不能正常访问disqus了，因为本站用到了disqus作为评论系统，所以自然是挂了… 真是感叹国内的互联网环境的安(xian)全(e)，看了一下解决方法：首先检测当前访问下能不能正常加载disqus（即在一定时间内有响应）；如果能，那么走正常的流程，调disqus的服务即可；如果不能，那么需要通过一个VPS来调用disqus-api来拿到结果，再返回给前端以展示出来。 立个FLAG，明天抽空来修复一下~（update: 好难… 修了一晚上没修好T_T）]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Comment</tag>
        <tag>Disqus</tag>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018年计划和一些感想]]></title>
    <url>%2Fmy-plans-for-2018.html</url>
    <content type="text"><![CDATA[建立这个博客已经两年多，自己从来还没有在这上面写过关于个人的东西。当初对这个博客的定位是希望能督促自己多写出一些技术类的文章，一来可以一边学习一边总结，二来也给自己留一个底，方便打捞自己曾经学习过的东西，三来获取能够帮助到别人。 今天想要写些关于自己的东西，多年没有动笔写作，才发觉文笔生疏了很多，想起五六年前还在读本科的自己，动不动地就在空间里文笔大攉强说愁，现在想来除了觉得有点可笑和幼稚之外，也多了一些感慨。 实话说，自从工作这一年半以来，我的内心深处一直藏着诸多的迷茫和空洞。不是说工作本身不忙，或者工作内容不够充实，是我发现自从毕业之后，我好像突然没有了一个明确的目标和方向，我每天工作这么忙和加班，但我不知道有什么意义和目的。 一个友人问我，“你最想得到什么？”。我的第一反应是关于钱和物质，我希望能早日财富自由。友人说那你应该能很容易能实现，但财富的积累并不是一件值得当做梦想的事，除了财富之外呢？我竟无言以对。 越长大越发现自己逐渐地开始迷失，以前的自己喜欢想象自己的未来，喜欢那些所谓的正能量和鸡汤文，喜欢给自己设定目标并付诸努力加以实现。但工作以来，我发现现在的自己仿佛在距离那个以前的自己越来越远，内心越来越注重现实，而缺少某种情怀。 昨晚看了一部电影，叫《被嫌弃的松子的一生》，在那些不了解松子（女主）的外人眼中，她的人生如此平庸无聊，甚至是失败和不堪。但通过了解了她的人生才发现，她的人生是多么的轰轰烈烈和敢爱敢恨，至少她那么毫无保留地敢于追逐自己向往的那种爱情过。因为种种的误会、冲动和不幸，她仿佛失去了所有美好的亲情、友情和爱情，在以为人生快要结束的时候顽强地开始，在最终选择重新开始的时候不幸结束，命运有时就是这样富有戏剧性。“生而为人，我很抱歉”，是她最后写给世界的话；但在我看来，如此热烈地活过，远胜于千千万万的那些所谓成功但实则平庸的人生。 我多么希望我也是这样一个勇敢而纯粹的人啊，有自己的情怀和向往并敢于追逐不向现实低头。 看完这部影片，我开始想象自己今后的人生。是啊，以后的自己会活成什么样子，是不是如同小时候那样的幻想，有一个光明的未来，抑或是一段普通平庸的时光而已。后来的我开始不敢想，因为就现在的生活状态而言，我不知道未来有什么光明可言，因为现在的我仿佛没有什么理想和追求，每天浑浑噩噩，犯懒，得过且过，好像只是希望获得那些常人看起来的所谓成功，在北京落户、有房有车。我突然觉得，这不是我内心想要真正追求的东西，我还在25岁，年轻时有更值得追求的东西。 所以，我回顾了之前一些自己的想法和目标。希望列出来，给自己一些提醒，也算是flags吧。公示之也是希望自己会有受到监督的感觉，能有一些压力和动力。 健康和外表： 一周至少运动一次； 多吃多增肌； 按时休息（11:30左右入睡，坚持午休），保持充沛的精力； 坚持护肤和保养，保持年轻的状态； 技术积累和职业规划 坚持1-2周能有一篇技术博文（不在乎字数，在乎学到的东西）； 坚持学习，对技术抱有好奇心和探索精神； 能多从产品和用户体验角度思考技术上的问题，能够drive一件事情往前直至做成为止（老板给的工作建议）； 开始关注transfer的事宜，早做准备（口语上和技术上）； 投入一些精力管理技术群； 财务 坚持学习理财知识，建立一个系统的理财计划，培养自己的风控意识（工作时间之外）； 能建立一个小的理财交流圈； 其他 每月能够抽空看一些文学类书籍，提升自己的文学修养； 每1-2周主动给家里打电话，主动关心家里人； 暂时先设这些计划（太多也怕自己完成不了…），后续需要逐步细化，如果有目标最好能够设定一个时间节点，督促自己完成。 新的一年，走过路过的也来聊聊你的计划和目标？（在评论区写下你的FLAG，让我们相互督促对方~） 愿我们的人生都能【甘于平凡，但不甘于平凡地溃败】。 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分类实战系列（一）：特征工程]]></title>
    <url>%2Ftext-classification-in-action.html</url>
    <content type="text"><![CDATA[本文的话题老生常谈，文本分类应该是很多NLPer非常常遇到和熟悉的任务之一了，下面总结一下博主在处理这类任务的过程中特征工程方面的经验，希望对各位NLP入门者或者在做此类任务的新手有所帮助。对于其他的文本处理任务，也会有一定的参考意义。 概述文本分类，顾名思义，就是根据文本内容本身将文本归为不同的类别，通常是有监督学习的任务。根据文本内容的长短，有做句子、段落或者文章的分类；文本的长短不同可能会导致文本可抽取的特征上的略微差异，但是总体上来说，文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。所以，特征工程就显得非常重要，特征找的好，分类效果也会大幅提高（当然前提是标注数据质量和数量也要合适，数据的好坏决定效果的下限，特征工程决定效果的上限）。 也许会有人问最近的深度学习技术能够避免我们构造特征这件事，为什么还需要特征工程？深度学习并不是万能的，在NLP领域深度学习技术取得的效果有限（毕竟语言是高阶抽象的信息，深度学习在图像、语音这些低阶具体的信息处理上更适合，因为在低阶具体的信息上构造特征是一件费力的事情），并不是否认深度学习在NLP领域上取得的成绩，工业界现在通用的做法都是会把深度学习模型作为系统的一个子模块（也是一维特征），和一些传统的基于统计的自然语言技术的特征，还有一些针对具体任务本身专门设计的特征，一起作为一个或多个模型（也称Ensemble，即模型集成）的输入，最终构成一个文本处理系统。 特征工程那么，对于文本分类任务而言，工业界常用到的特征有哪些呢？下面用一张图以概括： 我主要将这些特征分为四个层次，由下往上，特征由抽象到具体，粒度从细到粗。我们希望能够从不同的角度和纬度来设计特征，以捕捉这些特征和类别之间的关系。下面详细介绍这四个层次上常用到的特征表示。 基于词袋模型的特征表示词袋模型的基本思想是将文本符号化，将一段文本表示成一堆符号的集合；由于中文文本的多样性，通常导致构建的词袋维数较大，仅仅以词为单位（Unigram）构建的词袋可能就达到几万维，如果考虑二元词组（Bigram）、三元词组（Trigram）的话词袋大小可能会有几十万之多，因此基于词袋模型的特征表示通常是极其稀疏的。 词袋模型的one-hot表示示意图如下（假设我们构建了一个2w维的词袋模型，每一维表示一个词）： 从上到下，可以看出几种不同的表示方法： 第1种：Naive版本，不考虑词出现的频率，只要出现过就在相应的位置标1，否则为0； 第2种：考虑词频（即term frequency），认为一段文本中出现越多的词越重要，因此权重也越大； 第3种：考虑词的重要性，以TFIDF表征一个词的重要程度（不了解TFIDF的点这里）。简单来说，TFIDF反映了一种折中的思想：即在一篇文档中，TF认为一个词出现的次数越大可能越重要，但也可能并不是（比如停用词：“的”“是”之类的）；IDF认为一个词出现在的文档数越少越重要，但也可能不是（比如一些无意义的生僻词）。 通常情况下，我们都会采用第3种方法。原因也很直观，文本中所出现的词的重要程度是不太一样的，比如上面的例子中“我”，“喜欢”，“学习”这3个词就要比其他词更为重要。除了TFIDF的表征方法，还有chi-square，互信息（MI），熵等其他一些衡量词重要性的指标（见这里）。但是一般TFIDF用得比较普遍。 经验总结： 通常考虑unigram和bigram来构建词袋模型（trigram的话维数太高，取得的gain也不高）； 用TFIDF时，注意对TF作归一化，通常用词频除以文本的长度； 如果构建的词袋维数太高，可以用TF（或者TFIDF）来卡，将一些不常见的词（会有很多噪音词，如联系方式、邮箱之类的）过滤掉； 如果有一些先验的词袋，word count通常都是比较强的一维特征（比如情感分类中，正负情感词的出现次数），可以考虑； 基于词袋模型构建的特征通常高维但稀疏，通常使用非线性模型取得的效果较线性的要好，推荐大家尝试使用一些基于决策树的boosting模型，如GBDT；这也很好理解，较线性模型而言，非线性模型能够学习出更加复杂的规则，对于文本而言，体现在能够一定程度上考虑词出现的语境（context）情况，比如，对于识别文本是否为骂人语料，文本中出现“妈”，同时也出现“你”，那么为骂人的概率会增大。 词袋模型比较简单直观，它通常能学习出一些关键词和类别之间的映射关系，但是缺点也很明显： 丢失了文本中词出现的先后顺序信息； 仅将词语符号化，没有考虑词之间的语义联系（比如，“麦克风”和“话筒”是不同的词，但是语义是相同的）； 基于embedding的特征表示上一部分介绍了基于词袋模型如何提取文本特征，这主要是从词形的角度考虑的，并没有考虑词语之间的语义关联信息。提到语义关联，大家都会联想到著名的word2vec。word2vec的原理很简单，基本思想是用词出现的上下文来表示这个词，上下文越接近的词之间的语义相似性越高。例如，上一小节中举到的例子，“话筒”和“麦克风”两者的上下文可能非常接近，因此会被认为是语义接近的。（不过语义接近并不代表含义接近，例如“黑色”和“白色”的上下文是相似的，但所代表的含义可能却是相反的）。 目前做word embedding的方法很多，比较流行的有下面两种： word2vec GloVe word2vec和GloVe两者的思想是类似的，都是用词的上下文来表示这个词，但是用的方法不同：word2vec是predict-based，用一个3层的NN模型来预测词的上下文（或者反过来），词向量是训练过程的中间产物；而GloVe则是count-based的方法，通过对共现词矩阵做降维来获取词的向量。两者在效果上相差不大，但GloVe模型的优势在于矩阵运算可以并行化，这样训练速度能加快。具体两者的差别可以参考Quora上的回答。 有了word embedding之后，我们怎么得到文本的embedding呢？ 对于短文本而言，比较好的方法有： (1) 取短文本的各个词向量之和（或者取平均）作为文本的向量表示； (2) 用一个pre-train好的NN model得到文本作为输入的最后一层向量表示； 除此之外，还有TwitterLda，TwitterLda是Lda的简化版本，针对短文本做主题刻画，实际效果也还不错。 基于embedding的特征刻画的是语义、主题层面上的特征，较词匹配而言，有一定的泛化能力。 基于NN Model抽取的特征NN的好处在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。CNN和RNN都是NLP中常用的模型，两个模型捕捉特征的角度也不太一样，CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力，两者都可以用在文本分类任务中，而且效果都不错。 对于简单的文本分类任务，用几个简单的NN模型基本就够了（调参数也是一大累活儿）。网上有很多关于NN的实现，这里推荐一个TensorFlow的实现版本，里面有一个浅层的CNN和RNN实现（word-based和chat-based都有），代码也很好懂，可以快速实验验证效果。地址在这里。 最后我们可以将这些NNs预测的分值作为我们分类系统的一个特征，来加强分类系统的性能。 基于任务本身抽取的特征这一部分的特征主要是针对具体任务而设计的，通过我们对数据的观察和感知，也许能够发现一些可能有用的特征。有时候，这些手工特征对最后的分类效果提升很大。举个例子，比如对于正负面评论分类任务，对于负面评论，包含负面词的数量就是一维很强的特征。 这部分的特征设计就是在拼脑力和拼经验，建议可以多看看各个类别数据找找感觉，将那些你直观上感觉对分类有帮助的东西设计成特征，有时候这些经验主义的东西很有用（可能是模型从数据学习不出来的）。 特征融合在设计完这些特征之后，怎么融合更合适呢？对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如比较流行的GDBT, XGBoost）；对于特征维数较低、数据模式简单的情况，建议用简单的线性模型即可（如LR）。下面分享一个我做特征融合的模型框架，任务是正负面评论分类（负面评论定义是不适合出现在网络上的评论，如政治敏感、带有人身攻击、强烈负面情绪的评论）。 其中，橙色框表示模型，蓝色框表示用到的特征，[]里面表示特征的维数。这里需要注意的是，训练子模型（GBDT/DNN）的训练数据和训练融合模型（LR）的训练数据需要不一样，这也很好理解，就是防止子模型因为“见过”这些训练数据而产生偏向于子模型的情况。实际的模型训练中，可以用training数据集作为子模型的训练数据，dev数据集作为最终融合模型的训练数据。 模型融合能够从多个角度更加全面地学习出训练数据中的模式，往往能比单个模型效果好一点（2~3个点左右）。 另外，通过观察LR模型给各个特征分配的权重大小和正负，我们可以看出对于训练数据而言，这些特征影响分类的重要程度（权重大小（绝对值）），以及特征影响最终分类目标的极性。特别的，我们可以通过观察那些手工特征的权重来验证这些特征的有效性和有效程度。 总结这篇文章主要从宏观上介绍了对于文本分类任务而言设计特征的思路，对于其他的NLP任务，也可以参考类似的方法。总而言之，特征工程的核心是尽量从多个角度和纬度来捕捉数据中的模式，并用数值特征来加以刻画。最近流行的深度学习模型可以end2end地学习数据中隐含的模式，免去了人工提取特征的麻烦，然而对于信息高度抽象的文本数据而言，深度学习模型能取得的效果有限，在实际的产品中，我们往往会加入一些传统的基于统计学习的自然语言技术，以及根据我们对业务和数据的理解而人工设计的特征，来最终实现一个比较优良的结果。 下一篇，我会具体介绍一下文本分类中常用到的模型和算法。请各位坐等更新:) 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/text-classification-in-action.html 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Feature Engineering</tag>
        <tag>NLP</tag>
        <tag>TensorFlow</tag>
        <tag>Text Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解LSTM/RNN中的Attention机制]]></title>
    <url>%2Funderstand-attention-in-rnn.html</url>
    <content type="text"><![CDATA[导读目前采用编码器-解码器 (Encode-Decode) 结构的模型非常热门，是因为它在许多领域较其他的传统模型方法都取得了更好的结果。这种结构的模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，该模型能够学习出对应合理的向量表示。然而，这种模型存在的问题在于：当输入序列非常长时，模型难以学到合理的向量表示。 在这篇博文中，我们将探索加入LSTM/RNN模型中的attention机制是如何克服传统编码器-解码器结构存在的问题的。 通过阅读这篇博文，你将会学习到： 传统编码器-解码器结构存在的问题及如何将输入序列编码成固定的向量表示； Attention机制是如何克服上述问题的，以及在模型输出时是如何考虑输出与输入序列的每一项关系的； 基于attention机制的LSTM/RNN模型的5个应用领域：机器翻译、图片描述、语义蕴涵、语音识别和文本摘要。 让我们开始学习吧。 长输入序列带来的问题使用传统编码器-解码器的RNN模型先用一些LSTM单元来对输入序列进行学习，编码为固定长度的向量表示；然后再用一些LSTM单元来读取这种向量表示并解码为输出序列。 采用这种结构的模型在许多比较难的序列预测问题（如文本翻译）上都取得了最好的结果，因此迅速成为了目前的主流方法。 例如： Sequence to Sequence Learning with Neural Networks, 2014 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014 这种结构在很多其他的领域上也取得了不错的结果。然而，它存在一个问题在于：输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示。 这个问题限制了模型的性能，尤其是当输入序列比较长时，模型的性能会变得很差（在文本翻译任务上表现为待翻译的原始文本长度过长时翻译质量较差）。 “一个潜在的问题是，采用编码器-解码器结构的神经网络模型需要将输入序列中的必要信息表示为一个固定长度的向量，而当输入序列很长时则难以保留全部的必要信息（因为太多），尤其是当输入序列的长度比训练数据集中的更长时。” — Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015 使用attention机制Attention机制的基本思想是，打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。 Attention机制的实现是通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。 换一个角度而言，输出序列中的每一项的生成概率取决于在输入序列中选择了哪些项。 “在文本翻译任务上，使用attention机制的模型每生成一个词时都会在输入序列中找出一个与之最相关的词集合。之后模型根据当前的上下文向量 (context vectors) 和所有之前生成出的词来预测下一个目标词。 … 它将输入序列转化为一堆向量的序列并自适应地从中选择一个子集来解码出目标翻译文本。这感觉上像是用于文本翻译的神经网络模型需要“压缩”输入文本中的所有信息为一个固定长度的向量，不论输入文本的长短。” — Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015 虽然模型使用attention机制之后会增加计算量，但是性能水平能够得到提升。另外，使用attention机制便于理解在模型输出过程中输入序列中的信息是如何影响最后生成序列的。这有助于我们更好地理解模型的内部运作机制以及对一些特定的输入-输出进行debug。 “论文提出的方法能够直观地观察到生成序列中的每个词与输入序列中一些词的对齐关系，这可以通过对标注 (annotations) 权重参数可视化来实现…每个图中矩阵的每一行表示与标注相关联的权重。由此我们可以看出在生成目标词时，源句子中的位置信息会被认为更重要。” — Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015 大型图片带来的问题被广泛应用于计算机视觉领域的卷积神经网络模型同样存在类似的问题： 对于特别大的图片输入，模型学习起来比较困难。 由此，一种启发式的方法是将在模型做预测之前先对大型图片进行某种近似的表示。 “人类的感知有一个重要的特性是不会立即处理外界的全部输入，相反的，人类会将注意力专注于所选择的部分来得到所需要的信息，然后结合不同时间段的局部信息来建立一个内部的场景表示，从而引导眼球的移动及做出决策。” — Recurrent Models of Visual Attention, 2014 这种启发式方法某种程度上也可以认为是考虑了attention，但在这篇博文中，这种方法并不认为是基于attention机制的。 基于attention机制的相关论文如下： Recurrent Models of Visual Attention, 2014 DRAW: A Recurrent Neural Network For Image Generation, 2014 Multiple Object Recognition with Visual Attention, 2014 基于attention模型的应用实例这部分将列举几个具体的应用实例，介绍attention机制是如何用在LSTM/RNN模型来进行序列预测的。 1. Attention在文本翻译任务上的应用文本翻译这个实例在前面已经提过了。 给定一个法语的句子作为输入序列，需要输出翻译为英语的句子。Attention机制被用在输出输出序列中的每个词时会专注考虑输入序列中的一些被认为比较重要的词。 我们对原始的编码器-解码器模型进行了改进，使其有一个模型来对输入内容进行搜索，也就是说在生成目标词时会有一个编码器来做这个事情。这打破了之前的模型是基于将整个输入序列强行编码为一个固定长度向量的限制，同时也让模型在生成下一个目标词时重点考虑输入中相关的信息。 — Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015 Attention在文本翻译任务（输入为法语文本序列，输出为英语文本序列）上的可视化（图片来源于Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015） 2. Attention在图片描述上的应用与之前启发式方法不同的是，基于序列生成的attention机制可以应用在计算机视觉相关的任务上，帮助卷积神经网络重点关注图片的一些局部信息来生成相应的序列，典型的任务就是对一张图片进行文本描述。 给定一张图片作为输入，输出对应的英文文本描述。Attention机制被用在输出输出序列的每个词时会专注考虑图片中不同的局部信息。 我们提出了一种基于attention的方法，该方法在3个标准数据集上都取得了最佳的结果……同时展现了attention机制能够更好地帮助我们理解模型地生成过程，模型学习到的对齐关系与人类的直观认知非常的接近（如下图）。 — Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2016 Attention在图片描述任务（输入为图片，输出为描述的文本）上的可视化（图片来源于Attend and Tell: Neural Image Caption Generation with Visual Attention, 2016） 3. Attention在语义蕴涵 (Entailment) 中的应用给定一个用英文描述的前提和假设作为输入，输出假设与前提是否矛盾、是否相关或者是否成立。 举个例子： 前提：在一个婚礼派对上拍照 假设：有人结婚了 该例子中的假设是成立的。 Attention机制被用于关联假设和前提描述文本之间词与词的关系。 我们提出了一种基于LSTM的神经网络模型，和把每个输入文本都独立编码为一个语义向量的模型不同的是，该模型同时读取前提和假设两个描述的文本序列并判断假设是否成立。我们在模型中加入了attention机制来找出假设和前提文本中词/短语之间的对齐关系。……加入attention机制能够使模型在实验结果上有2.6个点的提升，这是目前数据集上取得的最好结果… — Reasoning about Entailment with Neural Attention, 2016 Attention在语义蕴涵任务（输入是前提文本，输出是假设文本）上的可视化（图片来源于Reasoning about Entailment with Neural Attention, 2016） 4. Attention在语音识别上的应用给定一个英文的语音片段作为输入，输出对应的音素序列。 Attention机制被用于对输出序列的每个音素和输入语音序列中一些特定帧进行关联。 …一种基于attention机制的端到端可训练的语音识别模型，能够结合文本内容和位置信息来选择输入序列中下一个进行编码的位置。该模型有一个优点是能够识别长度比训练数据长得多的语音输入。 — Attention-Based Models for Speech Recognition, 2015. Attention在语音识别任务（输入是音帧，输出是音素的位置）上的可视化（图片来源于Attention-Based Models for Speech Recognition, 2015） 5. Attention在文本摘要上的应用给定一篇英文文章作为输入序列，输出一个对应的摘要序列。 Attention机制被用于关联输出摘要中的每个词和输入中的一些特定词。 … 在最近神经网络翻译模型的发展基础之上，提出了一个用于生成摘要任务的基于attention的神经网络模型。通过将这个概率模型与一个生成式方法相结合来生成出准确的摘要。 — A Neural Attention Model for Abstractive Sentence Summarization, 2015 Attention在文本摘要任务（输入为文章，输出为文本摘要）上的可视化（图片来源于A Neural Attention Model for Abstractive Sentence Summarization, 2015） 进一步的阅读如果你想进一步地学习如何在LSTM/RNN模型中加入attention机制，可阅读以下论文： Attention and memory in deep learning and NLP Attention Mechanism Survey on Attention-based Models Applied in NLP What is exactly the attention mechanism introduced to RNN? （来自Quora） What is Attention Mechanism in Neural Networks? 目前Keras官方还没有单独将attention模型的代码开源，下面有一些第三方的实现： Deep Language Modeling for Question Answering using Keras Attention Model Available! Keras Attention Mechanism Attention and Augmented Recurrent Neural Networks How to add Attention on top of a Recurrent Layer (Text Classification) Attention Mechanism Implementation Issue Implementing simple neural attention model (for padded inputs) Attention layer requires another PR seq2seq library 总结通过这篇博文，你应该学习到了attention机制是如何应用在LSTM/RNN模型中来解决序列预测存在的问题。 具体而言，采用传统编码器-解码器结构的LSTM/RNN模型存在一个问题：不论输入长短都将其编码成一个固定长度的向量表示，这使模型对于长输入序列的学习效果很差（解码效果很差）。而attention机制则克服了上述问题，原理是在模型输出时会选择性地专注考虑输入中的对应相关的信息。使用attention机制的方法被广泛应用在各种序列预测任务上，包括文本翻译、语音识别等。 本文结束，感谢欣赏。 感谢原作者Jason Brownlee。原文链接见：Attention in Long Short-Term Memory Recurrent Neural Networks 同时，本译文稍作修改被刊登在公众号：AI科技大本营上，文章链接见一文读懂Attention：Facebook曾拿CNN秒杀谷歌，现如今谷歌拿它秒杀所有人。欢迎关注这个公众号（微信搜索rgznai100），上面有较多机器学习/深度学习相关的资源（尽管文章稍有些标题党=。=）。 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Deep Learning</tag>
        <tag>LSTM</tag>
        <tag>Machine Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人中的深度学习技术之二：基于检索模型的实现]]></title>
    <url>%2Fdeep-learning-for-chatbots-2.html</url>
    <content type="text"><![CDATA[上篇博文：聊天机器人中的深度学习技术之一：导读主要从宏观上对目前聊天机器人所用到的主要技术进行了介绍。这篇博文会介绍并实现一个基于检索的模型，使用了双层Decoder的LSTM模型，通过这个模型可以实现聊天机器人。 本文涉及到的数据和代码见Github仓库地址。 基于检索模型的聊天机器人本文我们将介绍和实现一个基于检索模型的聊天机器人。检索模型所使用的回复数据通常是预先存储且知道（或定义）的数据，而不像生成式模型那样可以创造出崭新的、未知的回复内容（模型没有见过）。准确来讲，检索式模型的输入是一段上下文内容 C (会话到目前未知的内容信息) 和一个可能作为回复的候选答案；模型的输出是对这个候选答案的打分。寻找最合适的回复内容的过程是：先对一堆候选答案进行打分及排序，最后选出分值最高的那个最为回复。 也许你会质疑为什么不直接使用生成式模型，生成式模型不需要预先存储且定义好的数据，比起检索模型更加的灵活多变。原因在于目前生成式模型的效果并不佳，由于生成式模型的约束条件少，过于多变的模型导致生成的response中出现一些语法错误和语义无关的内容。生成式模型需要海量的训练数据，且难以优化。目前工业界常用的模型还是基于检索的模型，或者以生成式模型作为补充的两者结合，谷歌的Smart Reply就是一个例子。尽管目前生成式模型是学术界的研究热点，但在实践中使用检索式模型是更加合适的选择。 Ubuntu对话数据集这篇博客我们将使用Ubuntu对话数据集（论文来源 github地址）。这个数据集（Ubuntu Dialog Corpus, UDC）是目前最大的公开对话数据集之一，它是来自Ubuntu的IRC网络上的对话日志。这篇论文介绍了该数据集生成的具体细节。下面简单介绍一下数据的格式。 训练数据有1,000,000条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。下面是数据示例： 数据集的生成使用了NLTK工具，包括分词、stemmed、lemmatized等文本预处理步骤；同时还使用了NER技术，将文本中的实体，如姓名、地点、组织、URL等替换成特殊字符。这些文本预处理并不是必须的，但是能够提升一些模型的性能。据统计，query的平均长度为86个word，而response的平均长度为17个word，更多的数据统计信息见Jupyter notebook。 数据集也包括了测试和验证集，但这两部分的数据和训练数据在格式上不太一样。在测试集和验证集中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。下面是数据示例： 模型的评测方式有很多种。其中最常用到的是recall@k，即经模型对候选的response排序后，前k个候选中存在正例数据（正确的那个）的占比；显然k值越大，该指标会越高，因为这对模型性能的要求越松。 在Ubuntu数据集中，负例数据都是随机生成的；然而在现实中，想要从全部的数据中随机生成负例是不可能的。谷歌的Smart Reply则使用了聚类技术，然后将每个类的中取一些作为负例，这样生成负例的方式显得更加合理（考虑了负例数据的多样性，同时减少时间开销）。 BASELINE在使用NN模型之前，先设立一些简单的baseline模型，以方便后续的效果对比。使用如下的函数来计算recall@k: def evaluate_recall(y, y_test, k=1): num_examples = float(len(y)) num_correct = 0 for predictions, label in zip(y, y_test): if label in predictions[:k]: num_correct += 1 return num_correct/num_examples 其中，y是所预测的以降序排列的模型预测分值，y_test是实际的label值。举个例子，假设y的值为[0,3,1,2,5,6,4,7,8,9]，这说明第0号的候选的预测分值最高、作为回复的可能性最高，而9号则最低。这里的第0号同时也是正确的那个，即正例数据，标号为1-9的为随机生成的负例数据。 理论上，最base的随机模型（Random Predictor）的recall@1的值为10%，recall@2的值为20%。相应的代码如下： # Random Predictor def predict_random(context, utterances): return np.random.choice(len(utterances), 10, replace=False) # Evaluate Random predictor y_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))] y_test = np.zeros(len(y_random)) for n in [1, 2, 5, 10]: print("Recall @ ({}, 10): {:g}".format(n, evaluate_recall(y_random, y_test, n))) 实际的模型结果如下： Recall @ (1, 10): 0.0937632 Recall @ (2, 10): 0.194503 Recall @ (5, 10): 0.49297 Recall @ (10, 10): 1 这与理论预期相符，但这不是我们所追求的结果。 另外一个baseline的模型为tfidf predictor。tfidf表示词频（term frequency）和逆文档词频（inverse document frequency），它衡量了一个词在一篇文档中的重要程度（基于整个语料库）。直观上，两篇文档对应的tfidf向量越接近，两篇文章的内容也越相似。同样的，对于一个QR pair，它们语义上接近的词共现的越多，也将越可能是一个正确的QR pair（这句话存疑，原因在于QR之间也有可能不存在语义上的相似，一个Q对应的R是多样的。）。tfidf predictor对应的代码如下（利用scikit-learn工具能够轻易实现）： class TFIDFPredictor: def __init__(self): self.vectorizer = TfidfVectorizer() def train(self, data): self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values)) def predict(self, context, utterances): # Convert context and utterances into tfidf vector vector_context = self.vectorizer.transform([context]) vector_doc = self.vectorizer.transform(utterances) # The dot product measures the similarity of the resulting vectors result = np.dot(vector_doc, vector_context.T).todense() result = np.asarray(result).flatten() # Sort by top results and return the indices in descending order return np.argsort(result, axis=0)[::-1] # Evaluate TFIDF predictor pred = TFIDFPredictor() pred.train(train_df) y = [pred.predict(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))] for n in [1, 2, 5, 10]: print("Recall @ ({}, 10): {:g}".format(n, evaluate_recall(y, y_test, n))) 模型结果如下： Recall @ (1, 10): 0.495032 Recall @ (2, 10): 0.596882 Recall @ (5, 10): 0.766121 Recall @ (10, 10): 1 显然这比Random的模型要好得多，但这还不够。之前的假设并不完美，首先query和response之间并不一定要是语义上的相近；其次tfidf模型忽略了词序这一重要的信息。使用NN模型我们能做得更好一些。 LSTM这篇博文将建立的NN模型为两层Encoder的LSTM模型（Dual Encoder LSTM Network），这种形式的网络被广泛应用在chatbot中（尽管可能效果并不是最佳的那个，你可以尽可能地尝试其他的NN模型）。seq2seq模型常用于机器翻译领域，并取得了较大的效果。使用Dual LSTM模型的原因在于这个模型被证明在这个数据集有较好的效果（详情见这里）,这可以作为我们后续模型效果的验证。 两层Encoder的LSTM模型的结构图如下（论文来源）： 大致的流程如下： (1) Query和Response都是经过分词的，分词后每个词embedded为向量形式。初始的词向量使用GloVe vectors，之后词向量随着模型的训练会进行fine-tuned（实验发现，初始的词向量使用GloVe并没有在性能上带来显著的提升）。 (2) 分词且向量化的Query和Response经过相同的RNN（word by word）。RNN最终生成一个向量表示，捕捉了Query和Response之间的[语义联系]（图中的c和r）；这个向量的维度是可以指定的，这里指定为256维。 (3) 将向量c与一个矩阵M相乘，来预测一个可能的回复r’。如果c为一个256维的向量，M维256*256的矩阵，两者相乘的结果为另一个256维的向量，我们可以将其解释为[一个生成式的回复向量]。矩阵M是需要训练的参数。 (4) 通过点乘的方式来预测生成的回复r’和候选的回复r之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过sigmoid函数归一化，转成概率形式。图中把第(3)步和第(4)步结合在一起了。 为了训练模型，我们还需要一个损失函数（loss function）。这里使用二元的交叉熵（binary cross-entropy）作为损失函数。我们已知实例的真实label y，值为0或1；通过上面的第(4)步可以得到一个概率值 y&#39;；因此，交叉熵损失值为L = -y * ln(y&#39;) - (1 - y) * ln(1 - y&#39;)。这个公式的意义是直观的，即当y=1时，L = -ln(y&#39;)，我们期望y&#39;尽量地接近1使得损失函数的值越小；反之亦然。 实现过程中使用了numpy、pandas、TensorFlow和TF Learn等工具。 数据预处理数据集的原始格式为csv格式，我们需要先将其转为TensorFlow专有的格式，这种格式的好处在于能够直接从输入文件中load tensors，并让TensorFlow来处理洗牌(shuffling)、批量(batching)和队列化(queuing)等操作。预处理中还包括创建一个字典库，将词进行标号，TFRecord文件将直接存储这些词的标号。 每个实例包括如下几个字段： Query：表示为一串词标号的序列，如[231, 2190, 737, 0, 912]； Query的长度； Response：同样是一串词标号的序列； Response的长度； Label； Distractor_[N]：表示负例干扰数据，仅在验证集和测试集中有，N的取值为0-8； Distractor_[N]的长度； 数据预处理的Python脚本见这里，生成了3个文件：train.tfrecords, validation.tfrecords 和 test.tfrecords。你可以尝试自己运行程序，或者直接下载和使用预处理后的数据。 创建输入函数为了使用TensoFlow内置的训练和评测模块，我们需要创建一个输入函数：这个函数返回输入数据的batch。因为训练数据和测试数据的格式不同，我们需要创建不同的输入函数。输入函数需要返回批量(batch)的特征和标签值(如果有的话)。类似于如下： def input_fn(): # TODO Load and preprocess data here return batched_features, labels 因为我们需要在模型训练和评测过程中使用不同的输入函数，为了防止重复书写代码，我们创建一个包装器(wrapper)，名称为create_input_fn，针对不同的mode使用相应的code，如下： def create_input_fn(mode, input_files, batch_size, num_epochs=None): def input_fn(): # TODO Load and preprocess data here return batched_features, labels return input_fn 完整的code见udc_inputs.py。整体上，这个函数做了如下的事情： (1) 定义了示例文件中的feature字段；(2) 使用tf.TFRecordReader来读取input_files中的数据；(3) 根据feature字段的定义对数据进行解析；(4) 提取训练数据的标签；(5) 产生批量化的训练数据；(6) 返回批量的特征数据及对应标签； 定义评测指标之前已经提到用recall@k这个指标来评测模型，TensorFlow中已经实现了许多标准指标（包括recall@k）。为了使用这些指标，需要创建一个字典，key为指标名称，value为对应的计算函数。如下： def create_evaluation_metrics(): eval_metrics = {} for k in [1, 2, 5, 10]: eval_metrics["recall_at_%d" % k] = functools.partial( tf.contrib.metrics.streaming_sparse_recall_at_k, k=k) return eval_metrics 如上，我们使用了functools.partial函数，这个函数的输入参数有两个。不要被streaming_sparse_recall_at_k所困惑，其中的streaming的含义是表示指标的计算是增量式的。 训练和测试所使用的评测方式是不一样的，训练过程中我们对每个case可能作为正确回复的概率进行预测，而测试过程中我们对每组数据（包含10个case，其中1个是正确的，另外9个是生成的负例/噪音数据）中的case进行逐条概率预测，得到例如[0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11]这样格式的输出，这些输出值的和并不要求为1（因为是逐条预测的，有单独的预测概率值，在0到1之间）；而对于这组数据而言，因为数据index=0对应的为正确答案，这里recall@1为0，因为0.34是其中第二大的值，所以recall@2是1（表示这组数据中预测概率值在前二的中有一个是正确的）。 训练程序样例首先，给一个模型训练和测试的程序样例，这之后你可以参照程序中所用到的标准函数，来快速切换和使用其他的网络模型。假设我们有一个函数model_fn，函数的输入参数有batched features，label和mode(train/evaluation)，函数的输出为预测值。程序样例如下： estimator = tf.contrib.learn.Estimator( model_fn=model_fn, model_dir=MODEL_DIR, config=tf.contrib.learn.RunConfig()) input_fn_train = udc_inputs.create_input_fn( mode=tf.contrib.learn.ModeKeys.TRAIN, input_files=[TRAIN_FILE], batch_size=hparams.batch_size) input_fn_eval = udc_inputs.create_input_fn( mode=tf.contrib.learn.ModeKeys.EVAL, input_files=[VALIDATION_FILE], batch_size=hparams.eval_batch_size, num_epochs=1) eval_metrics = udc_metrics.create_evaluation_metrics() # We need to subclass theis manually for now. The next TF version will # have support ValidationMonitors with metrics built-in. # It's already on the master branch. class EvaluationMonitor(tf.contrib.learn.monitors.EveryN): def every_n_step_end(self, step, outputs): self._estimator.evaluate( input_fn=input_fn_eval, metrics=eval_metrics, steps=None) eval_monitor = EvaluationMonitor(every_n_steps=FLAGS.eval_every) estimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor]) 这里创建了一个model_fn的estimator(评估函数)；两个输入函数，input_fn_train和input_fn_eval，以及计算评测指标的函数； 创建模型到目前为止，我们创建了模型的输入、解析、评测和训练的样例程序。现在我们来写LSTM的程序，create_model_fn函数用以处理不同格式的训练和测试数据；它的输入参数为model_impl，这个函数表示实际作出预测的模型，这里就是用的LSTM，当然你可以替换成任意的其他模型。程序如下： def dual_encoder_model( hparams, mode, context, context_len, utterance, utterance_len, targets): # Initialize embedidngs randomly or with pre-trained vectors if available embeddings_W = get_embeddings(hparams) # Embed the context and the utterance context_embedded = tf.nn.embedding_lookup( embeddings_W, context, name="embed_context") utterance_embedded = tf.nn.embedding_lookup( embeddings_W, utterance, name="embed_utterance") # Build the RNN with tf.variable_scope("rnn") as vs: # We use an LSTM Cell cell = tf.nn.rnn_cell.LSTMCell( hparams.rnn_dim, forget_bias=2.0, use_peepholes=True, state_is_tuple=True) # Run the utterance and context through the RNN rnn_outputs, rnn_states = tf.nn.dynamic_rnn( cell, tf.concat(0, [context_embedded, utterance_embedded]), sequence_length=tf.concat(0, [context_len, utterance_len]), dtype=tf.float32) encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h) with tf.variable_scope("prediction") as vs: M = tf.get_variable("M", shape=[hparams.rnn_dim, hparams.rnn_dim], initializer=tf.truncated_normal_initializer()) # "Predict" a response: c * M generated_response = tf.matmul(encoding_context, M) generated_response = tf.expand_dims(generated_response, 2) encoding_utterance = tf.expand_dims(encoding_utterance, 2) # Dot product between generated response and actual response # (c * M) * r logits = tf.batch_matmul(generated_response, encoding_utterance, True) logits = tf.squeeze(logits, [2]) # Apply sigmoid to convert logits to probabilities probs = tf.sigmoid(logits) # Calculate the binary cross-entropy loss losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets)) # Mean loss across the batch of examples mean_loss = tf.reduce_mean(losses, name="mean_loss") return probs, mean_loss 完整的程序见dual_encoder.py。基于这个，我们能够实例化model函数在我们之前定义的udc_train.py，如下： model_fn = udc_model.create_model_fn( hparams=hparams, model_impl=dual_encoder_model) 这样我们就可以直接运行udc_train.py文件，来开始模型的训练和评测了，你可以设定--eval_every参数来控制模型在验证集上的评测频率。更多的命令行参数信息可见tf.flags和hparams，你也可以运行python udc_train.py --help来查看。 运行程序的效果如下： INFO:tensorflow:training step 20200, loss = 0.36895 (0.330 sec/batch). INFO:tensorflow:Step 20201: mean_loss:0 = 0.385877 INFO:tensorflow:training step 20300, loss = 0.25251 (0.338 sec/batch). INFO:tensorflow:Step 20301: mean_loss:0 = 0.405653 ... INFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383 ... 模型的评测在训练完模型后，你可以将其应用在测试集上，使用： python udc_test.py --model_dir=$MODEL_DIR_FROM_TRAINING 例如： python udc_test.py --model_dir=~/github/chatbot-retrieval/runs/1467389151 这将得到模型在测试集上的recall@k的结果，注意在使用udc_test.py文件时，需要使用与训练时相同的参数。 在训练模型的次数大约2w次时(在GPU上大约花费1小时)，模型在测试集上得到如下的结果： recall_at_1 = 0.507581018519 recall_at_2 = 0.689699074074 recall_at_5 = 0.913020833333 其中，recall@1的值与tfidf模型的差不多，但是recall@2和recall@5的值则比tfidf模型的结果好太多。原论文中的结果依次是0.55,0.72和0.92，可能通过模型调参或者预处理能够达到这个结果。 使用模型进行预测对于新的数据，你可以使用udc_predict.py来进行预测；例如： python udc_predict.py --model_dir=./runs/1467576365/ 结果如下： Context: Example context Response 1: 0.44806 Response 2: 0.481638 你可以从候选的回复中，选择预测分值最高的那个作为回复。 总结这篇博文中，我们实现了一个基于检索的NN模型，它能够对候选的回复进行预测和打分，通过输出分值最高（或者满足一定阈值）的候选回复已完成聊天的过程。后续可以尝试其他更好的模型，或者通过调参来取得更好的实验结果。 以上内容为原文 DEEP LEARNING FOR CHATBOTS, PART 2 – IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW 的翻译，供自己学习及他人参考。本文涉及到的数据和代码见Github仓库地址。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/deep-learning-for-chatbots-2.html 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>ChatBot</tag>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解LSTM网络]]></title>
    <url>%2Funderstanding-lstm-network.html</url>
    <content type="text"><![CDATA[循环神经网络(RNN)人们的每次思考并不都是从零开始的。比如说你在阅读这篇文章时，你基于对前面的文字的理解来理解你目前阅读到的文字，而不是每读到一个文字时，都抛弃掉前面的思考，从头开始。你的记忆是有持久性的。 传统的神经网络并不能如此，这似乎是一个主要的缺点。例如，假设你在看一场电影，你想对电影里的每一个场景进行分类。传统的神经网络不能够基于前面的已分类场景来推断接下来的场景分类。 循环神经网络(Recurrent Neural Networks)解决了这个问题。这种神经网络带有环，可以将信息持久化。 在上图所示的神经网络$A$中，输入为$X_t$，输出为$h_t$。$A$上的环允许将每一步产生的信息传递到下一步中。环的加入使得RNN变得神秘。不过，如果你多思考一下的话，其实RNN跟普通的神经网络也没有那么不同。一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。如果我们将环展开的话： 这种链式结构展示了RNN与序列和列表的密切关系。RNN的这种结构能够非常自然地使用这类数据。而且事实的确如此。在过去的几年里，RNN在一系列的任务中都取得了令人惊叹的成就，比如语音识别，语言建模，翻译，图片标题等等。关于RNN在各个领域所取得的令人惊叹的成就，参见这篇文章。 LSTM是这一系列成功中的必要组成部分。LSTM(Long Short Term Memory)是一种特殊的循环神经网络，在许多任务中，LSTM表现得比标准的RNN要出色得多。几乎所有基于RNN的令人赞叹的结果都是LSTM取得的。本文接下来将着重介绍LSTM。 长期依赖(Long Term Dependencies)的问题RNN的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果RNN真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。 有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN可以被训练来使用这样的信息。 然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I grew up in France… I speak fluent French”中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要France这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。 不幸的是，随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。 理论上说，RNN是有能力来处理这种长期依赖(Long Term Dependencies)的。人们可以通过精心调参来构建模型处理一个这种玩具问题(Toy Problem)。不过，在实际问题中，RNN并没有能力来学习这些。Hochreiter (1991) German更深入地讲了这个问题，Bengio, et al. (1994)发现了RNN的一些非常基础的问题。 幸运的是，LSTM并没有上述问题！ LSTM网络LSTM，全称为长短期记忆网络(Long Short Term Memory networks)，是一种特殊的RNN，能够学习到长期依赖关系。LSTM由Hochreiter &amp; Schmidhuber (1997)提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM在许多问题上效果非常好，现在被广泛使用。 LSTM在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。 LSTM也有这种链状结构，不过其重复模块的结构不同。LSTM的重复模块中有4个神经网络层，并且他们之间的交互非常特别。 现在暂且不必关心细节，稍候我们会一步一步地对LSTM的各个部分进行介绍。开始之前，我们先介绍一下将用到的标记。 在上图中，每条线表示向量的传递，从一个结点的输出传递到另外结点的输入。粉红圆表示向量的元素级操作，比如相加或者相乘。黄色方框表示神经网络的层。线合并表示向量的连接，线分叉表示向量复制。 LSTM核心思想LSTM的关键是元胞状态(Cell State)，下图中横穿整个元胞顶部的水平线。 元胞状态有点像是传送带，它直接穿过整个链，同时只有一些较小的线性交互。上面承载的信息可以很容易地流过而不改变。 LSTM有能力对元胞状态添加或者删除信息，这种能力通过一种叫门的结构来控制。 门是一种选择性让信息通过的方法。它们由一个Sigmoid神经网络层和一个元素级相乘操作组成。 Sigmoid层输出0~1之间的值，每个值表示对应的部分信息是否应该通过。0值表示不允许信息通过，1值表示让所有信息通过。一个LSTM有3个这种门，来保护和控制元胞状态。 LSTM分步详解LSTM的第一步是决定我们将要从元胞状态中扔掉哪些信息。该决定由一个叫做“遗忘门(Forget Gate)”的Sigmoid层控制。遗忘门观察\(h_{t-1}\)和\(x_{t}\)，对于元胞状态\(C_{t-1}\)中的每一个元素，输出一个0~1之间的数。1表示“完全保留该信息”，0表示“完全丢弃该信息”。 回到之前的预测下一个单词的例子。在这样的一个问题中，元胞状态可能包含当前主语的性别信息，以用来选择正确的物主代词。当我们遇到一个新的主语时，我们就需要把旧的性别信息遗忘掉。 下一步是决定我们将会把哪些新信息存储到元胞状态中。这步分为两部分。首先，有一个叫做“输入门(Input Gate)”的Sigmoid层决定我们要更新哪些信息。接下来，一个tanh层创造了一个新的候选值，$\tilde{C_t}$，该值可能被加入到元胞状态中。在下一步中，我们将会把这两个值组合起来用于更新元胞状态。 在语言模型的例子中，我们可能想要把新主语的性别加到元胞状态中，来取代我们已经遗忘的旧值。 现在我们该更新旧元胞状态$C_{t-1}$到新状态$C_t$了。上面的步骤中已经决定了该怎么做，这一步我们只需要实际执行即可。 我们把旧状态$C_{t-1}$乘以$f_t$，忘掉我们已经决定忘记的内容。然后我们再加上$i_t * \tilde{C_t}$，这个值由新的候选值（$\tilde{C_t}$）乘以候选值的每一个状态我们决定更新的程度（$i_t$）构成。 还是语言模型的例子，在这一步，我们按照之前的决定，扔掉了旧的主语的性别信息，并且添加了新的信息。 最后，我们需要决定最终的输出。输出将会基于目前的元胞状态，并且会加入一些过滤。首先我们建立一个Sigmoid层的输出门(Output Gate)，来决定我们将输出元胞的哪些部分。然后我们将元胞状态通过tanh之后（使得输出值在-1到1之间），与输出门相乘，这样我们只会输出我们想输出的部分。 对于语言模型的例子，由于刚刚只输出了一个主语，因此下一步可能需要输出与动词相关的信息。举例来说，可能需要输出主语是单数还是复数，以便于我们接下来选择动词时能够选择正确的形式。 LSTM的变种本文前面所介绍的LSTM是最普通的LSTM，但并非所有的LSTM模型都与前面相同。事实上，似乎每一篇paper中所用到的LSTM都是稍微不一样的版本。不同之处很微小，不过其中一些值得介绍。 一个流行的LSTM变种，由Gers &amp; Schmidhuber (2000)提出，加入了“窥视孔连接(peephole connection)”。也就是说我们让各种门可以观察到元胞状态。 上图中，对于所有的门都加入了“窥视孔”，不过也有一些paper中只加一部分。 另一种变种是使用对偶的遗忘门和输入门。我们不再是单独地决定需要遗忘什么信息，需要加入什么新信息；而是一起做决定：我们只会在需要在某处放入新信息时忘记该处的旧值；我们只会在已经忘记旧值的位置放入新值。 另一个变化更大一些的LSTM变种叫做Gated Recurrent Unit，或者GRU，由Cho, et al. (2014)提出。GRU将遗忘门和输入门合并成为单一的“更新门(Update Gate)”。GRU同时也将元胞状态(Cell State)和隐状态(Hidden State)合并，同时引入其他的一些变化。该模型比标准的LSTM模型更加简化，同时现在也变得越来越流行。 另外还有很多其他的模型，比如Yao, et al. (2015)提出的Depth Gated RNNs。同时，还有很多完全不同的解决长期依赖问题的方法，比如Koutnik, et al. (2014)提出的Clockwork RNNs。 不同的模型中哪个最好？这其中的不同真的有关系吗？Greff, et al. (2015)对流行的变种做了一个比较，发现它们基本相同。Jozefowicz, et al. (2015)测试了一万多种RNN结构，发现其中的一些在特定的任务上效果比LSTM要好。 结论前文中，我提到了人们使用RNN所取得的出色的成就。本质上，几乎所有的成就都是由LSTM取得的。对于大部分的任务，LSTM表现得非常好。 由于LSTM写在纸上是一堆公式，因此看起来很吓人。希望本文的分步讲解能让读者更容易接受和理解。 LSTM使得我们在使用RNN能完成的任务上迈进了一大步。很自然，我们会思考，还会有下一个一大步吗？研究工作者们的共同观点是：“是的！还有一个下一步，那就是注意力(Attention)！”注意力机制的思想是，在每一步中，都让RNN从一个更大的信息集合中去选择信息。举个例子，假如你使用RNN来生成一幅图片的说明文字，RNN可能在输出每一个单词时，都会去观察图片的一部分。事实上，Xu, et al.(2015)做的正是这个工作！如果你想探索注意力机制的话，这会是一个很有趣的起始点。现在已经有很多使用注意力的令人兴奋的成果，而且似乎更多的成果马上将会出来…… 注意力并不是RNN研究中唯一让人兴奋的主题。举例说，由Kalchbrenner, et al. (2015)提出的Grid LSTM似乎极有前途。在生成式模型中使用RNN的工作——比如Gregor, et al. (2015)、Chung, et al. (2015)以及Bayer &amp; Osendorfer (2015)——看起来也非常有意思。最近的几年对于RNN来说是一段非常令人激动的时间，接下来的几年也必将更加使人振奋！ 本文结束，感谢欣赏。 注：本文翻译自colah’s blog，原文链接：http://colah.github.io/posts/2015-08-Understanding-LSTMs/。译文原作者：Naitong Yu，译文链接：https://yunaitong.cn/understanding-lstm-networks.html 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>LSTM</tag>
        <tag>Machine Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种系统下Shadowsocks客户端的安装与配置]]></title>
    <url>%2Fhow-to-install-and-setup-shadowsocks-client-in-different-os.html</url>
    <content type="text"><![CDATA[下面介绍如何在 Windows / Mac / Linux(Ubuntu) / iOS / Android 系统下对Shadowsocks的客户端（下面简称SS）的安装和配置，以便于使用相应的VPN服务。在此之前假设你已经知道了SS服务器的端口和密码，如果不知道的话，可以向VPN的提供者（管理员）索要。这里假设你端口号和密码已经获取得到，请结合自己所使用的系统环境找到对应的安装配置方法，在需要输入端口号和密码的地方输入即可。（注意：最新的版本的加密算法已经从rc4-md5改成chacha20-ietf-poly1305了。） Windows下SS客户端的安装与配置适用于Windows 7及以上的系统。 第一步：下载SS客户端 下载方式：百度云 下载链接：点击此处 密码：320a 下载解压（解压密码：jeyzhang）后，你会看到一个Shadowsocks.exe的文件。 第二步：配置客户端运行exe文件，你会看到如下窗口；请按照图片中的内容进行配置： 输入端口号和密码，点击OK，右键右下角的运行图标：勾选[启动系统代理]，之后你应该就可以正常访问外网了。 MAC系统下的VPN配置方法第一步：下载SS客户端 下载方式：百度云 下载链接：点击此处 密码：5zx9 解压缩（解压密码：jeyzhang）后，你会看到一个Shadowsocks-2.6.3.dmg文件。双击打开，并按照提示将ShadowsocksX拖拽至Applications文件夹完成安装。 第二步：运行客户端运行ShadowsocksX后，会提示输入密码进行安装，如下图所示，请输入帐号密码并继续。 点击左下角+号添加服务器，你会看到如下窗口；请按照图片中的内容进行配置： 输入给你的端口号和密码，点击OK即可。此时你应该就能够访问国外网络了。 Linux (Ubuntu 14.04)下Shadowsocks的配置方法下面介绍如何在Ubuntu 14.04 下通过配置Shadowsocks和浏览器来使用VPN服务，主要分两步： 配置Shadowsocks命令行程序； 配置浏览器（Firefox或Chrome）； 第一步：配置Shadowsocks命令行程序打开终端，输入： sudo apt-get update sudo apt-get install python-pip sudo apt-get install python-setuptools m2crypto 安装Shadowsocks，输入： sudo pip install shadowsocks 启动Shadowsocks sslocal -s 服务器域名或IP -p 服务器端口号 -k “密码” -l 1080 -t 600 -m rc4-md5 注意：密码格式为”*”，包含双引号。 正常情况下应显示如下： （注意图中的命令按照实际情况输入。） 第二步：配置浏览器这里仅以Firefox和Chrome为例，其他浏览器可自行搜索，配置方式相似。 1. Firefox浏览器 添加并安装插件 搜索”foxyproxy”， 重启浏览器后，配置foxyproxy 新建代理服务器： 起一个代理名称： 配置代理服务器： 添加“模式订阅”： 建议把下面两个模式文件都订阅一下： http://firefoxfan.org/gfwlist/gfwlist.txt http://sslite.top/gfwlist/gfwlist.txt 显示模式订阅导入成功： 之后重启浏览器，点击浏览器上的应用图标，选择“使用预定义模板的代理服器”： 此时你应该可以使用firefox浏览器浏览国外网站了。 2. Chrome浏览器 安装SwitchySharp插件、 在Chrome的插件商店里搜索”SwitchSharp”并安装： 安装完之后，重启浏览器，浏览器右上角会出现应用的图标： 然后点击”Options”，按照如下进行配置： 配置完成后，重启浏览器，点击应用图标，选择刚才配置的”SS”即可。之后你应该就可以正常访问国外网站啦。 iOS系统下的Shadowsocks配置适用于iPhone和iPad。 首先打开AppStore应用商店，搜索Wingy应用（目前是免费应用），下载并安装。（该方法已经不适用，最新方法需要下载Potatso lite或者Potatso 2。） 第一步：添加线路打开应用后，首页显示如下，点击[选择线路（添加线路）]： 第二步：配置按照如下的红框要求，填写配置信息（你的端口号和密码），最后保存退出。 回到首页后，点击中间大大的 [开启按钮] 即可打开VPN（注意：弹出的系统框选择Allow）。 之后你就可以自由地上网啦~ Android系统下的Shadowsocks配置下面介绍如何在Android系统下进行Shadowsocks的配置，以使用VPN服务。注意，你的安卓手机最好先root，root的教程就不再赘述了，网上搜索下载个root的软件即可。本教程使用的安卓手机为乐视1s手机，其他安卓系统的手机操作方式类似。 第一步：下载SS客户端 下载方式：百度云 下载链接：点击此处 密码：i7lr 第二步：导入apk文件至手机（1）手机连接PC后，选择如下的模式进行文件的导入： （2）打开手机中的”文件管理器”，点击相应的apk文件进行安装即可。 第三步：打开Shadowsocks客户端，进行相应的配置（1）填入相应的配置信息： 注意，服务器的端口和密码按照实际情况填入。 还可以通过“分应用代理”来选择什么应用使用代理服务（示例：这里只在使用Chrome的时候使用代理服务）： 点击右上角的“开启键”，并保持Shadowsocks客户端在后台的运行。之后你就可以正常访问国外网站啦。 使用Chrome浏览器访问， 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/how-to-install-and-setup-shadowsocks-client-in-different-os.html]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>Linux</tag>
        <tag>Mac</tag>
        <tag>Shadowsocks</tag>
        <tag>VPN</tag>
        <tag>Windows</tag>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C#学习笔记2：多线程]]></title>
    <url>%2Fcsharp-learning-notes-2-multithreading.html</url>
    <content type="text"><![CDATA[理解进程与线程进程 (Process) 是Windows系统中的一个基本概念，它包含着一个运行程序所需要的资源。进程之间是相对独立的，一个进程无法访问另一个进程的数据（除非利用分布式计算方式），一个进程运行的失败也不会影响其他进程的运行，Windows系统就是利用进程把工作划分为多个独立的区域的。进程可以理解为一个程序的基本边界。 应用程序域 (AppDomain) 是一个程序运行的逻辑区域，它可以视为一个轻量级的进程，.NET的程序集正是在应用程序域中运行的，一个进程可以包含有多个应用程序域，一个应用程序域也可以包含多个程序集。在一个应用程序域中包含了一个或多个上下文context，使用上下文CLR就能够把某些特殊对象的状态放置在不同容器当中。 线程 (Thread) 是进程中的基本执行单元，在进程入口执行的第一个线程被视为这个进程的主线程。在.NET应用程序中，都是以Main方法作为入口的，当调用此方法时系统就会自动创建一个主线程。线程主要是由CPU寄存器、调用栈和线程本地存储器 (Thread Local Storage，TLS) 组成的。CPU寄存器主要记录当前所执行线程的状态，调用栈主要用于维护线程所调用到的内存与数据，TLS主要用于存放线程的状态信息。 一个进程内可以包括多个应用程序域，也有包括多个线程，线程也可以穿梭于多个应用程序域当中。但在同一个时刻，线程只会处于一个应用程序域内。 线程的生命周期线程生命周期开始于 System.Threading.Thread 类的对象被创建时，结束于线程被终止或完成执行时。 下面列出了线程生命周期中的各种状态： 未启动状态：当线程实例被创建但 Start 方法未被调用时的状况。 就绪状态：当线程准备好运行并等待 CPU 周期时的状况。 不可运行状态：下面的几种情况下线程是不可运行的： 已经调用 Sleep 方法 已经调用 Wait 方法 通过 I/O 操作阻塞 死亡状态：当线程已完成执行或已中止时的状况。 线程的操作System.Threading.Thread类System.Threading.Thread 是用于控制线程的基础类，通过Thread可以控制当前应用程序域中线程的创建、挂起、停止、销毁。 属性 ManagedThreadId 是确认线程的唯一标识符，程序在大部分情况下都是通过 Thread.ManagedThreadId 来辨别线程的。而 Name 是一个可变值，在默认时候，Name 为一个空值 Null，开发人员可以通过程序设置线程的名称，但这只是一个辅助功能。 .NET 为线程设置了 Priority 属性来定义线程执行的优先级别，里面包含5个选项（Lowest, BelowNormal, Normal, AboveNormal, Highest），其中 Normal 是默认值。除非系统有特殊要求，否则不应该随便设置线程的优先级别。 通过 ThreadState 可以检测线程是处于 Unstarted、Sleeping、Running 等等状态，它比 IsAlive 属性能提供更多的特定信息。 一个应用程序域中可能包括多个上下文，而通过 CurrentContext 可以获取线程当前的上下文。 CurrentThread 是最常用的一个属性，它是用于获取当前运行的线程。 方法 程序示例通过Thread显示当前线程信息： static void Main(string[] args) { Thread thread = Thread.CurrentThread; thread.Name = "Main Thread"; string threadMessage = string.Format("Thread ID:{0}\n Current AppDomainId:{1}\n "+ "Current ContextId:{2}\n Thread Name:{3}\n "+ "Thread State:{4}\n Thread Priority:{5}\n", thread.ManagedThreadId, Thread.GetDomainID(), Thread.CurrentContext.ContextID, thread.Name, thread.ThreadState, thread.Priority); Console.WriteLine(threadMessage); Console.ReadKey(); } 结果如下： 实现多线程无参数传入：使用ThreadStart委托示例如下，首先在 Message 类中建立一个方法 ShowMessage()，里面显示了当前运行线程的 Id，并使用 Thread.Sleep(int) 方法模拟部分工作。在 main() 中通过 ThreadStart委托 绑定Message对象的 ShowMessage()方法，然后通过 Thread.Start() 执行异步方法。 public class Message { public void ShowMessage() { string message = string.Format("Async threadId is :{0}", Thread.CurrentThread.ManagedThreadId); Console.WriteLine(message); for (int n = 0; n &lt; 10; n++) { Thread.Sleep(300); Console.WriteLine("The number is:" + n.ToString()); } } } class Program { static void Main(string[] args) { Console.WriteLine("Main threadId is:"+ Thread.CurrentThread.ManagedThreadId); Message message=new Message(); Thread thread = new Thread(new ThreadStart(message.ShowMessage)); thread.Start(); Console.WriteLine("Do something ..........!"); Console.WriteLine("Main thread working is complete!"); } } 在调用 Thread.Start()方法 后，系统以异步方式运行 Message.ShowMessage()，而主线程的操作是继续执行的，在 Message.ShowMessage() 完成前，主线程已完成所有的操作。结果如下： 参数传入：三种方式使用 ThreadStart委托 的缺点在于无法给线程传递参数，下面介绍三种方式以实现给线程传递参数： 使用ParameterizedThreadStart委托 如果使用了ParameterizedThreadStart委托，线程的入口必须有一个object类型的参数，且返回类型为void。如下面的例子： class Program { static void Main(string[] args) { string hello = "hello world"; //这里也可简写成Thread thread = new Thread(ThreadMainWithParameters); //但是为了让大家知道这里用的是ParameterizedThreadStart委托，就没有简写了 Thread thread = new Thread(new ParameterizedThreadStart(ThreadMainWithParameters)); thread.Start(hello); Console.Read(); } static void ThreadMainWithParameters(object obj) { string str = obj as string; if(!string.IsNullOrEmpty(str)) Console.WriteLine("Running in a thread,received: {0}", str); } } 这里稍微有点麻烦的就是 ThreadMainWithParameters方法 里的参数必须是 object类型 的，我们需要进行类型转换。为什么参数必须是object类型呢？看看ParameterizedThreadStart委托的声明就知道了。 public delegate void ParameterizedThreadStart(object obj); //ParameterizedThreadStart委托的声明 创建自定义类 定义一个类，在其中定义需要的字段，将线程的主方法定义为类的一个实例方法。如下面的例子： public class MyThread { private string data; public MyThread(string data) { this.data = data; } public void ThreadMain() { Console.WriteLine("Running in a thread,data: {0}", data); } } class Program { static void Main(string[] args) { MyThread myThread = new MyThread("hello world"); Thread thread = new Thread(myThread.ThreadMain); thread.Start(); Console.Read(); } } 使用匿名方法 示例如下： class Program { static void Main(string[] args) { string hello = "hello world"; //如果写成Thread thread = new Thread(ThreadMainWithParameters(hello));这种形式，编译时就会报错 Thread thread = new Thread(() =&gt; ThreadMainWithParameters(hello)); thread.Start(); Console.Read(); } static void ThreadMainWithParameters(string str) { Console.WriteLine("Running in a thread,received: {0}", str); } 通过反编译体现了第三种方法其实和第二种方法其实是一样的。 关于async action的问题特别需要注意 new Task(async () =&gt; await FuncAsync()) 这种类似的写法，这种写法后面是不能await到函数执行结束的结果的。 见下面的例子： class TestTask { private async static Task TestAsync(int opt) { await Task.Delay(1000); Console.WriteLine("opt {0}: finish async task.", opt); } private static void TestSync(int opt) { Thread.Sleep(1000); Console.WriteLine("opt {0}: finish sync.", opt); } private async static Task StartAsync() { List&lt;Task&gt; tasks = new List&lt;Task&gt;(); tasks.Add(new Task(() =&gt; TestSync(1))); tasks.Add(new Task(async () =&gt; await TestAsync(2))); // 这条语句存在问题，不能await到函数执行的结果 foreach (var task in tasks) { task.Start(); } // 以下t1, t2, t3均不需要task.Start()方法 //var t1 = new Task&lt;Task&gt;(async () =&gt; await TestAsync(2)); //t1.Start(); //tasks.Add(t1.Unwrap()); //var t2 = Task.Factory.StartNew(async()=&gt;await TestAsync(2)); //tasks.Add(t2.Unwrap()); //var t3 = Task.Run(async () =&gt; await TestAsync(2)); //tasks.Add(t3); Task.WaitAll(tasks.ToArray()); Console.WriteLine("finish start."); } static void Main() { StartAsync().GetAwaiter().GetResult(); } } 执行结果为如下： 可以看出没有等到 TestAsync(2) 的返回结果。 应该将上述存在问题的语句改为如下中的任意一种即可： // 以下t1, t2, t3均不需要task.Start()方法 var t1 = new Task&lt;Task&gt;(async () =&gt; await TestAsync(2)); t1.Start(); tasks.Add(t1.Unwrap()); var t2 = Task.Factory.StartNew(async () =&gt; await TestAsync(2)); tasks.Add(t2.Unwrap()); var t3 = Task.Run(async () =&gt; await TestAsync(2)); tasks.Add(t3); 此时执行结果为： 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/csharp-learning-notes-2-multithreading.html 参考资料 C#综合揭秘——细说多线程 C#多线程 | 菜鸟教程 C# 给多线程传参的三种方式]]></content>
      <categories>
        <category>C#</category>
      </categories>
      <tags>
        <tag>C#</tag>
        <tag>Multithreading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人中的深度学习技术之一：导读]]></title>
    <url>%2Fdeep-learning-for-chatbots-1.html</url>
    <content type="text"><![CDATA[以下内容为原文 《Deep Learning For Chatbots, Part 1 - Introduction》 的翻译，供自己学习及他人参考。 这篇博文主要概述了目前聊天机器人主要用到的技术，从宏观上进行介绍，不涉及具体的技术细节。下一篇博文：聊天机器人中的深度学习技术之二：基于检索模型的实现会介绍并实现一个基于检索的LSTM模型，通过这个模型可以实现聊天机器人。 聊天机器人 (Chatbot)，也被称为对话引擎或者对话系统，是目前的热点之一。微软公司在聊天机器人上的投入巨大（链接），著名产品有小冰 (xiaoice)、bot framework等，其他公司也纷纷在这个领域发力，如Facebook的M，苹果公司的Siri等等。此外，一大批的创业公司发布了类似的产品，例如客户应用Operator，x.ai，bot framework Chatfuel，bot开发工具 Howdy’s Botkit。微软也发布了供开发者使用的 bot developer framework。 许多公司希望能开发出与用户进行自然语言式对话的机器人，并且声称使用了NLP和深度学习相关技术使之成为可能，然而这并不容易实现。在这个博文系列中，我将会重温一些被用于聊天机器人中的深度学习技术，披露出目前技术能够解决或者可能解决的问题以及几乎难以解决的问题。这篇文章是个概述，在接下来的博文中将介绍具体的技术细节。 模型分类基于检索技术的模型 VS 生成式模型基于检索技术的模型较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。 生成式模型则更加复杂，它不依赖于预定义好的回复内容，而是通过抓取(Scratch)的方法生成新的回复内容。生成式模型典型的有基于机器翻译模型的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是将用户的输入[翻译]为一个回答(response) 总结 以上两种模型均有优缺点。对于基于检索技术的模型，由于使用了知识库且数据为预先定义好的，因此进行回复的内容语法上较为通顺，较少出现语法错误；但是基于检索技术的模型中没有会话概念，不能结合上下文给出更加[智能]的回复。而生成式模型则更加[智能]一些，它能够更加有效地利用上下文信息从而知道你在讨论的东西是什么；然而生成式模型比较难以训练，并且输出的内容经常存在一些语法错误（尤其对于长句子而言），以及模型训练需要大规模的数据。 深度学习技术都能够用于基于检索技术的模型和生成式模型中，但是目前的研究热点在生成式模型上。深度学习框架例如Sequence to Sequence非常适合用来生成文本，非常多的研究者希望能够在这个领域取得成功。然而目前这一块的研究还在初期阶段，工业界的产品更多的还是使用基于检索计算的模型。 短对话 VS 长对话直观上处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。如果是一问一答的形式，技术上这将简单的多。通常对于客服对话而言，长对话更加常见，一次对话中往往会伴随着多个关联问题。 开放域 VS 特定领域面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。在一些社交网站例如Twitter和Reddit上的会话是属于开放域的，会话涉及的主题多种多样，需要的知识量也将非常巨大。 面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。这些系统通常是为了完成某些特定任务，尽管用户在该系统中也能够问些其他方面的东西，但是系统并不会给出相应的回复。 面临的挑战下面介绍一下聊天机器人技术所面临的挑战。 如何结合上下文信息为了产生质量更高的回复，聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了对话过程中的语言上下文信息和用户的身份信息等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。常见的方法是将一个会话转化为向量形式，但这对长会话而言是困难的。论文Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models和Attention with Intention for a Neural Network Conversation Model中的实验结果表明了这一点。另外，可以结合的上下文信息还包括会话进行时的日期地点信息、用户信息等。 语义一致性理论上来说，机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]。这理解起来是简单的，但却是学术界目前的难题之一（如下图）。许多系统都试图对相同语义而不同形式的问题给予语义上合理的回复，但却没有考虑一致性，最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。论文A Persona-Based Neural Conversation Model中提及的模型旨在创建具有固定统一人格的机器人。 对话模型的评测评价一个对话模型的好坏在于它是否很好地完成了某项任务，例如在对话中解决了客户的问题。这样的训练数据需要人工标注和评测，所以获取上需要一定人力代价。有时在开放域中的对话系统也没有一个清晰的优化目标。用于机器翻译的评测指标BLEU不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。论文How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation中给出结论，目前常用的评测指标均与人工评测无关。 意图和回复多样性生成式模型中的一个普遍问题是，它们都想要生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。早期的Google智能回复基本上以[I love you]回复所有的东西链接，这是一些模型最终训练出来的结果，原因在于训练数据和训练的优化目标。因此，有些研究学者开始关注如何提升机器人的回复的多样性，然而人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。 实际效果以目前的研究水平所制造的机器人能够取得的效果如何？使用基于检索技术的显然无法制作出面向开放域的机器人，这是因为你不能编写覆盖所有领域的语料；而生成式的面向开放域的机器人还属于通用人工智能(Artifical General Intelligence, AGI)水平，距离理想状态还相距甚远，但相关研究学者还在致力于此。 对于特定领域的机器人，基于检索的技术和生成式模型都能够利用。但是对于长对话的情境，也面临许多困难。 在最近对Andrew NG的采访中，NG提到： 目前深度学习的价值主要体现在能够获取大量数据的特定领域。目前一个无法做的事情是产生一个有意义的对话。 许多创业公司声称只要有足够多的数据，就能够产生自动智能的对话系统。然而，目前的水平生产出面向一个特定的子领域的对话应用（如利用Uber打车），而对于一个稍微开放点的领域就难以实现了（如自动销售）。但是，帮助用户提供自动回复建议以及语法纠正还是可行的。 使用基于检索技术的对话系统更加可控和稳定，给出的回复出现语法错误的几率更低。而使用生成式模型的风险在于回复不可控，且容易出现一些风险，例如微软的Tay。 即将到来的事情和阅读列表在之后的博文中将具体介绍深度学习方面的技术细节。提前阅读下面的文章将会对后面的学习更加有帮助。 Neural Responding Machine for Short-Text Conversation (2015-03) A Neural Conversational Model (2015-06) A Neural Network Approach to Context-Sensitive Generation of Conversational Responses (2015-06) The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems (2015-06) Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models (2015-07) A Diversity-Promoting Objective Function for Neural Conversation Models (2015-10) Attention with Intention for a Neural Network Conversation Model (2015-10) Improved Deep Learning Baselines for Ubuntu Corpus Dialogs (2015-10) A Survey of Available Corpora for Building Data-Driven Dialogue Systems (2015-12) Incorporating Copying Mechanism in Sequence-to-Sequence Learning (2016-03) A Persona-Based Neural Conversation Model (2016-03) How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation (2016-03) 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/deep-learning-for-chatbots-1.html 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>ChatBot</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C#学习笔记1：常见问题]]></title>
    <url>%2Fcsharp-learning-notes-1-qna.html</url>
    <content type="text"><![CDATA[概念同步、异步、阻塞、非阻塞转自知乎-严肃的回答： “阻塞”与”非阻塞”与”同步”与“异步”不能简单的从字面理解，提供一个从分布式系统角度的回答。 同步与异步 同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)。所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 典型的异步编程模型比如Node.js，举个例子： 普通B/S模式（同步）AJAX技术（异步） 同步：提交请求-&gt;等待服务器处理-&gt;处理完毕返回 这个期间客户端浏览器不能干任何事 异步: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕 举个通俗的例子： 你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 阻塞与非阻塞 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子， 你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 语法int? 的意思下面两种声明int变量的方式： int num int? num 区别在于，第二种中的num可以为空（或者赋值为null），而第一种只能赋值整数类型值。 对于其他的基础类型也同样适用。 关键字ref与out的区别C#中参数传递的方式有两种，分别是值传递和引用传递。ref与out这两种方式都是属于引用传递。唯一的区别在于： out关键字修饰的参数需要在调用的函数内部先初始化，再使用； ref关键字修饰的参数需要在调用函数前，先初始化再传入函数； static readonly与const的区别static readonly与const非常类似，共同点有：(1) 通过类名而不是对象名进行访问；(2) 在程序中只读(不可更改)等。两者的本质区别在于： const的值在编译期间就是确定的，只能在声明时通过常量表达式指定值； static readonly是在运行时计算出其值的，所以还可以通过静态构造函数来赋值。 下面的例子可以更加清楚地认识static readonly与const的区别： 1. static readonly MyClass myins = new MyClass(); 2. static readonly MyClass myins = null; 3. static readonly B = 10; static readonly A = B * 20; 4. static readonly int [] constIntArray = new int[] {1, 2, 3}; 5. void SomeFunction() { const int a = 10; ... } 6.private static string astr="abcd"; private const string str = astr+"efg"; 1：不可以 换成const。new操作符是需要执行构造函数的，所以无法在编译期间确定； 2：可以换成const。我们也看到，Reference类型的常量 （除了String）只能是Null； 3：可以换成const。我们可以在编译期间很明确的说，A等于200； 4：不可以换成 const。道理和1是一样的，虽然看起来1,2,3的数组的确就是一个常量； 5：不可以换成readonly，readonly只能用来修饰类 的field，不能修饰局部变量，也不能修饰property等其他类成员； 6.错误：如果在astr前加上const或者const改为readonly即可； 总结如下： const、readonly和static readonly定义的常量，指定初始值后(包括在构造函数内指定的初始值) 将不可更改，可读不可写； const定义时必须指定初始值，而readonly定义时可以不进行初始化(MS建议在定义时初始值),同时也可以在构造函数内指定初始值，并以构造函数内指定的值为准； const和static readonly定义的常量是静态的，只能由类直接访问；而readonly定义的常量是非静态的，只能由实例对象访问； static readonly常量，如果在构造函数内指定初始值，则必须是静态无参构造函数； const是编译时常量，readonly是运行时常量；cosnt较高效，readonly较灵活。在应用上以static readonly代替const，以平衡const在灵活性上的不足，同时克服编译器优化cosnt性能，所带来的程序集引用不一致问题； static关键字使用static修饰符声明属于类型本身而不是属于特定对象的静态成员。static修饰符可用于类、字段、方法、属性、运算符、事件和构造函数，但不能用于索引器、析构函数或类以外的类型。 static修饰的类称为静态类，静态类与非静态类的区别在于：静态类不能够被实例化（不能包含构造函数），仅包含静态成员，访问成员的方式是通过类名； 静态成员的初始化工作可以在静态构造函数内部完成（包括static readonly修饰符修饰的成员）； 与static修饰符相对应的是auto，通常它是默认的（即不用static修饰的都是auto的）。auto的含义是由程序自动控制变量的生存周期，通常指的就是变量在进入其作用域的时候被分配，离开其作用域的时候被释放；而static就是不auto，变量在程序初始化时被分配，直到程序退出前才被释放；也就是static是按照程序的生命周期来分配释放变量的，而不是变量自己的生命周期。看下面的例子： void func() { int a; static int b; } 每一次调用上面的func函数，变量a都是新的，因为它是在进入函数体的时候被分配，退出函数体的时候被释放，所以多个线程调用该函数，都会拥有各自独立的变量a，因为它总是要被重新分配的；而变量b不管你是否使用该函数，在程序初始化时就被分配的了，或者在第一次执行到它的声明的时候分配（不同的编译器可能不同），所以多个线程调用该函数的时候，总是访问同一个变量b，这也是在多线程编程中必须注意的。 语句使用using block在进行文件的读写操作、数据库连接操作等涉及程序结束时资源的释放，通常建议使用using block。格式如下： using(代码段) { 代码段... } 好处在于using block结束后，using里面声明的对象会自动释放内存。尽管.Net的垃圾处理机制会处理，但那是不可控制的。 下面是一个使用using block进行文件的读操作的示例，可以看出使用using block能够让代码更加简洁。 using (StreamReader sr=new .....) {} 相当于 StreamReader sr=null; try { sr=new ...; } finally { sr.Dispose(); } 使用LINQ语句多种数据源（如数据库、XML、数组、集合等）的数据格式是不一样的，因此常常需要对不同的数据格式进行相互转换，以及学习多种数据查询语言。为了使得数据查询和操作变得更加简便，.NET 3.5版本发布了LINQ。 LINQ全称为语言集成查询（Language Integrated Query），是一种用来进行数据访问的编程模型，它是C# 3.0和.Net 3.5中的主要新增功能。LINQ以统一的方式操作各种数据源，降低数据访问的复杂度。LINQ支持的数据源有SQL Sever、XML以及内存中的数据集合。 下面举一个简单的示例，以说明使用LINQ语句带来的简便性： bool flag = false; foreach (var elem in list) { if (elem...) { flag = true; break; } } ... 使用LINQ语句就是 bool flag = list.Any(elem =&gt; ...) 可以看到使用LINQ可以使得代码变得非常简洁。 OO相关C#中的field与propertyproperty与field的区别在于，property拥有访问器，访问器定义了读取或者写入属性值必须执行的代码。property不是变量，没有存储数据的功能，数据都存在字段中，因此不能对property直接赋值，必须通过set访问器。 为了类的封装性，一般将描述类的特征的字段定义为private，再将属性定义为public来操作私有字段。例如如下： private string phoneNumber; public string PhoneNumber { get { return phoneNumbe; } set { phoneNumber = value; } } 上述代码也可以简写为 public PhoneNumber { get; set; } 使用property最大的好处在于可以在对field进行赋值时，加入一些逻辑，例如限制只能给字段赋于某个范围的值、或是要求字段只能读或只能写等。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/cshape-learning-notes-1.html]]></content>
      <categories>
        <category>C#</category>
      </categories>
      <tags>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C#学习笔记0：入门]]></title>
    <url>%2Fcshape-learning-notes-0.html</url>
    <content type="text"><![CDATA[关于.NET Framework简介.NET Framework是微软公司为开发应用程序而设计的一个平台，它不限于运行的系统（尽管Windows系统上运行的最为广泛），包括Windows、linux（称为Mono）、Mac OS等。可以使用.NET Framework来创建各种不同类型的应用程序，例如Windows桌面应用程序、Web应用程序等等。.NET Framework的设计方式确保了它可以使用多种语言，例如C#，C++，VB等，所有的语言都可以访问.NET Framework，并且它们之间还可以互相通信，例如C#开发人员可以使用VB程序员编写的代码。 包含的内容.NET Framework包含了一个庞大的代码库，可以在客户语言（如C#）中通过面向对象编程技术来使用这些代码。代码库分为多个不同的模块，例如一个模块包含Windows应用程序的构件，另一个模块包含网络编程的代码块，还有一个模块包含Web开发的代码块，可以根据需要来选择使用不同的模块。 部分.NET Framework库定义了一些基本类型，类型是数据的一种表达方式，指定最基本类型有助于使用.NET Framework的各种语言之间进行交互操作。这成为通用系统类型（Common Type System， CTS）。 .NET Framework还包含.NET公共语言运行库（Common Language Runtime, CLR），它负责管理用.NET库开发的所有应用程序的执行。 程序的编写和执行代码编译使用.NET Framework编写应用程序，就是使用.NET代码库编写代码（使用支持.NET Framework的任何一种语言）。VS（Visual Studio）是一种强大的集成开发环境，能够方便将.NET功能集成到代码中。 使用C#创建程序，在程序执行中，需要把C#代码转换为目标操作系统能够理解的语言，即本机代码（naive code）。这种转换成为代码的编译，由编译器执行。在.NET Framework下，编译的过程包含两个阶段。 首先将代码编译为通用中间语言（Common Intermediate Language, CIL）代码（也称Microsoft Intermediate Language, MIL），这些代码不是专用于任何一种操作系统，也非专用于C#，其他的.NET语言如VB等也会在第一阶段编译为这种语言。使用VS开发C#应用程序时，这个编译步骤由VS完成。 要执行应用程序需要进一步的工作，这就是Just-In-Time(JIT)编译器的任务，它将CIL编译为专用于OS和目标机器结构的本机代码，这样才能在特定的机器和OS上运行程序。Just-In-Time反映了CIL代码仅在需要时才编译的事实，即在应用程序的运行过程中动态编译。 总结来说，JIT编译器使用的是CIL代码，CIL代码是独立于计算机、操作系统和CPU的，而不同的JIT编译器则可以针对最终程序执行的环境（如硬件条件、操作系统等）做优化。 程序集在编译应用程序时，所创建的CIL代码存储在一个程序集中。程序集包含可执行的应用程序文件（.exe文件）和其他应用程序使用的库（.dll文件）。除此之外，程序集还包含元数据（即程序集中包含的数据的描述信息）和可选的资源（CIL使用的其他数据，例如图片文件）。 托管代码在将代码编译为CIL，再用JIT编译器将其编译为本机代码后，CLR的任务尚未全部完成，还需要管理正在执行的用.NET Framework编写的代码（这个执行代码的阶段称为运行时(runtime)）。即CLR管理着应用程序，其方式是管理内存、处理安全以及允许进行跨语言调试等。 不受CLR控制运行的应用程序属于非托管类型，某些语言（如C++）可以用于编写此类应用程序，例如访问操作系统的低级功能。在C#中只能编写在托管环境下运行的代码。将使用CLR的托管功能，让.NET处理与操作系统的任何交互。 垃圾回收托管代码中一个重要的功能是垃圾回收，它可以确保应用程序在不使用某些内存时就完全释放这些内存。.Net垃圾回收会定期检查计算机内存，从中删除不再需要的内容，但是执行垃圾回收的时间并不固定。因为垃圾回收的执行时间是不确定的（但是一定会执行），所以设计程序的时候需要注意这一点，在需要清理内存时自己手动完成清理工作也是一个不错的选择。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/cshape-learning-notes-0.html]]></content>
      <categories>
        <category>C#</category>
      </categories>
      <tags>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whoosh+jieba：python下实现中文全文检索]]></title>
    <url>%2Frealization-of-full-chinese-text-search-using-whoosh-and-jieba.html</url>
    <content type="text"><![CDATA[需要安装 python 2.xx whoosh jieba whoosh和jieba的安装使用pip install即可。 快速入门下面的代码实现了简单的中文检索 # coding=utf-8 import os from whoosh.index import create_in from whoosh.fields import * from jieba.analyse import ChineseAnalyzer import json # 使用结巴中文分词 analyzer = ChineseAnalyzer() # 创建schema, stored为True表示能够被检索 schema = Schema(title=TEXT(stored=True, analyzer=analyzer), path=ID(stored=False), content=TEXT(stored=True, analyzer=analyzer)) # 存储schema信息至'indexdir'目录下 indexdir = 'indexdir/' if not os.path.exists(indexdir): os.mkdir(indexdir) ix = create_in(indexdir, schema) # 按照schema定义信息，增加需要建立索引的文档 # 注意：字符串格式需要为unicode格式 writer = ix.writer() writer.add_document(title=u"第一篇文档", path=u"/a", content=u"这是我们增加的第一篇文档") writer.add_document(title=u"第二篇文档", path=u"/b", content=u"第二篇文档也很interesting！") writer.commit() # 创建一个检索器 searcher = ix.searcher() # 检索标题中出现'文档'的文档 results = searcher.find("title", u"文档") # 检索出来的第一个结果，数据格式为dict{'title':.., 'content':...} firstdoc = results[0].fields() # python2中，需要使用json来打印包含unicode的dict内容 jsondoc = json.dumps(firstdoc, ensure_ascii=False) print jsondoc # 打印出检索出的文档全部内容 print results[0].highlights("title") # 高亮标题中的检索词 print results[0].score # bm25分数 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/realization-of-full-chinese-text-search-using-whoosh-and-jieba.html 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Jieba</tag>
        <tag>Python</tag>
        <tag>Search Engine</tag>
        <tag>Whoosh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习笔记与常见问题]]></title>
    <url>%2Fpython-learning-notes-and-common-problems.html</url>
    <content type="text"><![CDATA[列表使用lambda函数实现列表操作实现两个list的元素对应相乘，返回同等大小的list结果。 list1 = [...] list2 = [...] multiply_list = map(lambda (a, b): a * b, zip(list1, list2)) 列表的类型转换例如，经常遇到需要将元素类型为int的列表，转为元素类型为str的列表，可以方便的使用join等函数进行格式化处理。使用map函数将很简单： int_list = [1, 2, 3] str_list = map(str, int_list) # str_list = ['1', '2', '3'] # 类似的还有： map(int, list) # list中的元素均转为int类型 map(float, list) # list中的元素均转为float类型 字符串及编码python2中unicode编码下中文显示问题使用json对对象进行包装，再打印或者写入文件，如下 写入文件时，需要转化为UTF-8编码，如下 jsond = json.dumps(**, ensure_ascii=False) f.write(jsond.encode('utf-8')) ... 在python3中打印unicode编码字符串很简单，使用pprint；python2中也可以使用pprint，具体做法见这里。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/python-learning-notes-and-common-problems.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow中遇到的问题及解决方法]]></title>
    <url>%2Fproblems-with-solution-in-tensorflow.html</url>
    <content type="text"><![CDATA[本文记录一下自己在使用TensorFlow的过程中，遇到的较为困扰的问题及最终的解决方法。 Q1. 如何查看TensorFlow中Tensor, Variable, Constant的值？TensorFlow中的许多方法返回的都是一个Tensor对象。在Debug的过程中，我们发现只能看到Tensor对象的一些属性信息，无法查看Tensor具体的输出值；而对于Variable和Constant，我们很容易对其进行创建操作，但是如何得到它们的值呢？ 假设ts是我们想要查看的对象(Variable / Constant / 0输入的Tensor)，运行 ts_res = sess.run(ts) print(ts_res) 其中，sess为之前创建或默认的session. 运行后将得到一个narray格式的ts_res对象，通过print函数我们可以很方便的查看其中的内容。 但是，如果ts是一个有输入要求的Tensor，需要在查看其输出值前，填充(feed)输入数据。如下（假设ts只有一种输入）： input = ×××××× # the input data need to feed ts_res = sess.run(ts, feed_dict=input) print(ts_res) 其他要求多种输入的Tensor类似处理即可。 Q2. 模型训练完成后，如何获取模型的参数？模型训练完成后，通常会将模型参数存储于/checkpoint/×××.model文件(当然文件路径和文件名都可以更改，许多基于TensorFlow的开源包习惯将模型参数存储为model或者model.ckpt文件)。那么，在模型训练完成后，如何得到这些模型参数呢？ 需要以下两个步骤： Step 1: 通过tf.train.Saver()恢复模型参数 运行 saver = tf.train.Saver() 通过saver的restore()方法可以从本地的模型文件中恢复模型参数。大致做法如下： # your model's params # you don't have to initialize them x = tf.placeholder(tf.float32) y = tf.placeholder(tf.float32) W = tf.Variable(...) b = tf.Variable(...) y_ = tf.add(b, tf.matmul(x, w)) # create the saver saver = tf.train.Saver() # creat the session you used in the training processing # launch the model with tf.Session() as sess: # Restore variables from disk. saver.restore(sess, "/your/path/model.ckpt") print("Model restored.") # Do some work with the model, such as do a prediction pred = sess.run(y_, feed_dict={batch_x}) ... 有关TensorFlow中变量的创建、存储及恢复操作，详细见API文档. Step 2: 通过tf.trainable_variables()得到训练参数 tf.trainable_variables()方法将返回模型中所有可训练的参数，详细见API文档。类似于以下的变量参数不会被返回： tf_var = tf.Variable(0, name="××××××", trainable=False) 还可以通过Variable的name属性过滤出需要查看的参数，如下： var = [v for v in t_vars if v.name == "W"] （不断更新中…） 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/problems-with-solution-in-tensorflow.html 参考资料 Tensorflow: How to restore a previously saved model (python) Get original value of Tensor in Tensorflow Get the value of some weights in a model trained by TensorFlow]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记3：词向量]]></title>
    <url>%2Ftensorflow-learning-notes-3.html</url>
    <content type="text"><![CDATA[上篇博文讲了如何构建一个简单的CNN模型，并运行在MNIST数据集上。下面讲述一下如何在TensorFlow中生成词向量(Word Embedding)，使用的模型来自Mikolov et al。 本文的目录如下： 解释使用连续词向量的原因； 词向量模型的原理及训练过程； 在TensorFlow中实现模型的简单版本，并给出优化的方法； TensorFlow实现了两个版本的模型：简单版和正式版。如果想看源码的，可以直接下载。 为什么要使用Word Embedding在信号处理领域，图像和音频信号的输入往往是表示成高维度、密集的向量形式，在图像和音频的应用系统中，如何对输入信息进行编码(Encoding)显得非常重要和关键，这将直接决定了系统的质量。然而，在自然语言处理领域中，传统的做法是将词表示成离散的符号，例如将 [cat] 表示为 [Id537]，而 [dog] 表示为 [Id143]。这样做的缺点在于，没有提供足够的信息来体现词语之间的某种关联，例如尽管cat和dog不是同一个词，但是却应该有着某种的联系（如都是属于动物种类）。由于这种一元表示法(One-hot Representation)使得词向量过于稀疏，所以往往需要大量的语料数据才能训练出一个令人满意的模型。而Word Embedding技术则可以解决上述传统方法带来的问题。 向量空间模型(Vector space models, VSMs)将词语表示为一个连续的词向量，并且语义接近的词语对应的词向量在空间上也是接近的。VSMs在NLP中拥有很长的历史，但是所有的方法在某种程度上都是基于一种分布式假说，该假说的思想是如果两个词的上下文(context)相同，那么这两个词所表达的语义也是一样的；换言之，两个词的语义是否相同或相似，取决于两个词的上下文内容，上下文相同表示两个词是可以等价替换的。 基于分布式假说理论的词向量生成方法主要分两大类：计数法(count-based methods, e.g. Latent Semantic Analysis)和预测法(predictive methods, e.g. neural probabilistic language models)。Baroni等人详细论述了这两种方法的区别，简而言之，计数法是在大型语料中统计词语及邻近的词的共现频率，然后将之为每个词都映射为一个稠密的向量表示；预测法是直接利用词语的邻近词信息来得到预测词的词向量（词向量通常作为模型的训练参数）。 Wrod2vec是一个典型的预测模型，用于高效地学习Word Embedding。实现的模型有两种：连续词袋模型(CBOW)和Skip-Gram模型。算法上这两个模型是相似的，只不过CBOW是从输入的上下文信息来预测目标词(例如利用 [the cat sits on the] 来预测 [mat] )；而skip-gram模型则是相反的，从目标词来预测上下文信息。一般而言，这种方式上的区别使得CBOW模型更适合应用在小规模的数据集上，能够对很多的分布式信息进行平滑处理；而Skip-Gram模型则比较适合用于大规模的数据集上。 下面重点将介绍Skip-Gram模型。 噪声-对比(Noise-Contrastive)训练基于神经网络的概率语言模型通常都是使用最大似然估计的方法进行训练的，通过Softmax函数得到在前面出现的词语 \( h \) (history)的情况下，目标词 \( w_{t} \) (target)出现的最大概率，数学表达式如下： 其中，\( score(w_t, h) \) 为词 \(w_t\) 和上下文 \(h\) 的 [兼容程度]。上式的对数形式如下： 理论上可以根据这个来建立一个合理的模型，但是现实中目标函数的计算代价非常昂贵，这是因为在训练过程中的每一步，我们都需要计算词库 \(w’\) 中其他词在当前的上下文环境下出现的概率值，这导致计算量十分巨大。 然而，对于word2vec中的特征学习，可以不需要一个完整的概率模型。CBOW和Skip-Gram模型在输出端使用的是一个二分类器(即Logistic Regression)，来区分目标词和词库中其他的 \(k\) 个词。下面是一个CBOW模型的图示，对于Skip-Gram模型输入输出是倒置的。 此时，最大化的目标函数如下： 其中，\( Q_\theta(D=1 | w, h) \) 为二元逻辑回归的概率，具体为在数据集 \(D\) 中、输入的embedding vector \( \theta \)、上下文为 \( h \) 的情况下词语 \(w\) 出现的概率；公式后半部分为 \(k\) 个从 [噪声数据集] 中随机选择 \(k\) 个对立的词语出现概率(log形式)的期望值（即为Monte Carlo average）。 可以看出，目标函数的意义是显然的，即尽可能的 [分配(assign)] 高概率给真实的目标词，而低概率给其他 \( k \) 个 [噪声词]，这种技术称为负采样(Negative Sampling)。同时，该目标函数具有很好的数学意义：即在条件限制(训练时间)的情况下尽可能的逼近原有的Softmax函数（选择 \( k \) 个 [噪声点] 作为整个 [噪声数据] 的代表），这样做无疑能够大大提升模型训练的速度。实际中我们使用的是类似的噪声对比估计损失函数(noise-contrastive estimation (NCE))，在TensorFlow中对应的实现函数为tf.nn.nce_loss()。 下面看看具体是如何训练Skip-Gram模型的。 Skip-Gram模型举个例子，假设现在的数据集如下： the quick brown fox jumped over the lazy dog 这个数据集中包含了词语及其上下文信息。值得说明的是，上下文信息(Context)是一个比较宽泛的概念，有多种不同的理解：例如，词语周边的句法结构，词语的左边部分的若干个词语信息，对应的右半部分等。这里，我们使用最原始和基本的定义，即认为词语左右相邻的若干个词汇是该词对应的上下文信息。例如，取左右的词窗口为1，下面是数据集中的(上下文信息，对应的词)的pairs： ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ... Skip-Gram模型是通过输入的目标词来预测其对应的上下文信息，所以目标是通过[quick]来预测[the]和[brown]，通过[brown]来预测[quick]和[fox]… 将上面的pair转换为(input, output)的形式如下： (quick, the), (quick, brown), (brown, quick), (brown, fox), ... 目标函数定义如上，使用随机梯度下降算法(SGD)来进行最优化求解，并且使用mini-batch方法 (通常batch_size在16到512之间)。 下面将详细剖析一下训练过程。假设在训练的第 \(t\) 步，目标是得到上面第一个实例输入 [quick] 的输出预测；我们选择num_noise个 [噪声点数据]，简单起见，这里num_noise为1，假设选择 [sheep] 作为噪声对比词。那么，此时的目标函数值如下： 目标是更新embedding参数 \(\theta\) 以增大目标函数值，更新的方式是计算损失函数对参数 \(\theta\) 的导数，即 \( \frac{\partial}{\partial \theta} J_\text{NEG} \) (TensorFlow中有相应的函数以方便计算)，然后使得参数 \(\theta\) 朝梯度方向进行调整。当这个过程在训练数据集上执行多次后，产生的效果是使得输入的embedding vector的值发生改变，使得模型最终能够很好地区别目标词和 [噪声词]。 我们可以将学到的词向量进行降维(如t-SNE降维技术)和可视化，通过可视化发现连续的词向量能够捕捉到更多的语义和关联信息；有趣的是，在降维空间中某些特定的方向表征着特定的语义信息，例如下图中的[man-&gt;women]，[king-&gt;queen]方向表示性别关系(出自Mikolov et al., 2013)。 这也证实了连续词向量的作用，目前有非常多NLP中的任务(例如词性标注、命名实体识别等)都是使用连续词向量作为特征输入（更多可参考Collobert et al., 2011，Turian et al., 2010）。 下面看看具体在TensorFlow中，是如何实现模型的创建和训练的。 构建模型首先，我们要定义一下词嵌入矩阵(Embedding Matrix)，并随机初始化。 embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) 噪声-对比估计的损失函数在输出的逻辑回归模型中定义，为此，需要定义词库中每个词的权值和偏置参数(称为输出层权值参数)，如下： nce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocabulary_size])) 现在我们有了这些模型参数，接下来需要定义Skip-Gram模型。简单起见，假设我们已经将语料库中的词[整数化]，即每个词被表示为一个整数(具体见tensorflow/examples/tutorials/word2vec/word2vec_basic.py)。Skip-Gram模型有两种输入，都是整数形式表示：一种是批量的上下文词汇，一种是目标词。我们先为这些输入创建占位符(placeholder)，之后再进行数据的填充。 # Placeholders for inputs train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) 我们还需要能够查找(look up)batch中的输入词对应的vector，如下： embed = tf.nn.embedding_lookup(embeddings, train_inputs) 现在，我们有了每个词对应的embedding，接下来使用噪声-对比策略来预测目标词： # Compute the NCE loss, using a sample of the negative labels each time. loss = tf.reduce_mean( tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, num_sampled, vocabulary_size)) 现在，我们有了损失函数节点(loss node)，还需要利用随机梯度下降来进行优化，定义如下的优化器： # We use the SGD optimizer. optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) 模型的训练模型的训练方式很简单，只需要迭代地通过feed_dict进行训练数据的填充，并启动一个session。 for inputs, labels in generate_batch(...): feed_dict = {training_inputs: inputs, training_labels: labels} _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict) 完整的示例代码请参考tensorflow/examples/tutorials/word2vec/word2vec_basic.py。 Embedding的可视化模型训练结束后，我们利用t-SNE技术实现学习到的embedding可视化，如下图所示： 正如我们期望的那样，语义相似的词语会聚集在一起。关于word2vec更加高级的实现版本，可参考tensorflow/models/embedding/word2vec.py。 Embedding的评价：类比推理(Analogical Reasoning)Embedding在许多的NLP任务中都很有效果，那么如何评价Embedding的效果呢？一种简单的方式是，直接用来预测句法和语义的关联性，例如预测king is to queen as father is to ?，这称作Analogical Reasoning(By Mikolov and colleagues, 评价数据集可在这里下载)。 具体如何进行评价的，可以参考正式word2vec版本中的build_eval_graph()和eval()函数。 评价任务的准确性依赖于模型的超参数们，为了达到最佳的效果，往往需要将评价任务建立在一个巨大的数据集上，还可能需要使用一些trick，例如数据抽样、适当的fine tuning等。 进一步的优化以上的Vanilla版本展示了TensorFlow的简单易用。例如，只需要调用tf.nn.nce_loss()就可以替换tf.nn.sampled_softmax_loss()。如果你有关于损失函数的新想法，也可以自己在TensorFlow中手写一个，然后使用优化器计算导数并作优化。TensorFlow的简单易用，可以帮助你快速验证自己的想法。 一旦你有了一个令人满意的模型结构，你可以针对它进行优化使其更加高效。例如，原始版本中有个不足之处是，数据读取和填充是用Python实现的，因此会相对低效。你可以自己实现一个reader，参考数据格式要求。对于Skip-Gram模型，我们在这个版本中自定义了reader，可供参考。 如果你的模型在I/O上足够好了，但仍然想要提升效率，你可以自己编写TensorFlow Ops（参考这里）.优化版本中提供了示例。 总结这篇博文介绍了word2vec模型，一个用来高效学习出word embedding的模型。我们解释了为什么word embedding是有效的，讨论了如何更加高效地训练模型以及如何在TensorFlow中去实现。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/tensorflow-learning-notes-3.html 参考资料 TensorFlow: Vector Representation of Words 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(CNN)在句子建模上的应用]]></title>
    <url>%2Fcnn-apply-on-modelling-sentence.html</url>
    <content type="text"><![CDATA[之前的博文已经介绍了CNN的基本原理，本文将大概总结一下最近CNN在NLP中的句子建模（或者句子表示）方面的应用情况，主要阅读了以下的文献： Kim Y. Convolutional neural networks for sentence classification[J]. arXiv preprint arXiv:1408.5882, 2014. Kalchbrenner N, Grefenstette E, Blunsom P. A convolutional neural network for modelling sentences[J]. arXiv preprint arXiv:1404.2188, 2014. Hu B, Lu Z, Li H, et al. Convolutional neural network architectures for matching natural language sentences[C]//Advances in Neural Information Processing Systems. 2014: 2042-2050. He H, Gimpel K, Lin J. Multi-perspective sentence similarity modeling with convolutional neural networks[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015: 1576-1586. Wenpeng Yin, Hinrich Schütze. Convolutional Neural Network for Paraphrase Identification. The 2015 Conference of the North American Chapter of the Association for Computational Linguistics Zhang Y, Wallace B. A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification[J]. arXiv preprint arXiv:1510.03820, 2015. 下面对文献中CNN的结构和细节进行梳理。 Kim Y’s Paper模型结构及原理模型的结构如下： 说明如下： 输入层 如图所示，输入层是句子中的词语对应的word vector依次（从上到下）排列的矩阵，假设句子有 \( n \) 个词，vector的维数为 \( k \) ，那么这个矩阵就是 \( n × k \) 的。 这个矩阵的类型可以是静态的(static)，也可以是动态的(non static)。静态就是word vector是固定不变的，而动态则是在模型训练过程中，word vector也当做是可优化的参数，通常把反向误差传播导致word vector中值发生变化的这一过程称为Fine tune。 对于未登录词的vector，可以用0或者随机小的正数来填充。 第一层卷积层 输入层通过卷积操作得到若干个Feature Map，卷积窗口的大小为 \( h × k \) ，其中 \( h \) 表示纵向词语的个数，而 \( k \) 表示word vector的维数。通过这样一个大型的卷积窗口，将得到若干个列数为1的Feature Map。 池化层 接下来的池化层，文中用了一种称为Max-over-time Pooling的方法。这种方法就是简单地从之前一维的Feature Map中提出最大的值，文中解释最大值代表着最重要的信号。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。 最终池化层的输出为各个Feature Map的最大值们，即一个一维的向量。 全连接 + Softmax层 池化层的一维向量的输出通过全连接的方式，连接一个Softmax层，Softmax层可根据任务的需要设置（通常反映着最终类别上的概率分布）。 最终实现时，我们可以在倒数第二层的全连接部分上使用Dropout技术，即对全连接层上的权值参数给予L2正则化的限制。这样做的好处是防止隐藏层单元自适应（或者对称），从而减轻过拟合的程度。 实验部分1. 数据 实验用到的数据集如下（具体的名称和来源可以参考论文）： 2. 模型训练和调参 修正线性单元(Rectified linear units) 滤波器的h大小：3,4,5；对应的Feature Map的数量为100； Dropout率为0.5，L2正则化限制权值大小不超过3； mini-batch的大小为50； 这些参数的选择都是基于SST-2 dev数据集，通过网格搜索方法(Grid Search)得到的最优参数。另外，训练过程中采用随机梯度下降方法，基于shuffled mini-batches之上的，使用了Adadelta update rule(Zeiler, 2012)。 3. 预训练的Word Vector 这里的word vector使用的是公开的数据，即连续词袋模型(COW)在Google News上的训练结果。未登录次的vector值是随机初始化的。 4. 实验结果 实验结果如下图： 其中，前四个模型是上文中所提出的基本模型的各个变种： CNN-rand: 所有的word vector都是随机初始化的，同时当做训练过程中优化的参数； CNN-static: 所有的word vector直接使用无监督学习即Google的Word2Vector工具(COW模型)得到的结果，并且是固定不变的； CNN-non-static: 所有的word vector直接使用无监督学习即Google的Word2Vector工具(COW模型)得到的结果，但是会在训练过程中被Fine tuned； CNN-multichannel: CNN-static和CNN-non-static的混合版本，即两种类型的输入； 博主自己下载了论文作者的实现程序(Github地址)，最终在MR数据集上的运行结果如下： CNN-rand: 0.7669 CNN-static: 0.8076 CNN-non-static: 0.8151 和论文中的结果差不多。 5. 结论 CNN-static较与CNN-rand好，说明pre-training的word vector确实有较大的提升作用（这也难怪，因为pre-training的word vector显然利用了更大规模的文本数据信息）； CNN-non-static较于CNN-static大部分要好，说明适当的Fine tune也是有利的，是因为使得vectors更加贴近于具体的任务； CNN-multichannel较于CNN-single在小规模的数据集上有更好的表现，实际上CNN-multichannel体现了一种折中思想，即既不希望Fine tuned的vector距离原始值太远，但同时保留其一定的变化空间。 值得注意的是，static的vector和non-static的相比，有一些有意思的现象如下表格： 原始的word2vector训练结果中，bad对应的最相近词为good，原因是这两个词在句法上的使用是极其类似的（可以简单替换，不会出现语句毛病）；而在non-static的版本中，bad对应的最相近词为terrible，这是因为在Fune tune的过程中，vector的值发生改变从而更加贴切数据集（是一个情感分类的数据集），所以在情感表达的角度这两个词会更加接近； 句子中的!最接近一些表达形式较为激进的词汇，如lush等；而,则接近于一些连接词，这和我们的主观感受也是相符的。 Kim Y的这个模型很简单，但是却有着很好的性能。后续Denny用TensorFlow实现了这个模型的简单版本，可参考这篇博文；以及Ye Zhang等人对这个模型进行了大量的实验，并给出了调参的建议，可参考这篇论文。 下面总结一下Ye Zhang等人基于Kim Y的模型做了大量的调参实验之后的结论： 由于模型训练过程中的随机性因素，如随机初始化的权重参数，mini-batch，随机梯度下降优化算法等，造成模型在数据集上的结果有一定的浮动，如准确率(accuracy)能达到1.5%的浮动，而AUC则有3.4%的浮动； 词向量是使用word2vec还是GloVe，对实验结果有一定的影响，具体哪个更好依赖于任务本身； Filter的大小对模型性能有较大的影响，并且Filter的参数应该是可以更新的； Feature Map的数量也有一定影响，但是需要兼顾模型的训练效率； 1-max pooling的方式已经足够好了，相比于其他的pooling方式而言； 正则化的作用微乎其微。 Ye Zhang等人给予模型调参者的建议如下： 使用non-static版本的word2vec或者GloVe要比单纯的one-hot representation取得的效果好得多； 为了找到最优的过滤器(Filter)大小，可以使用线性搜索的方法。通常过滤器的大小范围在1-10之间，当然对于长句，使用更大的过滤器也是有必要的； Feature Map的数量在100-600之间； 可以尽量多尝试激活函数，实验发现ReLU和tanh两种激活函数表现较佳； 使用简单的1-max pooling就已经足够了，可以没必要设置太复杂的pooling方式； 当发现增加Feature Map的数量使得模型的性能下降时，可以考虑增大正则的力度，如调高dropout的概率； 为了检验模型的性能水平，多次反复的交叉验证是必要的，这可以确保模型的高性能并不是偶然。 论文附录中还附上了各种调参结果，感兴趣的可以前往阅读之。 Kalchbrenner’s PaperKal的这篇文章引用次数较高，他提出了一种名为DCNN(Dynamic Convolutional Neural Network)的网络模型，在上一篇（Kim’s Paper）中的实验结果部分也验证了这种模型的有效性。这个模型的精妙之处在于Pooling的方式，使用了一种称为动态Pooling的方法。 下图是这个模型对句子语义建模的过程，可以看到底层通过组合邻近的词语信息，逐步向上传递，上层则又组合新的Phrase信息，从而使得句子中即使相离较远的词语也有交互行为（或者某种语义联系）。从直观上来看，这个模型能够通过词语的组合，提取出句子中重要的语义信息（通过Pooling），某种意义上来说，层次结构的feature graph的作用类似于一棵语法解析树。 DCNN能够处理可变长度的输入，网络中包含两种类型的层，分别是一维的卷积层和动态k-max的池化层(Dynamic k-max pooling)。其中，动态k-max池化是最大化池化更一般的形式。之前LeCun将CNN的池化操作定义为一种非线性的抽样方式，返回一堆数中的最大值，原话如下： The max pooling operator is a non-linear subsampling function that returns the maximum of a set of values (LuCun et al., 1998). 而文中的k-max pooling方式的一般化体现在： pooling的结果不是返回一个最大值，而是返回k组最大值，这些最大值是原输入的一个子序列； pooling中的参数k可以是一个动态函数，具体的值依赖于输入或者网络的其他参数； 模型结构及原理DCNN的网络结构如下图： 网络中的卷积层使用了一种称之为宽卷积(Wide Convolution)的方式，紧接着是动态的k-max池化层。中间卷积层的输出即Feature Map的大小会根据输入句子的长度而变化。下面讲解一下这些操作的具体细节： 1. 宽卷积 相比于传统的卷积操作，宽卷积的输出的Feature Map的宽度(width)会更宽，原因是卷积窗口并不需要覆盖所有的输入值，也可以是部分输入值（可以认为此时其余的输入值为0，即填充0）。如下图所示： 图中的右图即表示宽卷积的计算过程，当计算第一个节点即\( s_1 \)时，可以假使\( s_1 \)节点前面有四个输入值为0的节点参与卷积（卷积窗口为5）。明显看出，狭义上的卷积输出结果是宽卷积输出结果的一个子集。 2. k-max池化 给出数学形式化的表述是，给定一个\( k \)值，和一个序列\( p \in R^p \)(其中\( p ≥ k \))，k-max pooling选择了序列\( p \)中的前\( k \)个最大值，这些最大值保留原来序列的次序（实际上是原序列的一个子序列）。 k-max pooling的好处在于，既提取除了句子中的较重要信息（不止一个），同时保留了它们的次序信息（相对位置）。同时，由于应用在最后的卷积层上只需要提取出\( k \)个值，所以这种方法允许不同长度的输入（输入的长度应该要大于\( k \)）。然而，对于中间的卷积层而言，池化的参数\( k \)不是固定的，具体的选择方法见下面的介绍。 3. 动态k-max池化 动态k-max池化操作，其中的\( k \)是输入句子长度和网络深度两个参数的函数，具体如下： $$ K_{l}=\max \left( k_{top}, \left \lceil \frac {L-l}{L} s \right \rceil \right) $$ 其中\( l \)表示当前卷积的层数（即第几个卷积层），\( L \)是网络中总共卷积层的层数；\( k_{top} \)为最顶层的卷积层pooling对应的\( k \)值，是一个固定的值。举个例子，例如网络中有三个卷积层，\( k_{top} = 3\)，输入的句子长度为18；那么，对于第一层卷积层下面的pooling参数\( k_{1} = 12\)，而第二层卷积层对于的为\( k_{2} = 6\)，而\( k_{3} = k_{top} = 3\)。 动态k-max池化的意义在于，从不同长度的句子中提取出相应数量的语义特征信息，以保证后续的卷积层的统一性。 4. 非线性特征函数 pooling层与下一个卷积层之间，是通过与一些权值参数相乘后，加上某个偏置参数而来的，这与传统的CNN模型是一样的。 5. 多个Feature Map 和传统的CNN一样，会提出多个Feature Map以保证提取特征的多样性。 6. 折叠操作(Folding) 之前的宽卷积是在输入矩阵\( d × s \)中的每一行内进行计算操作，其中\(d\)是word vector的维数，\(s\)是输入句子的词语数量。而Folding操作则是考虑相邻的两行之间的某种联系，方式也很简单，就是将两行的vector相加；该操作没有增加参数数量，但是提前（在最后的全连接层之前）考虑了特征矩阵中行与行之间的某种关联。 模型的特点 保留了句子中词序信息和词语之间的相对位置； 宽卷积的结果是传统卷积的一个扩展，某种意义上，也是n-gram的一个扩展； 模型不需要任何的先验知识，例如句法依存树等，并且模型考虑了句子中相隔较远的词语之间的语义信息； 实验部分1. 模型训练及参数 输出层是一个类别概率分布（即softmax），与倒数第二层全连接； 代价函数为交叉熵，训练目标是最小化代价函数； L2正则化； 优化方法：mini-batch + gradient-based (使用Adagrad update rule, Duchi et al., 2011) 2. 实验结果 在三个数据集上进行了实验，分别是(1)电影评论数据集上的情感识别，(2)TREC问题分类，以及(3)Twitter数据集上的情感识别。结果如下图： 可以看出，DCNN的性能非常好，几乎不逊色于传统的模型；而且，DCNN的好处在于不需要任何的先验信息输入，也不需要构造非常复杂的人工特征。 Hu’s Paper模型结构与原理1. 基于CNN的句子建模 这篇论文主要针对的是句子匹配(Sentence Matching)的问题，但是基础问题仍然是句子建模。首先，文中提出了一种基于CNN的句子建模网络，如下图： 图中灰色的部分表示对于长度较短的句子，其后面不足的部分填充的全是0值(Zero Padding)。可以看出，模型解决不同长度句子输入的方法是规定一个最大的可输入句子长度，然后长度不够的部分进行0值的填充；图中的卷积计算和传统的CNN卷积计算无异，而池化则是使用Max-Pooling。 卷积结构的分析 下图示意性地说明了卷积结构的作用，作者认为卷积的作用是从句子中提取出局部的语义组合信息，而多张Feature Map则是从多种角度进行提取，也就是保证提取的语义组合的多样性；而池化的作用是对多种语义组合进行选择，过滤掉一些置信度低的组合（可能这样的组合语义上并无意义）。 2. 基于CNN的句子匹配模型 下面是基于之前的句子模型，建立的两种用于两个句子的匹配模型。 2.1 结构I 模型结构如下图： 简单来说，首先分别单独地对两个句子进行建模（使用上文中的句子模型），从而得到两个相同且固定长度的向量，向量表示句子经过建模后抽象得来的特征信息；然后，将这两个向量作为一个多层感知机(MLP)的输入，最后计算匹配的分数。 这个模型比较简单，但是有一个较大的缺点：两个句子在建模过程中是完全独立的，没有任何交互行为，一直到最后生成抽象的向量表示后才有交互行为（一起作为下一个模型的输入），这样做使得句子在抽象建模的过程中会丧失很多语义细节，同时过早地失去了句子间语义交互计算的机会。因此，推出了第二种模型结构。 2.2 结构II 模型结构如下图： 图中可以看出，这种结构提前了两个句子间的交互行为。 第一层卷积层 第一层中，首先取一个固定的卷积窗口\( k1 \)，然后遍历 \( S_{x} \) 和 \( S_{y} \) 中所有组合的二维矩阵进行卷积，每一个二维矩阵输出一个值（文中把这个称作为一维卷积，因为实际上是把组合中所有词语的vector排成一行进行的卷积计算），构成Layer-2。下面给出数学形式化表述： 第一层卷积层后的Max-Pooling层 从而得到Layer-2，然后进行2×2的Max-pooling： 后续的卷积层 后续的卷积层均是传统的二维卷积操作，形式化表述如下： 二维卷积结果后的Pooling层 与第一层卷积层后的简单Max-Pooling方式不同，后续的卷积层的Pooling是一种动态Pooling方法，这种方法来源于参考文献[1]。 结构II的性质 保留了词序信息； 更具一般性，实际上结构I是结构II的一种特殊情况（取消指定的权值参数）； 实验部分1. 模型训练及参数 使用基于排序的自定义损失函数(Ranking-based Loss) BP反向传播+随机梯度下降； mini-batch为100-200,并行化； 为了防止过拟合，对于中型和大型数据集，会提前停止模型训练；而对于小型数据集，还会使用Dropout策略； Word2Vector：50维；英文语料为Wikipedia(~1B words)，中文语料为微博数据(~300M words)； 使用ReLu函数作为激活函数； 卷积窗口为3-word window； 使用Fine tuning； 2. 实验结果 一共做了三个实验，分别是(1)句子自动填充任务，(2)推文与评论的匹配，以及(3)同义句识别；结果如下面的图示： 其实结构I和结构II的结果相差不大，结构II稍好一些；而相比于其他的模型而言，结构I和结构II的优势还是较大的。 He’s Paper第四篇论文即He的文章中所提出的模型，是所有基于NN的模型中，在Paraphrase identification任务标准数据集MSRP上效果最佳的。下面我们来学习一下这个模型。 模型结构与原理模型主要分为两个部分： 句子的表征模型：得到句子的表征(representation)，以供后续的相似度计算； 相似度计算模型：使用多种相似度计算方法，针对句子表征后的局部进行相应的计算； 模型不需要借助WordNet, 句法解析树等资源；但是可以选择性地使用词性标注、word embedding等方法来增强模型的性能；与之前的模型区别在于，文中的模型使用了多种类型的卷积、池化方法，以及针对得到的句子表征的局部进行相应的相似度计算。（这样做的优点在于能够更加充分地挖掘出句子中的特征信息，从而提升性能，但同时使得模型变得复杂、耗时） 模型的整体框架如下： 下面具体看看这两个模型是如何实现的。 句子的表征模型 模型是基于CNN的，卷积层有两种卷积方式，池化层则有三种。 卷积层 假设模型的输入为二维矩阵 \( Sent \)，\( Sent \in R^{len×Dim} \)，其中 \(len\) 表示句子切分为Token List后的长度(Token可以是词/字)，\(Dim\) 表示Token的Embedding表示的维度。由此有 \(Sent_{i}\) 表示矩阵的第 \(i\) 行，即输入中的第 \(i\) 个Token的Embedding表示；\(Sent_{i:j}\) 表示矩阵中的第 \(i\) 到第 \(j\) 行的一个切片，也是一个子矩阵；\(Sent_{i}^{[k]}\) 表示矩阵的第 \(i\) 行第 \(k\) 列的值，对应是Embedding的第 \(k\) 个值；而 \(Sent_{i:j}^{[k]}\) 则是矩阵中第 \(i\) 行到第 \(j\) 行中的第 \(k\) 列的一个切片。 卷积层有两种卷积的方式：(1)粒度为word的卷积;(2)粒度为embedding 维度上的卷积。如下图： 其中，第一种卷积方式与之前的Kim Y提出模型中的相同，相当于是n-gram特征的抽取；而对于第二种卷积方式，论文作者给出的解释是，(1)这种方式有助于充分地提取出输入的特征信息；(2)由于粒度更小，所以在学习过程中的参数调整上，每一个维度能够得到不同程度的参数调整。（第二种卷积方式从直观上没有太多的物理意义，而作者也是直说不能够给出符合人直观想法上的解释）。 池化层 模型除了使用传统的max-pooling，还使用了min-pooling和mean-pooling方式。 假设 \(group(ws, pooling, sent)\) 表示卷积宽度为 \(ws\)，使用 \(pooling\) 池化函数，应用在输入的句子 \(sent\) 上。我们使用了两种类型的building block，分别是 \(block_{A}\) 和 \(block_{B}\) 上，定义如下 $$ block_{A} = \lbrace group_{A}(ws_{a}, p, sent): p \in {max, min, mean} \rbrace $$ 这里 \(block_{A}\) 有三组卷积层，卷积窗口的宽度一致(都是 \(ws_{a}\) )，每一组对应一种池化操作。这里池化操作和卷积层是一一对应的，也就是说并不是一个卷积层上实施三种池化操作(虽然也可以这么做，作者没有这么做的原因是由于激活函数的存在，对每个卷积结果都进行max-pooling和min-pooling是没有必要的)。 而 \(block_{B}\) 的定义如下： $$ block_{B} = \lbrace group_{B}(ws_{b}, p, sent): p \in {max, min} \rbrace $$ 这里 \(block_{B}\) 有两组卷积层，卷积窗口的宽度为 \(ws_{b}\)，两组分别对应max-pooling和min-pooling的操作。值得说明的是，\(group_{B}(*)\) 中的卷积层对应有 \(Dim\) 个以embedding dimension为粒度的卷积窗口，也就是对embedding的每一维度做卷积运算。 这里只所以要组合这些多样的卷积和池化操作，原因是希望能够从多个方面来提取出输入中的特征信息，以供后续的决策任务。 多种窗口尺寸 与传统的n-gram模型相似，这里在building block中使用了多种尺寸的卷积窗口。如下图所示： 其中 \(ws\) 表示卷积时卷积的n-gram长度，而 \(ws=\infty\) 表示卷积窗口为整个word embedding矩阵。\(ws\) 的值及Feature Map 的数量都是需要调参的。 相似度计算模型 下面介绍在得到句子的表征向量之后，如何计算它们的相似度。直观的想法是，我们可以使用传统的相似度计算方法如余弦相似度等来计算两个句子向量的相似度。但是，直接应用这种做法在两个句子向量上并不是最优的，原因在于最后生成的句子向量中的每一个部分的意义各不相同，这样简单粗暴的计算势必会影响效果，所以做法是对句子向量中的各个部分进行相应的比较和计算(Structured Comparision)。为了使得句子向量中的局部间的比较和计算更加有效，我们需要考虑如下方面： (1) 是否来自相同的building block；(2) 是否来自相同卷积窗口大小下的卷积结果；(3) 是否来自相同的pooling层；(4) 是否来自相同的Feature Map； 最终比较句子中的相应部分时，需要至少满足以上两个条件。为了识别句子中的哪些对应部分需要参与到相似度计算，文中提供了两种算法。 2.1. 相似度计算单元(Unit) 两种相似度计算单元如下： 2.2. 基于句子局部的相似度计算 算法1和算法2为句子表征向量的两种计算方法，其中算法1仅用在 \(block_{A}\) 上；而算法2则都用在 \(block_{A}\) 和 \(block_{B}\) 上，两种算法都是针对相同类型(pooling和block类型)的输出做局部比较。 给出如下的符号假设： 算法的伪代码如下： 下面的图示说明了在 \(block_{A}\) 上，两种算法的计算方式的区别，算法一表现了向量在水平方向上的比较；而算法二则是在垂直方向。 需要注意的是，在算法二中相同类型的pooling的输出groups中，向量是两两进行比较的（图中的红色虚线只是为了说明比较的方向，并不是只针对group中相同大小的卷积窗口作比较）；而算法一中的每一行都要作比较，不仅仅是第一行。 模型的其他细节 相似度向量输出 + 全连接层 基于句子局部的相似度计算之后，得到相应的相似度向量；然后这组向量之后连接一个全连接层，最后softmax对应输出。如果是计算相似度度量值，可以用softmax输出的类别概率值。 激活函数 使用tanh函数作为激活函数。 实验部分 实验数据集 Microsoft Research Paraphrase Corpus (MSRP) 用于评测同义句检测 (Paraphrase Identification) 任务的经典数据集，数据集来源于新闻；包含5801对句子对，其中4076对用于模型训练，而1725对用于测试；每一对句子拥有一个标签，0或者1,0表示两个句子不是互为同义句，而1则表示两个句子互为同义句。因此这是一个二分类的任务。 Sentences Involving Compositional Knowledge (SICK) 数据来源于2014年SemEval比赛，数据集有9927对句子对，其中4500对用于模型训练，500对用于模型验证，而剩下的4927对用于模型测试。这些句子都是在图片和视频描述中抽取得到的，每一对句子对有一个相关分数，区间在[1, 5]，分数越高表示句子越相关。 Microsoft Video Paraphrase Corpus (MSRVID) 数据集来源于2012年的SemEval比赛，包含1500对短文本（用于描述视频信息）。其中一般用于模型训练，一半用于模型测试，每一对句子有一个相关性分数，区间在[0, 5]，分数越高表示句子越相关。 模型训练 针对MSRP和其他两个数据集，分别使用两种损失函数。对于MSRP数据集，损失函数（Hinge Loss）如下： 对于其余两个数据集，损失函数（KL-divergence Loss）如下： 实验参数设置 \(ws\) 的值：\(ws \in [1, 3]\)和 \(ws=\infty\). Word Embedding: 300维的GloVe word embedding；对于MSRP数据集，还额外使用了200维的POS embedding (Standford POS tagger)和25维的Paragram Vectors (Wieting et al., 2015 PDF，数据下载地址)。因此对于MSRP任务而言，word embedding的维数为525维 (200+300+25)；而其余两个任务则对应是300维。 在MSRP上使用了5-折交叉验证的方式，对模型参数进行tuning. Tuning好的模型参数将会用在另外两个数据集任务上。 只有在MSRP数据集任务上，允许模型参数进行更新。 输出的全连接层，MSRP有250个神经元节点，而SICK和MSRVID则是150个。 在 \(block_{A}\) 中，Feature Map 的数量与输入的embedding维数相同，即MSRP是525个，而SICK和MSRVID则是300个。 优化算法使用随机梯度下降方法。 学习率为0.01，而正则化参数 \(\lambda=10^{-4}\). 实验结果 MSRP数据集 可以看出，文中的模型是所有基于NN的方法中在MSRP数据集上性能最好的。 SICK数据集 MSRVID数据集 而模型在SICK和MSRVID数据集上的表现也很好。 模型的敏感度分析 下面的表格说明了在不使用某种技术下，模型性能在实验数据集上的变化情况。 从中可以得出以下结论： 对于MSRP数据集任务而言，增加POS Embedding和Paragram Vector效果显著； 移除相似度计算层的影响显著，说明结构化的句子局部比较方法是有效且必要的； Horizontal和Vertical算法均有一定的提升效果，而Vertical算法的提升程度更高； max-pooling方式确实要比min-pooling和mean-pooling强太多。 总结 文中的模型包含两个部分：卷积-池化模型和相似度计算模型。实验部分已经验证了模型的有效性，在MSRP数据集上模型取得了仅次于state-of-art的结果，并且在基于NN的方法中是最好的。模型中的相似度计算层是有必要的，因为对卷积池化处理后的句子成分进行了针对性的比较，从直观上要比直接扔进全连接层更合理，而实验结果也表明了这一点。 然而，个人觉得，文中的模型结构较为复杂，而且其中有很多trick的地方，比如为什么要对word embedding中的每一维度做卷积，\(block_{B}\) 中的pooling方式为什么只用了max和min，不用mean的方式等问题，而这些方式或许是作者自己做了大量实验后，从果到因而使用的。 Yin’s PaperYin的这篇论文提出了一种叫Bi-CNN-MI的架构，其中Bi-CNN表示两个使用Siamese框架的CNN模型；MI表示多粒度的交互特征。Bi-CNN-MI包含三个部分： 句子分析模型 (CNN-SM) 这部分模型主要使用了上述Kal在2014年提出的模型，针对句子本身提取出四种粒度的特征表示：词、短ngram、长ngram和句子粒度。多种粒度的特征表示是非常必要的，一方面提高模型的性能，另一方面增强模型的鲁棒性。 句子交互计算模型 (CNN-IM) 这部分模型主要是基于2011年Socher提出的RAE模型，做了一些简化，即仅对同一种粒度下的提取特征做两两比较。 LR或Softmax网络层以适配任务 模型结构论文提出的模型主要是基于Kal的模型及Socher的RAE模型的结合体，如下图： 通过模型图可以看出模型的主要思想：一方面利用Kal的模型进行多种粒度上的特征提取，另一方面采取RAE模型的思想，对提取出来的特征进行两两的相似度计算，计算完成的结果通过dynamic pooling的方式进一步提取少量特征，然后各个层次的pooling计算结果平摊为一组向量，通过全连接的方式与LR(或者softmax)层连接，从而适配同义句检测任务本身。 这个模型具体的计算细节不再赘述了，感兴趣的读者可以直接去看论文。除了提出这种模型结构之外，论文还有一个亮点在于使用了一种类似于语言模型的CNN-LM来对上述CNN部分的模型进行预训练，从而提前确定模型的参数。CNN-LM的网络结构如下图： CNN-LM模型的训练预料使用了最终的实验数据集，即MSRP；另外，由于MSRP的数据规模较小，所以作者又增加了100,000个英文句子语料。CNN-LM模型最终能够得到word embedding, 模型权值等参数。需要注意的是，这些参数并不是固定的，在之后的句子匹配任务中是会不断更新的。从后面的实验结果中可以看出，CNN-LM的作用是显著的。 实验结果论文仅使用了一种数据集，即公认的PI (Paraphrase Identification)任务数据集，MSRP。实验结果如下： 可以看出，CNN-LM的预训练效果显著，预训练后的模型性能很强（但是结果上比之前He提出的模型稍差一些）。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html 参考文献 [1] R. Socher, E. H. Huang, and A. Y. Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in NIPS, 2011. 推荐资料 A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification Implementing a CNN for Text Classification in TensorFlow Kim Y’s Implement: Convolutional Neural Networks for Sentence Classification 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>NLP</tag>
        <tag>Sentence Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词向量表示技术(Word Representation)介绍]]></title>
    <url>%2Fintroduction-to-word-representation.html</url>
    <content type="text"><![CDATA[先刨个坑，以后来填:) 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/introduction-to-word-representation.html 参考资料 Deep Learning in NLP （一）词向量和语言模型 博士论文《基于神经网络的词和文档语义向量表示方法研究》 《How to Generate a Good Word Embedding?》导读 Deep Learning实战之word2vec]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>Word Embedding</tag>
        <tag>Word2Vector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015年校招总结：技术面试干货]]></title>
    <url>%2F2015-campus-recurit-technology-interview-summary.html</url>
    <content type="text"><![CDATA[关于实习及校招的全面概括性总结在这篇博文，里面也提出了一些技术面试过程中的注意事项。本文主要是单纯针对程序员技术面试的面试内容，将（1）推荐一些优秀的资源（包括书籍、网站等），以及（2）总结一下自己及周遭同学在实习与校招技术面试过程中遇到的各种原题，以供后人参考。 推荐资源书籍算法类 《Crack the code interview》(Gayle Laakmann著) [PDF下载地址] 这本书是经典的程序员技术面试必备书了，作者是曾经的Google面试官，从面试官的角度教你应该如何一步步地准备面试。书中分析了硅谷的一些巨头公司的面试风格和特点，对于想要面国外公司的再合适不过了；还帮助你制定了面试准备的流程和计划，给出写简历的建议，如何应对行为面试(Behavioral Interview)等；当然，最主要的篇幅集中在技术面试的准备中，总结了常见的数据结构及相应的算法题，数理概率，及一些其他面试中常见的技术题型。 《进军硅谷：程序员面试揭秘》(陈东锋著) [豆瓣地址] 尽管这本书在豆瓣上的评分很低（leetcode作者认为该书抄袭了leetcode上的题目…），但对于面试者来说，这本书还是值得推荐的。这本书前面部分也是主要介绍了一下面试流程和注意事项，硅谷公司的特点；其余的大篇幅都是集中在算法题的解题思路分析和代码实现，确实大部分的算法题与leetcode上的一样，所以刷leetcode的时候配合这本书，应该会顺畅挺多的。这本书的代码都是Java，简单易懂。 《剑指Offer》(何海涛著) [PDF下载地址] 这本书的结构其实与前两本比较类似，但是有一个亮点是，对于所有的算法题都会给出测试样例，包括特殊边界和正常功能测试样例等。写算法题能够提前考虑测试样例是非常好的编程习惯，称之为防御式编程；大多数人都是习惯写完代码后，再进行样例测试，然后修修补补之类的。 《微软面试100题系列》(July著) [PDF下载地址] 严格上来说，这个并不是一本正式的书籍。但是这个资料里收集了许多经典真实的企业面试题。题型比较杂，大部分是算法题，还有智力题等。虽然答案不是很全，但是值得好好看看里面的题，从本人的笔试面试经历来看，遇到了里面挺多的原题~ 《编程之美：微软技术面试心得》[PDF下载地址] 如果时间充裕的话，这本书也可一看。这本书是由MSRA的一些FTE和实习生们编写的，老实说，这本书中很多题还是挺有难度的，有许多数学相关的题，不折不扣地考验你的智商……偶尔翻翻，转转脑子也挺好的。 此外，还有一些神书，例如《算法导论》《编程珠玑》也可一看。但是，时间总是有限的，认真刷刷1-2本书，然后多动手配合刷题（刷题平台下面有推荐），应付面试的算法能力自然会慢慢变强。 数据结构类 《Java数据结构和算法》(Robert Lafore著) [PDF下载地址] 相比起清华的严奶奶那本，这本书通俗易懂得多:)要是觉得之前的数据结构掌握的不够好，这本书绝对能拉你入门~ 《数据结构：C语言版》(严蔚敏著) [PDF下载地址] 虽然刚学的时候觉得晦涩难懂，但是还是国内经典的书籍，对数据结构研究的比较深刻，内容较上本会丰富很多。 编程语言类 Java 《Java编程思想》(Bruce Eckel著) [PDF下载地址] 《Effective Java》(Joshua Bloch著)(中文版) [PDF下载地址] | (英文版) [PDF下载地址] 《疯狂Java讲义》(李刚著) [PDF下载地址] C 《C Primer Plus》(Stephen Prata著) [PDF下载地址] 《征服C指针》(前桥和弥著) [PDF下载地址] Python 《Python基础教程》(Magnus Lie Hetland著) [PDF下载地址] 《Python简明教程》(Swaroop著) [PDF下载地址] 《利用Python进行数据分析》(Wes McKinney著) [PDF下载地址] 《Learn Python The Hard Way》[PDF下载地址] 数据库类《SQL必知必会》(Ben Forta著) [PDF下载地址] 《深入浅出SQL》(Lynn Beighley著) [PDF下载地址] 《高性能MySQL》(Baron Schwarlz等著) [PDF下载地址] 刷题网站 Leetcode 众所周知的刷题网站了，许多公司的面试题都是从里面出的。建议刷3遍左右。 Lintcode 一个类似于leetcode的刷题网站，但是比起leetcode，里面的题目更加齐全。还有一些特色的功能，如限时提交，编程风格检测等。 九度OJ 里面收录了《剑指Offer》中的题，可以配合看书练习。还有一些考研机试、比赛类型的题，适合刷完leetcode等网站后，磨练算法能力。 hihoCoder 这个平台经常举办一些编程比赛，一些公司的笔试会选择在这个平台进行，例如微软(中国)、网易游戏等。另外，这个平台里面的题有一定难度，适合算法能力中上的人。 网站与论坛 九章算法 曾经上过它的算法课，还可以。里面有leetcode中大多数题的解答（只有代码，大多数是Java），还有一些面筋之类的分享。有时间和米的还可以去听听他家的课，都是PPT+白板+语音的形式。 GeeksforGeeks Career Cup 以上这两个网站上面有很多国外最近的、真实的面试题分享和讨论，也可以经常去水水~另外，这个知乎问题，票数第一的回答还总结了挺多的。 July的cnblog 这个博主总结了之前提及的《微软面试100题系列》，有个教你如何迅速秒杀99%的海量数据处理面试题也写得不错，基本足够应付面试中遇到的大数据相关的题。 我的cnblog 之前在面试准备过程中，在cnblog上建了个博客，记录了以下刷的算法题及面试题。欢迎访问。 真实面筋算法和数据结构 Google SED实习 非递归实现二叉树深度的求解 如何实现双端队列 阿里春招实习 一维的连连看实现 动归和贪心的区别 大小为999的一维数组，按序存放着1-1000的数字，但有一个数字缺失，找到它 最长公共子序列 一个排序的数组, 如何做压缩? 三个排序数组, 找到同时存在于三个数组中的元素 three sum 判断链表中有环 环的大小 写个KMP 给定一些金币，金币的数额都是2的整数幂，然后每种硬币都有两枚，然后给定一个数额，求可行的组合方式（重复不算）[动态规划] 给定一个m*n的长方形，然后每次对长方形进行分割，分割的直线均平行于长方形的边，而且落在长方形内，给定一系列有顺序的分割直线，问每次分割后形成的所有小长方形中面积最大的。[最小堆] Leetcode: Excel Sheet Column Title 给定N个骰子, 每一面上有一个字母. 给定一个长度为M的单词. 问这些骰子您不能拼出这个单词. 每个骰子只能用一次, 顺序随便排. 两个骰子, 每个骰子各个面上可以放不同数字(自己安排数字), 问能否组成1~31. 如果扩展为N个骰子, 能组成的最长的连续数字(从1开始)是多大. 数组表示的数, 实现一个加一函数. 字符串中找到字母数不超过M的最长子串. Hulu实习 判断一颗二叉树是否完全二叉树 给一个整数数组，找其中的连续子序列，使得字段和的绝对值最小 给一个单链表，写个快排 给一个值已排序的双链表，双链表存储在一个Node*数组里，每个元素是一个指向双链表某个节点的指针。现在只有一次查询x，找到x在双链表中的位置或报告找不到 整数数组，找到满足f(j) &gt; f(i) 最大的（j - i） 一个数组两种操作，1）修改数组中的一个值，2）计算数组某个子区间所有数字之和。写个线段树？ 一个矩形格子区域，每个格子上有一个数字，还有红蓝2中颜色其中之一，初始从某个位置开始，问能否找到一条能够走到边界外的路线，这条路线要满足，1）数字大小严格递增，2）红蓝两色至少各出现一次。（搜索？我先说宽搜，后来想想只要找一条就行那就深搜。这里貌似可以记忆化一下存储一些信息，可惜当时并没有很清晰地考虑清楚，而且时间不很足够的样子了。写代码。） 写代码判断一棵树是否是完全二叉树 一个长度为n的数组，求最小连续子段和，求绝对值最小的连续子段和，求绝对值最接近某个数的连续子段和 蛇形打印矩阵 n个数字，a1,a2…an，数字之间可以添加+、*、括号变成一个表达式，求表达式的最大值 有一个字符串流，里面是一些单词和空格。给一个api：read_char()，每次调用将在流中读取一个字符，如果遇到流的结尾，则返回0。请设计函数print_stream()，通过调用read_char()打印流中的单词，要求，每行长度不超过M，一个单词不能跨越两行，单词之间只保留1个空格，删除首尾空格。 n个元素的数组，设计算法找出现次数大于n/3的元素，要求时空复杂度尽可能小 有道实习 判断2个url是否是同一个网站的。比如news.sina.com.cn/asdasd和car.sina.com.cn/asdasd/asdasd就是同一个网站的. 给定整数b，求最大的a，满足a*(a+b)是完全平方数 给定一个棋盘，马初始在(0,0)，棋盘上有些点为禁行点，用*表示。另外棋盘上有个兵，兵的移动路线已知，每次移一步，在棋盘上用1,2,3….表示出。要求计算马最少跳多少步能把兵吃掉。 豌豆荚 实现atoi函数 码镜像二叉树 找最后一个出现的字符串匹配 求树的深度 高精度加法 网易游戏实习 m个数中找最小的n个 删除链表的某个节点 无向普通图G中找两个点最短路径 大数据 阿里春招实习 集合A：40亿个未排序，不重复的unsigned int；集合B：1万个unsined int；判断集合B中的数是否属于集合A。（输出1W个bool值） 1亿个查询记录，找出frequency top1000；follow up：讲解堆的调整过程。 一个100G的文件, 存放搜索的关键词, 统计其中出现最多的20个 现在有淘宝的登录日志5亿条, 支付宝登录日志3亿条, 假设账号最多20个汉字. 找到所有两个都登陆过的账号. (哈希表使用的具体数据结构, 冲突如何处理, 分析下分解之后文件应该有多大才能保证内存能装下, 重复条目处理) 100万个数字, 没有重复的有序数组, 有什么办法压缩大小. Hadoop题：一个表每一行是一个key和许多value，有另一个表，记录着value到value’的映射。问题1）若第二个表不很大，写个hadoop 2）若第二个表很大，写个hadoop。 1亿条搜索输入文本记录，找到频率topN条，写代码。 数据挖掘和机器学习 阿里春招实习 如何识别买家评价的虚假评价 SVM特征怎么提的，参数怎么调的，半监督学习是在干嘛，整体学习是在干嘛? 讲解CNN，CNN和DNN相比有什么优点为什么用它? 随机森林和决策树的基本原理 SVM原理及公式推导 Boosting算法 对数线性模型 概率主题模型，LDA思想 怎么判断两个词指的是同一个东西（语言模型，Wordvec） 对推荐和搜索排序的理解 编程语言C/C++ 阿里春招实习 C++的多态和虚函数 malloc和new关键字 类的构造函数初始化和初始化列表初始化区别 虚函数表 网易游戏实习 如果一个static对象被创建，什么时候被创建和删除 介绍overload和override 介绍inline (原理，编译器，优缺点，虚函数和Inline同时声明) 多态的实现机制(虚函数表，虚指针) 知不知道智能指针，介绍一下，如果多个线程同用一个shared_ptr，会不会互相影响，实现机制是什么样的(比如shared_ptr和它所指向的对象分别存在哪) vector实现机制，他和list区别 map和set的实现机制，以及为什么不用其他的平衡二叉树；除了上面的BST实现方法,map还能怎么实现，以及实现机制 虚拟内存的作用和实现机制 介绍动态链接库和静态连接库，如果在运行时找到DLL中的那个函数入口 Java 阿里春招实习 Spring的IOC是什么？Spring是怎么实现依赖控制的？ Java的synchronized和lock有什么区别？volatile有什么作用？ Java的hashmap怎么实现。 网易游戏实习 豌豆荚实习 类继承/接口实现 synchronize 线程安全的单例模式 Python移动客户端开发Android 阿里春招实习 Android的fragment和activity有什么区别？activity能否在不同的进程中启动？ iOS计算机网络 阿里春招实习 HTTP协议中的SSL怎么实现？ 1G文件, 点到点传输, 提高传输速率 网易游戏实习 TCP三次握手和四次握手 TCP, UPD, HTTP的关系，还问我会不会socket编程 cache的作用和实现机制 数据库操作系统 阿里春招实习 操作系统分页和分块有什么区别？ 什么是线程安全？ 网易游戏实习 进程之间通信方法 Linux系统设计 阿里春招实习 设计个系统：在搜索框里输入一个词，找到以它为前缀的商品，显示给用户作为辅助输入提示 数学、智力题 Hulu实习 四个瓶子，每个瓶子10个药丸，某些瓶子里的药丸全是坏的，正常药丸是10g，坏药丸是9g。现在只能进行一次称量重量操作，确定哪些药瓶里的药丸是坏的。药瓶里的药丸全是好的或者全是坏的；不准切割或溶解药丸。 如果每个瓶子里只有7个药丸呢？ 四个人A、B、C、D，站成一排，面向西边，每人头顶戴顶帽子，帽子有红黄蓝三种颜色，每个人只能看见自己前面的人的帽子的颜色，比如C可以看见A的和B的，A看不见任何人的帽子。现在有3顶红色帽子，2顶黄色帽子，1顶蓝色帽子，随机给这几个人带上。然后D说自己不知道自己是什么颜色，C说自己不知道自己是是什么颜色，B说自己不知道自己是什么颜色，A说自己知道自己是什么颜色，问A怎么推测的。在A说出自己帽子的颜色后，B、C、D能否确定自己帽子的颜色？ 杂项 阿里春招实习 接触过什么开源项目 最近读过的值得推荐的书是什么 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/2015-campus-recurit-technology-interview-summary.html 最后特别感谢2015年面点交流群各位伙伴的面筋:)]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>IT</tag>
        <tag>Interview</tag>
        <tag>Job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(CNN)学习笔记2：模型训练]]></title>
    <url>%2Fcnn-learning-notes-2.html</url>
    <content type="text"><![CDATA[上篇博文主要对CNN的基本网络结构及连接方式做了简单的介绍，还介绍了一个界内经典的LeNet-5模型。下面重点介绍CNN模型的训练过程/参数学习，在阅读本文之前，最好需要有以下方面的预备知识： 神经网络基础（网络结构，前向/后向传播方式，激活函数等）； 基础的最优化求解方法（梯度法，牛顿法等）； 机器学习基础 神经网络模型常用于处理有监督学习的问题，例如分类问题，CNN也不例外。模型需要一些有标注的数据进行训练，训练过程中主要涉及到网络的前向传播和反向传播计算，前向传播体现了特征信息的传递，而反向传播则是体现误差信息对模型参数的矫正。 CNN前向传播与普通的神经网络的前向传播过程一样。用 \( l \) 表示当前层，\( x^{l} \) 表示当前层的输出，\( W^{l} \) 和 \( b^{l} \) 分别表示当前层的权值和偏置，则前向传播可以用下面的公式表示： $$ x^{l} = f\left( u^{l}\right), \ with \; u^{l} = W^{l}x^{l-1} + b^{l} $$ 其中 \(f\left( \right)\) 函数为激活函数，可以选择sigmod或者tanh等函数。 对于卷积层，其前向传播如下图： CNN反向传播代价函数代价函数（或损失函数）有较多形式，常用的有平方误差函数，交叉熵等。这里我们用平方误差函数作为代价函数，公式如下： $$ E^{n} = \dfrac {1} {2}\sum _{k=1}^{c}\left( t_{k}^{n} - y_{k}^{n}\right) ^{2} = \dfrac {1} {2}||t^{n} - y^{n}||_{2}^{2}$$ 以上公式描述了样本 \( n \) 的训练误差，其中 \( c \) 为输出层节点的个数（通常就是最终的分类类别数目），\( t \) 是训练样本的正确结果，\( y \) 是网络训练的输出结果。 BP反向传播基本的反向传播与BP神经网络类似，首先，简单回顾一下BP神经网络中的反向传播计算过程： 权值参数调整的方向如下公式： $$ \Delta W^{l} = -\eta \dfrac {\partial E} {\partial W^{l}}, \ \ \dfrac {\partial E} {\partial W^{l}} = x^{l-1}(\delta ^{l})^{T} $$ 其中，\( \eta \) 为学习率。 $$ \dfrac {\partial E} {\partial b} = \dfrac {\partial E} {\partial u} \dfrac {\partial u} {\partial b} = \dfrac {\partial E} {\partial u} = \delta $$ 其中，\( \delta \) 称之为敏感度，也就是误差度。 \( \delta \)的计算方式如下： $$ \delta ^{L} = f’(u^{L})\circ (y^{n} - t^{n}) $$ $$ \delta ^{l} = (W^{l+1})^{T}\circ f’(u^{l}) $$ 其中，\( L \) 表示网络的最后一层，\( l \) 表示网络的其他层，\( \circ \) 表示点乘。 以上的两个公式反映了误差由网络的最后一层逐步向前传递的计算过程。 特殊的反向传播由于CNN中有不同类型的层级，并且层级之间的连接关系有可能是不确定的（如LeNet-5网络中S2层到C3层）。所以，有几个情形下的反向传播比较特别： 情况一：当前为Pooling层，前一层是卷积层； 情况二：当前为卷积层，前一层是Pooling层； 情况三：当前层与前一层的连接关系不确定（？尚不理解？）； 情况一：当前为Pooling层，前一层是卷积层 其中，Kronecker乘积的计算如下： 情况二：当前为卷积层，前一层是Pooling层 以上的矩阵1和矩阵2进行卷积操作时，需要将矩阵2先水平翻转，然后再垂直翻转；最后在矩阵1上进行卷积操作（和前向传播时类似）。 情况三：当前层与前一层的连接关系不确定个人理解，当前层与前一层的连接关系不确定时，反向传播与传统的BP算法类似，只不过更新的是局部连接的那些值。所以需要提前记录当前层的神经元与前一层的哪些元素是连接的。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/cnn-learning-notes-2.html 参考资料 卷积神经网络全面解析 CNN卷积神经网络反向传播机制的理解 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(CNN)学习笔记1：基础入门]]></title>
    <url>%2Fcnn-learning-notes-1.html</url>
    <content type="text"><![CDATA[概述卷积神经网络(Convolutional Neural Network, CNN)是深度学习技术中极具代表的网络结构之一，在图像处理领域取得了很大的成功，在国际标准的ImageNet数据集上，许多成功的模型都是基于CNN的。CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。 图像处理中，往往会将图像看成是一个或多个的二维向量，如之前博文中提到的MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过局部连接、权值共享等方法避免这一困难，有趣的是，这些方法都是受到现代生物神经网络相关研究的启发（感兴趣可阅读以下部分）。 下面重点介绍下CNN中的局部连接(Sparse Connectivity)和权值共享(Shared Weights)方法，理解它们很重要。 局部连接与权值共享下图是一个很经典的图示，左边是全连接，右边是局部连接。 对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。 尽管减少了几个数量级，但参数数量依然较多。能不能再进一步减少呢？能！方法就是权值共享。具体做法是，在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核(也称滤波器)的大小），如下图。 这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为Feature Map。如果有100个卷积核，最终的权值参数也仅为100 × 100 = 10^4个而已。另外，偏置参数也是共享的，同一种滤波器共享一个。 卷积神经网络的核心思想是：局部感受野(local field)，权值共享以及时间或空间亚采样这三种思想结合起来，获得了某种程度的位移、尺度、形变不变性（？不够理解透彻？）。 网络结构下图是一个经典的CNN结构，称为LeNet-5网络。 可以看出，CNN中主要有两种类型的网络层，分别是卷积层和池化/采样层(Pooling)。卷积层的作用是提取图像的各种特征；池化层的作用是对原始特征信号进行抽象，从而大幅度减少训练参数，另外还可以减轻模型过拟合的程度。 卷积层卷积层是卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果。如下图所示。 下面的动图能够更好地解释卷积过程： 池化/采样层通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种： Max-Pooling: 选择Pooling窗口中的最大值作为采样值； Mean-Pooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值； 如下图所示。 LeNet-5网络详解以上较详细地介绍了CNN的网络结构和基本原理，下面介绍一个经典的CNN模型：LeNet-5网络。 LeNet-5网络在MNIST数据集上的结果 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/cnn-learning-notes-1.html 参考资料 Deep Learning（深度学习）学习笔记整理系列之（七） 部分图片出自北京大学信息科学技术学院李戈教授的《深度学习技术与应用》课件 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记2：构建CNN模型]]></title>
    <url>%2Ftensorflow-learning-notes-2.html</url>
    <content type="text"><![CDATA[上篇博文主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。 然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。 深度学习模型TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。 基本设置在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。 加载MNIST数据集TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序： from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data', one_hot=True) 这里mnist是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。 开启TensorFlow会话TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。 这里我们使用了一个更为方便的类，InteractiveSession，这能让你在构建代码时更加灵活。InteractiveSession允许你做一些交互操作，通过创建一个计算流图(computation graph)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用InteractiveSession，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。 下面是如何创建一个InteractiveSession： import tensorflow as tf sess = tf.InteractiveSession() 计算流图(Computation Graph)为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了Theano和Torch上。 所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见这里。 Softmax Regression模型见上篇博文。 CNN模型Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。 权值初始化为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的噪声以破坏参数整体的对称性，同时避免梯度为0.由于我们使用ReLU激活函数（详细介绍)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数： def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 卷积(Convolution)和池化(Pooling)TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，我们始终使用vanilla版本。这里的卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式： def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 其中，padding=&#39;SAME&#39;表示通过填充0，使得输入和输出的形状一致。 第一层：卷积层第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。 W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) 为了使得图片与计算层匹配，我们首先reshape输入图像x为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（？第1维为什么是-1？） x_image = tf.reshape(x, [-1,28,28,1]) 然后，使用weight tensor对x_image进行卷积计算，加上bias，再应用到一个ReLU激活函数，最终采用最大池化。 h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) 第二层：卷积层为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。 W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) 全连接层到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个ReLu函数。 W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 整个CNN的网络结构如下图： Dropout为了减少过拟合程度，在输出层之前应用dropout技术（即丢弃某些神经元的输出结果）。我们创建一个placeholder来表示一个神经元的输出在dropout时不被丢弃的概率。Dropout能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的tf.nn.dropout操作能够利用mask技术处理各种规模的神经元输出。 keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最终，我们用一个softmax层，得到类别上的概率分布。（与之前的Softmax Regression模型相同）。 W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 模型训练和测试为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前Softmax Regression模型中的训练、测试过程类似。区别在于： 用更复杂的ADAM最优化方法代替了之前的梯度下降； 增了额外的参数keep_prob在feed_dict中，以控制dropout的几率； 在训练过程中，增加了log输出功能（每100次迭代输出一次）。 下面是程序： cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(tf.initialize_all_variables()) for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict={ x:batch[0], y_: batch[1], keep_prob: 1.0}) print("step %d, training accuracy %g"%(i, train_accuracy)) train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print("test accuracy %g"%accuracy.eval(feed_dict={ x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})) 最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的Softmax Regression模型。 完整代码及运行结果利用CNN模型实现手写体识别的完整代码如下： __author__ = 'chapter' import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data def weight_varible(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) print("Download Done!") sess = tf.InteractiveSession() # paras W_conv1 = weight_varible([5, 5, 1, 32]) b_conv1 = bias_variable([32]) # conv layer-1 x = tf.placeholder(tf.float32, [None, 784]) x_image = tf.reshape(x, [-1, 28, 28, 1]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) # conv layer-2 W_conv2 = weight_varible([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) # full connection W_fc1 = weight_varible([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # dropout keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # output layer: softmax W_fc2 = weight_varible([1024, 10]) b_fc2 = bias_variable([10]) y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) y_ = tf.placeholder(tf.float32, [None, 10]) # model training cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv)) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(tf.initialize_all_variables()) for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuacy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) print("step %d, training accuracy %g"%(i, train_accuacy)) train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5}) # accuacy on test print("test accuracy %g"%(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))) 运行结果如下图： 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/tensorflow-learning-notes-2.html 参考资料 TensorFlow: Deep MNIST for Experts 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10下网络连接问题及解决方案]]></title>
    <url>%2Fnetwork-connection-problems-in-win10.html</url>
    <content type="text"><![CDATA[回家后，发现笔记本能连上家里的wifi，但是就是不能上网。网络诊断问题提示是【此计算机上缺少一个或多个网络协议】。尝试了多种方法一直解决不了，经过一番折腾后终于成功，特此记录一下，以供后人参考。 操作系统环境：Windows 10 网络诊断错误：此计算机缺少一个或多个网络协议 可能的解决方法：开启特定的网络服务项网上大多都是这种方法，如下： 按windows+R键，在运行窗口中输入services.msc，检查以下服务是否正常开启： Telephony; Remote Access Connection Manager; Remote Access Auto Connection Manager; 找到上述服务中手动开启的项，右键属性；确认修改所选服务的启动类型为自动，如果服务状态为停止，点击启动来启动服务。 但是，这个方法并没有解决问题。本人尝试后失败。 可行的解决方法： 卸载目前的驱动程序在开始处，点击右键，选择网络连接。 找到连接上的无线网络，点击右键，选择状态,点开应该如下图： 点击图中的属性，如下图 点击配置，选择驱动程序 &gt; 卸载，如下图 勾选卸载相应的驱动程序，完成卸载。重启系统，然后重连wifi即可。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/network-connection-problems-in-win10.html]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Win10</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown写作中的常见问题]]></title>
    <url>%2Fmarkdown-common-problems.html</url>
    <content type="text"><![CDATA[本文记录markdown写作过程中所遇到的问题及相应的解决方法，以供参考。 Q1: 代码中出现{% 和 %}所包围的语句如果代码中出现了类似于{%××××××%}格式的语句，需要在这些语句块的首尾加上 1&#123;% raw %&#125; 和1&#123;% endraw %&#125; 以保证显示原始的语句。 示例如下： 显示效果如下： {% if theme.leancloud_visitors.enable %} {% include "_scripts/lean-analytics.swig" %} {% endif %} Q2: 如何显示原始的html代码在html代码块的上下均加上三个连续的反引号。 示例如下： 显示效果如下： 123&#123;% if theme.leancloud_visitors.enable %&#125;&#123;% include '_scripts/lean-analytics.swig' %&#125;&#123;% endif %&#125; Q3: 怎样给字体阴影的效果例如这样的阴影效果，只需要在内容的前后加上一个反引号，如下图： Q4: 如何显示公式中的花括号{}markdown中正常文本中使用\对{}进行转义即可；而公式中的{}即使这样转义也是不会显示的，正确做法是使用\lbrace \rbrace来表示左右花括号。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/markdown-common-problems.html 参考资料 Markdown 语法说明 (简体中文版) issue#587: Markdown代码块中的Markdown语法 V2EX: markdown反引号内怎么转义反引号 Markdown中写数学公式]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo的NexT主题个性化：添加文章阅读量]]></title>
    <url>%2Fhexo-next-add-post-views.html</url>
    <content type="text"><![CDATA[关于Hexo的文章阅读量设置问题，大多数人都是使用不蒜子的代码实现。但是这个方法仅局限于在文章页面显示阅读数，首页是不显示的。 下面介绍如何在首页及文章页面都显示文章的阅读量，显示效果如下： 配置LeanCloud注册打开LeanCloud官网，进入注册页面注册。完成邮箱激活后，点击头像，进入控制台页面，如下： 创建新应用创建一个新应用(类型为JavaScript SDK)，点击应用进入； 创建名称为Counter的Class 修改配置文件编辑网站根目录下的_config.yml文件，添加如下： # add post views leancloud_visitors: enable: true app_id: **你的app_id** app_key: **你的app_key** 其中，app_id和app_key在你所创建的应用的设置-&gt;应用Key中。 Web安全性为了保证应用的统计计数功能仅应用于自己的博客系统，你可以在应用-&gt;设置-&gt;安全中心的Web安全域名中加入自己的博客域名，以保证数据的调用安全。 修改NexT主题文件添加lean-analytics.swig文件在主题目录下的\layout\_scripts路径下，新建一个名称为lean-analytics.swig的文件，并添加如下内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;!-- custom analytics part create by xiamo --&gt;&lt;script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"&gt;&lt;/script&gt;&lt;script&gt;AV.initialize("&#123;&#123;theme.leancloud_visitors.app_id&#125;&#125;", "&#123;&#123;theme.leancloud_visitors.app_key&#125;&#125;");&lt;/script&gt;&lt;script&gt;function showTime(Counter) &#123; var query = new AV.Query(Counter); $(".leancloud_visitors").each(function() &#123; var url = $(this).attr("id").trim(); query.equalTo("url", url); query.find(&#123; success: function(results) &#123; if (results.length == 0) &#123; var content = '0 ' + $(document.getElementById(url)).text(); $(document.getElementById(url)).text(content); return; &#125; for (var i = 0; i &lt; results.length; i++) &#123; var object = results[i]; var content = object.get('time') + ' ' + $(document.getElementById(url)).text(); $(document.getElementById(url)).text(content); &#125; &#125;, error: function(object, error) &#123; console.log("Error: " + error.code + " " + error.message); &#125; &#125;); &#125;);&#125;function addCount(Counter) &#123; var Counter = AV.Object.extend("Counter"); url = $(".leancloud_visitors").attr('id').trim(); title = $(".leancloud_visitors").attr('data-flag-title').trim(); var query = new AV.Query(Counter); query.equalTo("url", url); query.find(&#123; success: function(results) &#123; if (results.length &gt; 0) &#123; var counter = results[0]; counter.fetchWhenSave(true); counter.increment("time"); counter.save(null, &#123; success: function(counter) &#123; var content = counter.get('time') + ' ' + $(document.getElementById(url)).text(); $(document.getElementById(url)).text(content); &#125;, error: function(counter, error) &#123; console.log('Failed to save Visitor num, with error message: ' + error.message); &#125; &#125;); &#125; else &#123; var newcounter = new Counter(); newcounter.set("title", title); newcounter.set("url", url); newcounter.set("time", 1); newcounter.save(null, &#123; success: function(newcounter) &#123; console.log("newcounter.get('time')="+newcounter.get('time')); var content = newcounter.get('time') + ' ' + $(document.getElementById(url)).text(); $(document.getElementById(url)).text(content); &#125;, error: function(newcounter, error) &#123; console.log('Failed to create'); &#125; &#125;); &#125; &#125;, error: function(error) &#123; console.log('Error:' + error.code + " " + error.message); &#125; &#125;);&#125;$(function() &#123; var Counter = AV.Object.extend("Counter"); if ($('.leancloud_visitors').length == 1) &#123; addCount(Counter); &#125; else if ($('.post-title-link').length &gt; 1) &#123; showTime(Counter); &#125;&#125;); &lt;/script&gt; 其中，控制显示的格式的主要为content变量，按自己的需求相应修改即可。 修改post.swig文件在主题的layout\_macro路径下，编辑post.swig文件，找到相应的插入位置（大概在98行左右）： 插入如下代码 123456&#123;% if theme.leancloud_visitors.enable %&#125;&amp;nbsp; | &amp;nbsp;&lt;span id="&#123;&#123; url_for(post.path) &#125;&#125;"class="leancloud_visitors" data-flag-title="&#123;&#123; post.title &#125;&#125;"&gt; &amp;nbsp;&#123;&#123;__('post.visitors')&#125;&#125; &lt;/span&gt;&#123;% endif %&#125; 修改layout.swig文件在主题目录下的layout目录下，编辑_layout.swig文件，在&lt;/body&gt;的上方（大概在70行左右）插入如下代码： 123&#123;% if theme.leancloud_visitors.enable %&#125;&#123;% include '_scripts/lean-analytics.swig' %&#125;&#123;% endif %&#125; 修改语言配置文件如果你的网站使用的是英语，则只需要编辑主题目录下的languages\en.yml文件，增加post字段如下： post: sticky: Sticky posted: Posted on visitors: Views // 增加的字段 ... 如果网站使用的是中文，则编辑languages\zh-Hans.yml文件，相应的增加 post: posted: 发表于 visitors: 阅读次数 ... 其他语言与之类似，将visitors设置成你希望翻译的字段。 最后，重新生成并部署你的网站即可。 增加网站的浏览次数与访客数量统计功能网站的浏览次数，即pv；网站的访客数为uv。pv的计算方式是，单个用户连续点击n篇文章，记录n次访问量；uv的计算方式是，单个用户连续点击n篇文章，只记录1次访客数。你可以根据需要添加相应的统计功能。 安装busuanzi.js脚本如果你使用的是NexT主题（其他主题类似），打开/theme/next/layout/_partial/footer.swig文件，拷贝下面的代码至文件的开头。 12&lt;script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 显示统计标签同样编辑/theme/next/layout/_partial/footer.swig文件。 如果你想要显示pv统计量，复制以下代码至你想要放置的位置， 123&lt;span id="busuanzi_container_site_pv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt; 如果你想要显示uv统计量，复制以下代码至你想要放置的位置， 123&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次&lt;/span&gt; 你可以自己修改文字样式，效果图如下： 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/hexo-next-add-post-views.html 参考资料 为NexT主题添加文章阅读量统计功能]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>LeanCloud</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在markdown中插入公式]]></title>
    <url>%2Fhow-to-insert-equations-in-markdown.html</url>
    <content type="text"><![CDATA[MathJax插件著名的Stackoverflow网站上的漂亮公式，就是使用了MathJax插件的效果。添加MathJax插件也非常简单，只需要在markdown文件中，添加MathJax CDN，如下： &lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt; &lt;/script&gt; 就可以在md文件中插入Tex格式的公式了。 行间公式的形式为 $$ 此处插入公式 $$ 而行内公式的形式为 \\( 此处插入公式 \\) 在MarkdownPad 2中编辑公式之前的博文有推荐Markdown Pad 2作为Window下的Markdown编辑器。如果你是使用该软件作为markdown的编辑器，你只需要在软件的Tools-&gt; Options-&gt; Advanced-&gt; HTML Head Editor中添加上述的MathJax CDN即可。 这样你就不必每次都在md文件中重复添加了。 好用的Tex公式生成器推荐一个在线手写公式转Tex格式的利器：Web Equation。通过手写公式，即可得到公式所对应的Tex格式，非常好用。 示例举个栗子。在Markdown Pad 2中新建文件，添加如下内容： 最后，我们在一个图片类别的evidence中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence为 $$ evidence\_{i}=\sum \_{j}W\_{ij}x\_{j}+b\_{i} $$ 其中，\\( W\_i \\) 和 \\( b\_i \\) 分别为类别 \\( i \\) 的权值和偏置。 （注意：markdown文件中的_前需要加上\转移符。） 最终效果如下（在Markdown Pad 2编辑器进行预览，快捷键为F6）： Hexo中显示数学公式值得注意的是，原生的Hexo并不支持数学公式的显示。所以，如果你仅仅完成了以上步骤，在hexo g -d之后，你会发现公式的效果并没有被渲染出来。 安装hexo-math插件在网站根目录下，打开git bash，输入 npm install hexo-math --save 然后，在根目录下的_config.yml文件中添加 plugins: hexo-math 之后重新生成和部署网站即可。 参考资料： MathJax with Markdownpad 2 Hexo上使用MathJax来实现数学公式的表达]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Equation</tag>
        <tag>Markdown</tag>
        <tag>MarkdownPad 2</tag>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记1：入门]]></title>
    <url>%2Ftensorflow-learning-notes.html</url>
    <content type="text"><![CDATA[TensorFlow 简介TensorFlow是Google在2015年11月份开源的人工智能系统（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音识别、图片识别等多个领域。 官网上对TensorFlow的介绍是，一个使用数据流图(data flow graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点节点之间的边，代表多维数据(tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph)数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些tensors的flow也就是TensorFlow的命名来源。 节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 TensorFlow的特性1 灵活性 TensorFlow不是一个严格的神经网络工具包，只要你可以使用数据流图来描述你的计算过程，你可以使用TensorFlow做任何事情。你还可以方便地根据需要来构建数据流图，用简单的Python语言来实现高层次的功能。 2 可移植性 TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。 3 提高开发效率 TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。 4 支持语言选项 目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口） 5 充分利用硬件资源，最大化计算性能 基本使用你需要理解在TensorFlow中，是如何： 将计算流程表示成图； 通过Sessions来执行图计算； 将数据表示为tensors； 使用Variables来保持状态信息； 分别使用feeds和fetches来填充数据和抓取任意的操作结果； 概览TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。一个Tensor是一个多维数组，例如，你可以将一批图像表示为一个四维的数组[batch, height, width, channels]，数组中的值均为浮点数。 TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPUs和GPUs)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor. 图计算TensorFlow程序中图的创建类似于一个 [施工阶段]，而在 [执行阶段] 则利用一个session来执行图中的节点。很常见的情况是，在 [施工阶段] 创建一个图来表示和训练神经网络，而在 [执行阶段] 在图中重复执行一系列的训练操作。 创建图在TensorFlow中，Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解Graph Class文档。 见下面的示例代码： import tensorflow as tf # Create a Constant op that produces a 1x2 matrix. The op is # added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. matrix1 = tf.constant([[3., 3.]]) # Create another Constant that produces a 2x1 matrix. matrix2 = tf.constant([[2.],[2.]]) # Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs. # The returned value, 'product', represents the result of the matrix # multiplication. product = tf.matmul(matrix1, matrix2) 默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。 在Session中执行图计算见下面的示例代码，更多可了解Session Class： # Launch the default graph. sess = tf.Session() # To run the matmul op we call the session 'run()' method, passing 'product' # which represents the output of the matmul op. This indicates to the call # that we want to get the output of the matmul op back. # # All inputs needed by the op are run automatically by the session. They # typically are run in parallel. # # The call 'run(product)' thus causes the execution of threes ops in the # graph: the two constants and matmul. # # The output of the op is returned in 'result' as a numpy `ndarray` object. result = sess.run(product) print(result) # ==&gt; [[ 12.]] # Close the Session when we're done. sess.close() Sessions最后需要关闭，以释放相关的资源；你也可以使用with模块，session在with模块中自动会关闭： with tf.Session() as sess: result = sess.run([product]) print(result) TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果你想在第二块多余的GPU上执行： with tf.Session() as sess: with tf.device("/gpu:1"): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... device中的各个字符串含义如下： &quot;/cpu:0&quot;: 你机器的CPU； &quot;/gpu:0&quot;: 你机器的第一个GPU； &quot;/gpu:1&quot;: 你机器的第二个GPU； 关于TensorFlow中GPU的使用见这里。 交互环境下的使用以上的python示例中，使用了Session和Session.run()来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用InteractiveSession类，以及Tensor.eval()、Operation.run()等方法。例如，在交互的Python环境下执行以下代码： # Enter an interactive TensorFlow Session. import tensorflow as tf sess = tf.InteractiveSession() x = tf.Variable([1.0, 2.0]) a = tf.constant([3.0, 3.0]) # Initialize 'x' using the run() method of its initializer op. x.initializer.run() # Add an op to subtract 'a' from 'x'. Run it and print the result sub = tf.sub(x, a) print(sub.eval()) # ==&gt; [-2. -1.] # Close the Session when we're done. sess.close() TensorsTensorFlow中使用tensor数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个tensor具有固定的类型、级别和大小，更加深入理解这些概念可参考Rank, Shape, and Type。 变量(Variables)变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色： # Create a Variable, that will be initialized to the scalar value 0. state = tf.Variable(0, name="counter") # Create an Op to add one to `state`. one = tf.constant(1) new_value = tf.add(state, one) update = tf.assign(state, new_value) # Variables must be initialized by running an `init` Op after having # launched the graph. We first have to add the `init` Op to the graph. init_op = tf.initialize_all_variables() # Launch the graph and run the ops. with tf.Session() as sess: # Run the 'init' op sess.run(init_op) # Print the initial value of 'state' print(sess.run(state)) # Run the op that updates 'state' and print 'state'. for _ in range(3): sess.run(update) print(sess.run(state)) # output: # 0 # 1 # 2 # 3 赋值函数assign()和add()函数类似，直到session的run()之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。 抓取(Fetches)为了抓取ops的输出，需要先执行session的run函数。然后，通过print函数打印状态信息。 input1 = tf.constant(3.0) input2 = tf.constant(2.0) input3 = tf.constant(5.0) intermed = tf.add(input2, input3) mul = tf.mul(input1, intermed) with tf.Session() as sess: result = sess.run([mul, intermed]) print(result) # output: # [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 所有tensors的输出都是一次性 [连贯] 执行的。 填充(Feeds)TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序： input1 = tf.placeholder(tf.float32) input2 = tf.placeholder(tf.float32) output = tf.mul(input1, input2) with tf.Session() as sess: print(sess.run([output], feed_dict={input1:[7.], input2:[2.]})) # output: # [array([ 14.], dtype=float32)] 如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考MNIST fully-connected feed tutorial (source code)。 示例：曲线拟合下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。 # 简化调用库名 import tensorflow as tf import numpy as np # 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3 x_data = np.random.rand(100).astype("float32") y_data = x_data * 0.1 + 0.3 # 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值） W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) b = tf.Variable(tf.zeros([1])) y = W * x_data + b # 最小化均方误差 loss = tf.reduce_mean(tf.square(y - y_data)) optimizer = tf.train.GradientDescentOptimizer(0.5) train = optimizer.minimize(loss) # 初始化TensorFlow参数 init = tf.initialize_all_variables() # 运行数据流图（注意在这一步才开始执行计算过程） sess = tf.Session() sess.run(init) # 观察多次迭代计算时，w和b的拟合值 for step in xrange(201): sess.run(train) if step % 20 == 0: print(step, sess.run(W), sess.run(b)) # 最好的情况是w和b分别接近甚至等于0.1和0.3 MNIST手写体识别任务下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。 MNIST数据集介绍MNIST是一个简单的图片数据集（数据集下载地址），包含了大量的数字手写体图片。下面是一些示例图片： MNIST数据集是含标注信息的，以上图片分别代表5, 0, 4和1。 由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作： from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。 MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用xs和ys分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。 一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下： 但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。 所以MNIST的训练数据集可以是一个形状为55000 * 784位的tensor，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（”tensor”中的像素值在0到1之间）。如下图： MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示3。所以，mnist.train.labels是一个55000 * 10的二维数组。如下： 以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。 Softmax Regression模型数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9. Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。 Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）： Step 1: add up the evidence of our input being in certain classes;Step 2: convert that evidence into probabilities. 为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）： 最后，我们在一个图片类别的evidence(不知如何翻译..)中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为 $$ evidence_{i}=\sum _{j}W_{ij}x_{j}+b_{i} $$ 其中，\( W_i \) 和 \( b_i \) 分别为类别 \( i \) 的权值和偏置，\( j \) 是输入图片 \( x \) 的像素索引。然后，我们将得到的evidence值通过一个”softmax”函数转化为概率值 \( y \) : $$ y = softmax(evidence) $$ 这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下： $$ softmax(x) = normalize(exp(x)) $$ 具体计算方式如下 $$ softmax(x)_{i} = \dfrac {exp\left( x_{i}\right) } {\Sigma _{j}exp\left( x_{j}\right) } $$ 这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程： 如果我们将这个过程公式化，将得到 实际的计算中，我们通常采用矢量计算的方式，如下 也可以简化成 $$ y = softmax( Wx + b ) $$ Softmax Regression的程序实现为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。 为了使用TensorFlow，我们需要引用该库函数 import tensorflow as tf 我们利用一些符号变量来描述交互计算的过程，创建如下 x = tf.placeholder(tf.float32, [None, 784]) 这里的 \( x \) 不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。 我们使用Variable(变量)来表示模型中的权值和偏置，这些参数是可变的。如下， W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) 这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。 实现softmax regression模型仅需要一行代码，如下 y = tf.nn.softmax(tf.matmul(x, W) + b) 其中，matmul函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。 模型的训练在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下： $$ H_{y’}\left( y\right) = -\sum _{i}y_{i}’\log \left( y_{i}\right) $$ 这里 \( y \) 是所预测的概率分布，而 \( y’ \) 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数，可参考这篇博文。 为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值， y_ = tf.placeholder(tf.float32, [None, 10]) 然后得到交叉熵，即\( -\sum y’\log \left( y\right) \)： cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。 接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用后向传播算法来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下， train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多其他的优化算法，仅需要一行代码即可实现调用。 TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化： init = tf.initialize_all_variables() 我们可以在一个Session里面运行模型，并且进行初始化： sess = tf.Session() sess.run(init) 接下来，进行模型的训练 for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) 每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。 以上过程成为随机梯度下降，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。 模型的评价怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。 首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下： correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。 accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 最后，我们来获取模型在测试集上的准确率， print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。（这里有众多模型在MNIST数据集上的运行结果）。 完整代码及运行结果利用Softmax模型实现手写体识别的完整代码如下： __author__ = 'chapter' import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) print("Download Done!") x = tf.placeholder(tf.float32, [None, 784]) # paras W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x, W) + b) y_ = tf.placeholder(tf.float32, [None, 10]) # loss func cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # init init = tf.initialize_all_variables() sess = tf.Session() sess.run(init) # train for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) print("Accuarcy on Test-dataset: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) 运行结果如下图： 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/tensorflow-learning-notes.html 参考资料 TensorFlow官方帮助文档 文章写得不错？打赏一个呗:) 【打赏1.99￥以上，备注你的邮箱，可获得博主精心为你准备的深度学习/机器学习/自然语言处理的学习资料大礼包】 近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将博客托管至Coding及相应的DNS设置]]></title>
    <url>%2Fblog-on-gitcafe-with-dns-settings.html</url>
    <content type="text"><![CDATA[在之前的博文中，已经介绍了如何创建一个Hexo网站并且将其托管至Github上，从而实现一个静态的博客网站。由于国内访问Github速度较慢甚至无法访问，因此有了国内版的Github，也就是Gitcafe。将网站托管至Gitcafe上的好处就是，你的网站即使在国内的网络环境下也能被访问，同时也便于百度等中文搜索引擎的收录。 将博客托管至Gitcafe与托管至Github类似，但是仍存在一些细小的差别。下面将介绍如何将之前所创建的网站托管至Gitcafe上，主要包含两方面的内容： 将网站托管至Gitcafe上； 相应的DNS域名解析设置以实现国内外分流访问网站（即国外网络环境下是通过Github Page访问你的网站，而国内则是通过Gitcafe Page访问）。 Gitcafe目前已经面临关闭，博文最后介绍了将静态网站迁移至Coding的方法。 将博客托管至Gitcafe上注册Gitcafe账号打开Gitcafe官网，注册账号。 配置SSH假设你已经完成将网站托管至Github上，此时你的本地已经生成了SSH公钥文件，打开这个文件并复制其中的内容。下面是我的ssh公钥文件路径 C:\Users\ZhangJie\.ssh\id_rsa.pub 打开Gitcafe主页，登陆后进入到“账户设置”中，点击左侧的SSH公钥管理，然后添加新的公钥，将之前的内容粘贴到对应的栏目里即可。 为了测试ssh是否配置成功，打开本地的git bash，输入 ssh -T git@gitcafe.com 如果显示 Hi USERNAME! You've successfully authenticated... 则说明配置成功，此时你可以免密码将本地的项目文件同步至Gitcafe中。 如果依然存在问题，可以查看官网详细的帮助页面。 创建同名项目以及修改配置文件在Gitcafe上新建一个与你的账户名同名的项目。 修改站点目录下的配置文件，找到deploy项。我的deploy项内容如下 # Deployment ## Docs: http://hexo.io/docs/deployment.html deploy: type: git repo: github: git@github.com:JeyZhang/JeyZhang.github.io.git gitcafe: git@gitcafe.com:JeyZhang/JeyZhang.git,gitcafe-pages 注意对于gitcafe，网站应该托管至其gitcafe-pages分支上。 将本地网站同步至Gitcafe项目中在网站根目录下，打开gitbash，输入 hexo g -d 即可将本地网站同步至Github和Gitcafe上。 （注意：在本地网站根目录下的source文件夹下，需要新建一个文件名为CNAME的文件（无后缀名），里面填写你所绑定的域名地址，如www.jeyzhang.com.） 测试是否成功将网站托管至Gitcafe上如果项目根目录下存在CNAME文件，暂时现将其删除。（因为目前你还没有将你的网站域名解析至Gitcafe服务器上） 在浏览器输入地址 http://jeyzhang.gitcafe.io 看网站是否访问成功（国内网络即可）。 DNS解析设置笔者使用DNSPod的域名解析,我的域名解析设置如下 国内线路选择Gitcafe，国外线路选择Github，从而实现国内外分流访问网站。主机记录为@可以实现访问××××××.com时，自动填充”www”开头，因为之前我们绑定的网站是www.××××××.com的二级域名。 等待一段时间生效即可。 将博客托管至Coding平台托管的方法与Gitcafe的类似，托管完成后，你可以手动删除Gitcafe上的项目（2016年5月31号之后系统也会自动清除的）。 注册Coding账户并建立项目去Coding的官网注册，在个人主页的项目中创建一个项目，最好创建与你账户名相同的项目。例如，我的账户名为jeyzhang, 创建的项目名为JeyZhang（大小写不区分）。 上传SSH文件在Coding的个人主页的账户中，进入SSH公钥。添加你的公钥，如果你之前生成过，公钥在C:\Users\你的用户名\.ssh\id_rsa.pub。复制里面的内容在SSH-RSA公钥内容中即可。 打开git bash，输入 ssh -T git@git.coding.net 进行测试，如果显示如下则SSH配置成功： Hello ...! You've conected to Coding.net by SSH successfully! 修改网站的配置文件修改网站根目录下的配置文件_config.yml，找到deploy的设置处，改为如下： deploy: type: git repo: github: git@github.com:JeyZhang/jeyzhang.github.io.git coding: git@git.coding.net:JeyZhang/JeyZhang.git,master 注意要改成你的项目地址。 将网站文件部署至Coding在网站根目录下打开git bash，输入 hexo g -d 进行网站文件的生成和部署。成功之后，进入你的Coding对应的项目中应该能看到网站文件。 配置Coding的Page服务进入你在Coding上的项目，点击左侧的代码可以看到Coding Pages服务。输入分支为master，点击开启服务。在自定义域名处填上你的网站域名，如下图所示： 配置DNS笔者使用DNSPod进行网站的DNS设置。将国内线路设置为CNAME的page.coding.me即可。如下图所示。 等待一会儿，你就能在国内网络快速访问你的网站了。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/blog-on-gitcafe-with-dns-settings.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Coding</tag>
        <tag>DNS</tag>
        <tag>Gitcafe</tag>
        <tag>Github</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo网站优化之SEO]]></title>
    <url>%2Fhexo-website-seo.html</url>
    <content type="text"><![CDATA[SEO (Search Engine Optimization)，即搜索引擎优化。对网站做SEO优化，有利于提高搜索引擎的收录速度及网页排名。下面讲解一些简单的SEO优化方法，主要针对Hexo网站。 SEO优化之title编辑站点目录下的themes/layout/index.swig文件， 将下面的代码 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endlock %&#125; 改成 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125; &#123;% endlock %&#125; 这时将网站的描述及关键词加入了网站的title中，更有利于详细地描述网站。 添加robots.txtrobots.txt是一种存放于网站根目录下的ASCII编码的文本文件，它的作用是告诉搜索引擎此网站中哪些内容是可以被爬取的，哪些是禁止爬取的。robots.txt应该放在站点目录下的source文件中，网站生成后在网站的根目录(站点目录/public/)下。 我的robots.txt文件内容如下 User-agent: * Allow: / Allow: /archives/ Allow: /categories/ Allow: /about/ Disallow: /vendors/ Disallow: /js/ Disallow: /css/ Disallow: /fonts/ Disallow: /vendors/ Disallow: /fancybox/ 添加sitemapSitemap即网站地图，它的作用在于便于搜索引擎更加智能地抓取网站。最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。 Step 1: 安装sitemap生成插件 npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save Step 2: 编辑站点目录下的_config.yml，添加 # hexo sitemap网站地图 sitemap: path: sitemap.xml baidusitemap: path: baidusitemap.xml Step 3: 在robots.txt文件中添加 Sitemap: http://www.jeyzhang.com/sitemap.xml Sitemap: http://www.jeyzhang.com/baidusitemap.xml 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/hexo-website-seo.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>SEO</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015年校招求职总结]]></title>
    <url>%2F2015-campus-recruit-summary.html</url>
    <content type="text"><![CDATA[2015年，对于我而言，是充满忙碌和煎熬的一年。从去年年底到前不久，我几乎全是在实习，面试准备和面试中度过。从刚开始的迷茫，到中间的误打误撞，再到最终的尘埃落定，整个过程中我感受到了自己的成长和进步。 非常感激一路上给予我指点的师兄师姐，实习期间的同事们以及共同求职的小伙伴们，他们在我面试前准备，获取面试机会及最终的工作选择上给予了很多的支持和帮助。如今自己也收获了一份非常满意的工作，在此对过去一年的求职历程（包括实习及校招）做一个总结，希望能够帮助后来人。 本人求职的岗位是IT技术岗（偏机器学习背景的工程师岗位），不同行业，同一行业的不同职位，甚至不同公司的相同职位的求职都可能存在巨大的差异，所以本文尽量凸显求职中的共性问题，避免过多的求职细节。一些求职细节，比如面筋之类的会另辟博文。 实习篇我是从研二上学期的末期开始实习的。累积了两段实习经历，分别是搜狗移动搜索研究部和微软（中国）互联网工程院的小冰项目组，实习时间都是半年。在搜狗实习，主要做的是移动端网页的优化排序，包括网页意图识别及用户个性化方面的基础性工作，实习工作主要是独立完成的，所以感觉这段时间学习到了挺多东西的；之后的暑期去了微软实习，在小冰组的CS项目组，CS(Customer Service)项目组主要是负责小冰的商业盈利，实现小冰与企业级客户的合作，例如与招商银行的智能客服项目等。在小冰组做的事情较零散，主要是因为市场需求和决策变化的关系，主要做了些特征工程及分类器训练，以及其他工程相关的工作，微软内部的code review是比较严格的，所以自己的工程能力得到较大的提高。 现在回头看，个人觉得实习对于找工作来说优势还是很明显的，下面详细地介绍下关于实习的那些事儿。 实习的前提条件 实验室有较宽松的条件，主要是时间条件和导师的态度方面； 不影响正常的科研和毕业。 实习有哪些好处实习的好处是显而易见的，主要如下 1. 充实简历内容 一般来说，有1-2段的实习经历会让简历看起来更加充实，一般可以将“实习经历”放在“教育背景”和“专业技能”的后面。但是，实习内容需要与你最终求职的方向较相关，否则不太建议放在简历，简历上的东西尽量不要有冗余或者与求职目的背道而驰的内容（如应聘技术岗，尽量不要放太多文艺社团活动之类的）。关于如何书写一份好的简历，会在其他的博文中详述。 值得说明的是，优秀的实习经历会给简历增色不少。建议实习选择平台较大，业界名气较大的公司进行实习，例如微软、谷歌、BAT之类的。这是因为，如果你能去知名的企业实习，一定程度上也说明了你的能力，因为去这些公司实习也是有一定门槛的。优秀的实习经历能让你在正式校招中，更容易得到同行中其他公司的认可。 2. 提升自身的技术能力及沟通合作能力 一般来说，在公司和在学校的时候还是有较大差别的。首先，公司以盈利为目的，在互联网公司中，一个项目往往需要能够迅速完成并不断地上线迭代，工作压力较大。其次，周边的同事们往往会比较专注于自己的事情，而且同事之间或许存在这样那样的利益冲突。所以，公司环境较学校环境复杂一些。 现实情况下，产品需求往往是多变的，所以常常要求你能在短时间内快速学习新知识并给予实现。你也能遇到很多的机会，去实践你之前在书本上学习到的理论知识，所以实习对提升自身的技术实践能力有很大的帮助。而且，实习生一般需要经常与你的主管交流沟通，也可能需要和其他的同事合作，所以在这个过程中，你的沟通合作能力也将得到一定的提升。 技术能力和沟通合作能力，是正式招聘面试中考察的两大核心点。在“校招篇”中，我将详细说明。 3. 人脉积累和可能的转正机会 实习过程中你能认识一些公司的正式员工或者和你一样的实习生们，这些人可能会在今后会提供一些直接或间接的工作机会，毕竟互联网圈子并不太大。更加重要的是，通过实习，你的表现优秀通常能够获得公司内部直接转正的机会，如果能够成功转正，那么你将在正式校招之前获得一份保底的offer。 一般来说，每年的春季(4-5月份)会有很多大公司进行大规模的实习生招聘，这种类型的实习生往往是有转正机会的，而且公司也会把这一部分的实习生作为校招的提前备选人员。值得一说的是，通常而言公司更加青睐实习过并表现良好的同学，所以很多时候，通过先去该公司实习再转正会比直接参与该公司校招更加简单一些。 4. 经济上的回报 在互联网公司实习通常能够获得较高的工资（与其他传统行业相比），通常在月薪在3000-5000之间，当然也有网易游戏这样能够给出近万工资的土豪公司。但是，个人建议，选择实习主要要以学东西和自己的兴趣方向为主，不必贪恋这短短数个月的实习工资高低。 如何获取实习机会1. 师兄师姐的内推 可以询问认识的师兄师姐，看看他们所在公司内部是否有相应的内推机会。一般而言，内推是互利的，一方面内推你的人如果内推成功往往能够获得一定的奖励；另一方面，相较于普通的实习生求职，内推的成功率会更高一些，因为公司更愿意选择自己员工所推荐的人选。 2. BBS论坛 找实习有几个常用的BBS论坛，分别是北邮人兼职实习版块，水木社区实习版块和未名BBS实习版块等。 3. 相关的学生群 偶尔一些班级群、年级群等地方，也会有人发布一些实习信息。所以，可以多多加入一些同学群之类的，便于收集这方面的信息。 实习面试前该怎么准备1. 筛选实习信息，确定候选的意向职位 BBS论坛上每天都会发布大量的实习信息，如果你对自己喜欢的以及以后要做的事情比较明确，那么筛选起来会比较简单；如果你是第一次找实习，或者对自己今后要做什么不太确定，建议可以按照如下进行筛选： 排除自己不感兴趣的和不喜欢的 往往人对自己排斥的和没感觉的东西是比较明确的，所以可以先排除那些你不感兴趣的，甚至不喜欢的实习工作。 总结自己的特长，过去的经历带给你的优势 举个例子，例如你之前在数学竞赛或者建模竞赛方面的经历较丰富并且还获过一些奖项，那么一方面说明你潜意识里对数学方面还是比较喜欢的，并且比一般的人更擅长；另一方面，这些经历对你申请数据分析师或者算法工程师等需要数据基础较好的职位，会有很大的优势。那么这些职位就可以成为你的候选职位了。 另外，又例如你尽管没有互联网产品相关的经历，但是由于之前听过相关的讲座活动，比较感兴趣并想深入了解下。那么也可以将这样的职位作为你的备选，寻找实习和实习准备的过程中，你也会更加了解这些职位是干什么的，自己究竟喜不喜欢。 总之，投递实习时可以针对自身的特长兴趣和经历进行选择，多了解和多尝试总是没错的。 2. 针对职位和自身，制作简历 一份内容优质和格式清晰的简历，能够给你争取一个好的第一印象，也是获取下一步面试资格的必要条件。简单来说，简历需要包含以下条目（排名分前后）：(1)基本的个人信息，包括姓名和联系方式；(2)教育背景，包括学校、学历、专业、GPA及排名情况；(3)专业技能；(4)实习经历/科研经历/项目经历；(5)在校期间获得的各种荣誉和奖励（尽量和所投的职位相关）；(6)社会实践和个人评价（可省略）。 针对职位制作简历的要点在于，尽量在简历中突出该职位要求的要点，举个例子，下面是某BBS上一则实习生招聘信息： 数据挖掘实习生 工作职责: o 数据采集/信息挖掘提取及相关工具开发 o 数据处理流程及相关工具开发 任职资格: o 熟悉 C/C++，有爬虫开发经验，并有相关实际项目经验以及脚本经验 o 熟练掌握至少一门常用脚本语言 o 优秀的分析问题和解决问题的能力 o 有良好的学习能力及团队合作精神 如果希望投递该职位，那么简历上最好突出几条： (1) 熟悉C/C++编程，良好的编程规范 (2) 熟悉某种脚本编程语言，例如Python或者Shell... (3) 有过相关的项目经历，如自己用Python编写过爬虫软件，实现了某某功能等 换位思考，如果我们是该公司的HR，即使不懂技术，暂从简历上看会觉得这个同学和该职位比较匹配，至少会给该同学一个面试机会。 针对自身制作简历的要点在于，认清自己的能力，不要过分谦虚，也不要过分夸奖。简历本身是一种对自己的包装，所以尽量真实地表达自己，请注意在简历中慎用“精通”两字，自己都不熟悉的项目经历最好不要写上简历，总之请确保你对简历上的每一句话都有足够的自信和充分的解释能力。 3. 投递简历的情商 论坛上常常充斥着大量的实习招聘信息，而背后的求职者往往要比这多几个数量级。所以对于实习的发布者而言，通常也能够收到大量的求职邮件和求职者的简历信息。下面有一些我自己总结的投递简历的注意事项， (1) 如果实习招聘信息中有明确的邮件发送格式，请严格按照该格式撰写邮件。因为很有可能招聘人员是编写了一些基于规则的脚本解析程序，来对大量的邮件进行分类和整理的，如果你发的邮件格式不合要求，很有可能会被淹没在收件箱中。 (2) 如果实习招聘信息中没有明确的邮件发送格式，一般而言，邮件的标题可写为“[实习]职位+学校+学历+专业+姓名”，最好让收件人一看邮件标题就知道邮件的来意；邮件的正文可以简单的介绍一下自己的基本情况和个人优势（引起对方阅读你简历的兴趣），注意礼貌；最后记得附上你的简历即可（最好是pdf格式的，因为word格式有可能不同版本的软件打开格式会乱）。 下面是一个示例的邮件正文模板， 您好： 在***看到您的实习招聘信息，对该职位很感兴趣并有一定的基础及实践经历。 附件为我的中英文简历，请查阅。 期待您的回复。 谢谢，祝好。 可做参考。 4. 针对职位，准备面试内容 面试准备主要包括两部分，一是熟悉你的简历，要求能够对简历上的任何一个项目都足够熟悉，可以提前思考可能存在的问题及当时的解决方法；二是准备技术面试，如果是投递IT技术岗位，基本的算法和数据结构要比较熟悉，其次是你熟悉的语言方面的细节问题（如Java中的容器底层实现之类的），如果你投递的岗位有相关背景，你还需要额外准备一下背景知识，如机器学习背景的，你需要熟悉常见的机器学习算法及简单的原理公式等。 如何选择一份好的实习考虑如下因素（排名有先后） 是否有助于你的职业发展？ 例如如果你今后希望做数据相关的岗位，实习也应该选择类似的职位，因为这段实习经历将会对你今后正式求职过程中给予直接的帮助。几乎所有公司都希望招聘有过职位相关经验的人员。 是不是你所感兴趣的，有热情的？ 选择自己感兴趣和有热情的岗位，将不会让你在实习过程中感到无聊和乏味。如果实在没有感兴趣的岗位，那就将这次的实习当做是一次尝试也未尝不可。 你能获取的资源有哪些？ 去大公司实习，往往能够学习到大公司中的做事规范，也能认识许多经验丰富的前辈。去小公司实习，往往自己身上的事情会比较多，做事情或许没有那么规范，但是需要你短时间能去学习很多东西。所以，不论在哪种环境下，你都要充分观察你能获取的资源有哪些？哪里的资源对你更加有利。 其他因素 例如，待遇情况，离校距离远近，软性福利，工作时间及压力等。 实习期间你需要注意的事项 时间观念很重要 时间观念包括了很多方面，例如保证实习时间（按之前约定的一周几天之类的，有事情一定要记得请假），开组会不要迟到，不要轻易拖后项目的ddl等。 与你的主管保持充分沟通 充分沟通包括，一是主动去寻找任务，弄清楚你主管的任务需求，然后尽量去完成；二是明确任务中各项细节，确保你做的东西是你主管真正需要的；三是，时刻让你的主管知道你的任务进度和遇到的问题。 沟通的方式有很多种，面对面，邮件，电话或信息等。总之，不要胆怯，有问题或者想法尽量去找你的主管交流，他有这个责任帮助和指导你。如果你的主管比较忙，你可以多用书面的形式（例如邮件）进行沟通。 “特殊”的实习？一般来说，每年的春季，大概在3-4月份会有许多公司进行暑期实习生的招聘，与平时所招聘的实习生相比，这种类型的实习生往往是具有直接转正机会的，而且流程简单，成功概率较大。例如，微软中国的实习生分为project intern和summer intern两种，summer intern的转正面试一般只需要1-2轮，而project intern的转正面试一般在3轮以上。 校招篇 校招什么时候开始校招主要分秋招和春招两批，秋招的时间段为9-12月份，而春招则在下年的3-5月份，秋招的求职机会较春招会多很多，很多企业如果秋招招满就不进行春招了。一般来说，互联网相关企业（包括BAT等私企，MS、GG等外企，华为等国企）的校招在每年的9-11月份比较集中，而研究所、银行以及其他大多数国企一般会稍晚一些，主要集中在11-12月份。 如何获取校招信息校招信息可以通过以下渠道获得： 各企业的官方网站中的招聘专栏，相应的微信公众号、微博号等； 高校BBS以及求职相关的群； 学校举办的校招宣讲会 同学、同行们的分享信息； 总之，多关注自己心仪公司的官网里的校招信息（可关注其微博号、微信号等），多关注学校的BBS求职相关的版面，以及就业中心发布的企业宣讲会信息，还有多和周边一起求职的同学们多交流。 校招面试的形式及注意事项一般来说，除了内推之外，正式校招时在获得面试通知前，会先进行投递简历和笔试等流程。如果简历和笔试关都顺利通过，那么你就能进入正式的面试环节了。面试的形式主要有两种，单面和群面。对于技术面试，一般是一对一的面试；对于产品、运营等方面的面试，有可能会有群面。由于本人投递的是技术岗，所以没有遭遇过群面，下面主要讲讲技术面试的流程及注意事项。 对于技术岗，正式校招一般会有1-4轮的技术面试+1轮的HR面试。技术面试主要是考察你的技术能力、表达能力以及沟通能力，普遍的流程是 1.自我介绍 -&gt; 2.谈谈简历上的内容(论文、项目、实习经历等) -&gt; 3.技术测试题(算法题、系统设计、数学题、智力题等) 以上1和2并不是必须的，有些公司为节省时间可能会直接上来让你做题(如GG)。不同方向岗位的技术测试题可能各有偏重，其中，算法题是最常见的，主要考察算法、数据结构及你的基本编程能力，题型可参照[leetcode]上的题目。如果你是求职C++开发岗，C++语言特性及细节是你需要重点准备的；如果你是求职机器学习相关岗位，那么你需要熟悉常用的分类、聚类算法，对一些常见简单的模型，要能够进行公式推导(如LR,朴素贝叶斯等)。 校招面试前该如何准备1. 准备一段精简的自我介绍 自我介绍往往是面试的开场白，一方面你可以进行下预热，逐渐进入面试状态；另一方面，也有助于面试官在极短的时间内对你有一定的了解，并有一定时间可以浏览你的简历。所以，为了使得面试官在几分钟内对你产生一个较好的第一印象，你需要在自我介绍中释放出足够的信息量，以让对方觉得你的资历与岗位要求是匹配的。 自我介绍的时间一般在1-3分钟，如果对方有时间上的要求，自我介绍就尽量简短，包含要点即可。自我介绍一般包含以下几个要素（主要针对技术岗面试）： 基本信息（姓名，学校学院，专业，研究方向等） 专业技能（熟悉的编程语言，工具平台，技能达到的熟练程度等） 实习/项目/论文经历（你在什么时间做了什么事情，担任的什么角色，最终获得的结果和成就） 强调你的求职目标及意愿（表达你对该职位的强烈兴趣和意愿） 其中，专业技能和实习/项目/论文经历最好与你求职的岗位要求相关，即你的自我介绍中的每一句话应该都要对你求职该岗位是有利的。自我介绍时，如果是面对面的面试，注意目光应该平视，时不时应该要与面试官有眼神交流，（在电话面试中）表达要自信流畅，给别人一种靠谱踏实的感觉。 2. 熟悉简历上的内容 面试时最好随身携带简历，一来表示你的求职态度比较真诚；二来防止面试官忘记打印你的简历。对于简历上的内容，应该要做到对其中的每一项内容都足够熟悉，足够自信并且有能力来给予证明。 根据经验，需要注意一下的情形： 避免出现精通、特别擅长等较极端词汇 如果你确实是个大神，请忽略这条。否则，最好用一些较平和的词汇，如熟悉、了解、比较擅长等。这样做的好处是，即使你在面试中遇到某一方面不会的问题，也有一个台阶下，不至于尴尬；否则，如果一开始给别人一个很高的期望值，而结果距离该期望值太远，那么面试官给你no hire的结果也就有充分理由了。总之，尽量保持谦逊的态度，因为谦逊的人往往更易于相处与合作。 删去有陌生感和不确定的简历内容 有一些项目经历，可能是你本科时候的经历，（如果你已经是硕博士）这对你可能会有一些陌生和模糊感。如果你确实不太记得其中的很多技术细节，最好不要写在简历上，因为万一面试官问起，你又支支吾吾地回答，会给人一种不自信或者不诚信的感觉。类似的相关简历内容都最后删去。 3. 技术面试准备 对于IT的技术面试，算法题是最常见的题型，主要是因为算法题可以在短时间内对应试者的算法、数据结构、编程语言及风格等方面进行考察。对于算法题的相关准备，有以下的一些建议： 选择常见的编程语言（最好是面试官所熟悉的），例如C/C++/Java/Python等； 勤刷算法题，相关的OJ练习平台有leetcode、lintcode、国内的九度等； 打好算法及数据结构基础，推荐书籍如下：算法面试相关的有《Crack the code interview》、《剑指Offer》、《微软经典面试100题系列》和《编程之美》等；算法理论相关的有《算法导论》、《算法设计与分析》(屈婉玲版)和《Java数据结构和算法》(Robert Lafore版)。 除了算法题之外，技术面试中常考察的还有编程语言的相关细节、数据结构、操作系统、数据库和系统设计等。编程语言方面的考察，一般会以你熟悉的编程语言或者是岗位所要求的编程语言为主。例如，C++中常见的考点是指针、const用法、虚函数、构造函数与构析函数等；Java则是垃圾回收机制、容器细节等(如ArrayList和LinkedList的区别，HashTable和HashMap的区别等)。数据结构方面，一定要熟悉常见的数据结构（链表、二叉树及BST、栈及队列、哈希表等），并要能够给予实现，对于高级的数据结构（如红黑树、后缀数组等）做简单了解即可，当然如果你面试GG这样的公司，或许能够深入理解则是更好的；操作系统方面需要掌握一些常见问题，如进程和线程的区别、常见的调度算法等；数据库方面需要对常见的数据库管理软件例如MySQL有所了解，能够编写简单的SQL语句，推荐的入门书籍有《SQL必知必会》，然后刷刷leetcode上的SQL相关的题即可。系统设计方面主要熟悉一些常见的系统模式，如工厂模式和单例模式等，要能够做简单的实现。 技术面试除了以上之外，可能会根据求职方向的不同有所偏倚，例如求职大数据相关的职位，可能需要了解常见的大数据工具如Hadoop，Spark等的原理和使用。技术面试准备时，可以结合职位要求中的条目，如果职位要求中有一些你必备的技术，这方面则需要尽量准备。 4. HR面试注意事项 如果前面的技术面试顺利，将会进入到HR面试。一般来说，很多公司的HR面试是终面，所以也不能掉以轻心。HR面试的内容可能有以下项目： 基本信息：包括你的学历情况，基本的家庭情况，对公司和职位的感兴趣和了解程度等。主要考察你的个人信息，家庭对你的期望，以及你对公司和职位的渴求程度。 性格和处事能力：这一部分可能会问一些你的经历，包括项目经历和实习经历等。[你过程中遇到的困难是什么，如何解决的？]，[你遇到问题是怎么和上级沟通的？]等问题，可以体现出你是一个什么样性格的人，沟通能力以及解决困难的能力。 HR面试中尽量表现出自己积极自信的一面，表现出自己对该职位的感兴趣和渴望。回答问题时，注意目光要平视，语速平顺；进出面试时，要注意基本礼仪；最后，注意态度要真诚，要相信HR阅人无数，撒谎是很容易被识别出来的。 模拟？模拟！如果可以的话，最好找一些职场上的前辈或者同学，给自己做一下模拟面试。模拟面试可以让你提前熟悉面试流程，减轻正式面试过程中的紧张感。最重要的是，可以与你的那位[面试官]进行角色互换，从别人和另一个角度，对自己和别人身上存在的问题进行校正。如果是外企的英文面试，那么模拟面试是十分重要和需要的。 面试中的“潜规则” 不要在技术面试中，问及薪资待遇等与技术无关的问题； 技术面试中，尽量表现出你对该职位的兴趣和热情，轮到你提问时，可以问一下近期的项目情况、遇到的问题及目前的解决方法等，对其中感兴趣的问题可以深入了解； 面试结束后，不要急切地问面试结果如何，但是可以问面试结果的通知时间； 一般来说，如果面试通过，那么你将很快接受到下一轮面试的信息；如果等待了超过一周时间，则很有可能被默拒；很多公司不会发拒信。 面试准备的过程中，多换位思考，假想你是面试官，希望给什么样的面试者机会；多总结，不论面试结果成功还是失败，可以总结总结面筋及经验。 如何谈offer如果你顺利通过了所有的面试，那么接下来HR会和你沟通职位信息，包括所在部门、职位、级别及薪资。大多数公司定薪水的依据，一般是你的面试表现，也会和你的学历、个人期望值有关。大多数公司的offer都会有两个档位，普通和special，档位相对应的薪水一般是固定的，没有太多的谈判空间。 但是，有一些公司（尤其是创业型公司，例如滴滴、京东等）可以依人给不同价位的offer，这时候就需要去谈offer了。这时谈offer主要看你对公司的稀缺程度及手上的offer情况，当你拿到一些该公司竞争对手的offer时，对谈offer是极其有利的，因为如果该公司不能够给予至少同等的待遇是不够具有吸引力的。谈offer时，你可以告诉HR你目前手上的offer以及你的期望薪资，表明出如果对方公司愿意用更高的offer，你就会归顺于该公司的决心。 当然，谈offer也要适可而止，对于一个缺乏工作经验和社会经验的应届生而言，公司的争取力度也是有限的。摆正自己的姿态很重要。 如何选择一份好的工作推荐使用offer比较器。 找完工作后该如何规划 完成毕业论文，按时毕业； 技术充电； 完成毕业前的小心愿，或者索性旅游、玩耍一小段时间； 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/2015-campus-recruit-summary.html 推荐资料 连续创业者：如何招聘到适合的员工]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>IT</tag>
        <tag>Interview</tag>
        <tag>Job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT主题个性化设置]]></title>
    <url>%2Fnext-theme-personal-settings.html</url>
    <content type="text"><![CDATA[提前说明： 假设网站的根目录为D:/Hexo/，也称为站点目录 站点配置文件 是指网站目录下的_config.yml文件，即D:/Hexo/_config.yml 主题配置文件 是指网站目录下对应的主题文件夹下的_config.yml文件，即D:/Hexo/themes/next/_config.yml. 下面的功能设置完成后，记得 hexo g -d 以完成网站的生成和部署。 添加分类、标签云、关于等页面以添加分类页面为例， 在站点目录下，打开git bash，输入 hexo new page "categories" 之后在站点目录下的source文件夹下，会新增一个categories的文件夹，里面有一个index.md文件，打开如下 title: categories date: 2015-12-04 15:37:22 type: "categories" comments: false --- 其中，comments可以设置为false，含义是打开分类页面，评论插件不显示；如要显示则改为true。 tags, about页面的创建类似，输入 hexo new page "tags" hexo new page "about" 添加站内搜索功能NexT支持Swiftype插件以实现站内搜索功能。 Step 1: 注册Swiftype Step 2: 创建一个新的搜索引擎 (点击Create an engine，按要求创建即可) Step 3: 点击新建的搜索引擎，按如下点击INSTALL SEARCH 然后复制下面蓝色底的字串 Step 4: 编辑站点配置文件，添加如下内容 # Swiftype Search Key swiftype_key: xxxxxxxxx(粘贴以上复制的内容) 设置右侧栏头像编辑站点配置文件，添加如下内容 avatar: your avatar url 其中，your avatar url可以是(1) 完整的互联网URL，你可以先将设置的头像图片放到图床上；(2) 本地地址：如/upload/image/avatar.png (你需要将avatar.png文件放在/站点目录/source/upload/image/里面)。 设置favicon图标Step 1:首先要有一个常见格式名(如.jpg, .png等)的图片作为备选favicon，选择一个favicon制作网站完成制作，例如比特虫是一个免费的在线制作ico图标网站。 Step 2:将favicon.ico文件放在网站根目录下的source文件夹内即可。刷新网站，就可以看到效果了。 添加社交链接编辑站点配置文件，添加 social: github: https://github.com/your-user-name twitter: https://twitter.com/your-user-name weibo: http://weibo.com/your-user-name douban: http://douban.com/people/your-user-name zhihu: http://www.zhihu.com/people/your-user-name # 等等 可根据自身需要自行删减。 添加友情链接以添加github官网(https://www.github.com)为友情链接为例 编辑站点配置文件，添加如下内容 # title links_title: Links # links links: Github: https://www.github.com 其中，links_title为友情链接的名称。 添加评论区支持Disqus和多说两种评论样式。建议中文网站选择多说，英文网站选择Disqus。下面以Disqus为例说明。 Step 1: 注册Disqus Step 2: 登陆后进入到Settings，点击Add Disqus To Site，然后点击页面的右上角的Install on Your Site Step 3: 复制你的shortname Step 4: 编辑站点配置文件，添加 disqus_shortname: your disqus shortname 这样你的所有文章及页面下面，会自动加载Disqus的评论插件。如果在分类、标签云等页面，不想显示评论区，可以打开这个page文件夹下的md文件，添加 comments: false 首页文章以摘要形式显示最简单的方式是：打开主题配置文件，找到如下位置，修改 auto_excerpt: enable: true length: 150 其中length代表显示摘要的截取字符长度。 设置首页文章显示篇数Step 1: 安装相关插件 输入如下命令 npm install --save hexo-generator-index npm install --save hexo-generator-archive npm install --save hexo-generator-tag Step 2: 安装完插件后，在站点配置文件中，添加如下内容 index_generator: per_page: 5 archive_generator: per_page: 20 yearly: true monthly: true tag_generator: per_page: 10 其中per_page字段是你希望设定的显示篇数。index, archive及tag开头分表代表主页，归档页面和标签页面。 设置404公益页面在站点目录的source文件夹下，新建404.html文件，将下面的代码复制进去保存即可。 &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;404 - arao'blog&lt;/title&gt; &lt;meta name="description" content="404错误，页面不存在！"&gt; &lt;meta http-equiv="content-type" content="text/html;charset=utf-8;"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt; &lt;meta name="robots" content="all" /&gt; &lt;meta name="robots" content="index,follow"/&gt; &lt;/head&gt; &lt;body&gt; &lt;script type="text/javascript" src="http://qzonestyle.gtimg.cn/qzone_v6/lostchild/search_children.js" charset="utf-8"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 显示效果如下 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/next-theme-personal-settings.html 参考资料 NexT官方帮助文档]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github: 搭建属于自己的静态博客]]></title>
    <url>%2Fhexo-github-blog-building.html</url>
    <content type="text"><![CDATA[Hexo是一个快速、简洁且高效的博客框架，而Github是一个免费的代码托管工具，利用Github Page可以免费创建一个静态网站。下面将介绍如何使用Hexo和Github，在win10环境下搭建一个静态的博客。 全文分为三个部分： 安装和配置Hexo及Github 选择Hexo主题及发表文章 注册及绑定自己的域名地址 安装和配置Hexo及Github安装Hexo安装Hexo前，需要安装以下： Node.js Git 如果已经安装完成以上程序，打开Git-bash或者cmd，输入 npm install -g hexo-cli 即可完成Hexo的安装。 使用Hexo进行本地建站选择一个本地的文件夹，如D:\hexo。 输入 hexo init D:\hexo cd D:\hexo npm install 如果hexo安装成功，则在D:\hexo文件夹下的文件目录为 . ├── _config.yml // 网站的配置信息，你可以在此配置大部分的参数。 ├── package.json ├── scaffolds // 模板文件夹。当你新建文章时，Hexo会根据scaffold来建立文件。 ├── source // 存放用户资源的地方 | ├── _drafts | └── _posts └── themes // 存放网站的主题。Hexo会根据主题来生成静态页面。 详细文件或文件夹的具体含义见 Hexo官方文档之建站 为了测试本地建站是否成功，输入 hexo s 如果显示如下 则说明本地建站成功，访问本地地址可以看到Hexo默认主题的效果。 至此，Hexo的安装和本地建站完成，如需更加深入全面地了解Hexo，可访问Hexo官方文档。 创建Github账号如果已经注册Github，可跳过此步骤。否则，访问Github官网进行注册，下面假设你注册Github账号名为MyGithub。 创建与账号同名的Repository注册并登陆Github官网成功后，点击页面右上角的+，选择New repository。 在Repository name中填写你的Github账号名.github.io，这里是MyGithub.github.io。Description中填写对此repository的描述信息(可选，但建议填写，如Personal website)。 点击Create repository，完成创建。 配置SSH(1) 生成SSH 检查是否已经有SSH Key，打开Git Bash，输入 cd ~/.ssh 如果没有这个目录，则生成一个新的SSH，输入 ssh-keygen -t rsa -C "your e-mail" 其中，your e-mail是你注册Github时用到的邮箱。 然后接下来几步都直接按回车键，最后生成如下 (2) 复制公钥内容到Github账户信息中 打开~/.ssh/id_rsa.pub文件，复制里面的内容； 打开Github官网，登陆后进入到个人设置(点击头像-&gt;setting)，点击右侧的SSH Keys，点击Add SSH key；填写title之后，将之前复制的内容粘贴到Key框中，最后点击Add key即可。 (3) 测试SSH是否配置成功 输入 ssh -T git@github.com 如果显示以下，则说明ssh配置成功。 Hi username! You've successfully authenticated, but GitHub does not provide shell access. 将网站发布到Github的同名repository中打开D:\Hexo文件夹中的_config.yml文件，找到如下位置，填写 # Deployment ## Docs: http://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:MyGithub/MyGithub.github.io 注： (1) 其中MyGithub替换成你的Github账户; (2) 注意在yml文件中，:后面都是要带空格的。 此时，通过访问http://MyGithub.github.io可以看到默认的Hexo首页面（与之前本地测试时一样）。 选择Hexo主题及发表文章简洁的Next主题本网站使用的是Next主题。该主题简洁易用，在移动端也表现不错。 (1) 下载Next主题 cd D:\Hexo git clone https://github.com/iissnan/hexo-theme-next themes/next (2) 修改网站的主题为Next 打开D:\Hexo下的_config.yml文件，找到theme字段，将其修改为next # Extensions ## Plugins: http://hexo.io/plugins/ ## Themes: http://hexo.io/themes/ theme: next (3) 本地验证是否可用 输入 hexo s --debug 访问本地网站，确认网站主题是否切换为Next. (4) 更新Github 输入 hexo g -d 完成Github上网页文件的更新。 发表新文章发表文章操作非常简单，在网站存放的根目录打开git bash，输入 hexo n "name of the new post" 回车后，在source文件夹下的_post文件夹下，可以看到新建了一个name of the new post.md的文件，打开 title: name of the new post date: 2015-12-09 22:55:25 tags: --- 可以给文章贴上相应的tags，如有多个则按照如下格式 [tag1, tag2, tag3, ...] 在- - -下方添加正文内容即可，注意需要使用markdown语法进行书写。 在这里有关于Markdown语法的简单说明。推荐使用MarkdownPad2进行md文件的编辑工作。 文章撰写完成后保存，输入 hexo g -d 即可生成新网站，并且同步Github上的网站内容。 注册及绑定自己的域名地址截止到目前为止，你应该可以通过访问http://MyGithub.github.io来看到以上创建的网站了。 但是，如何拥有一个属于自己的域名地址，并将其指向在Github上所创建的网站呢？ 注册域名推荐选择国内的万网或者国外的Goddady进行域名的注册。 DNS域名解析设置如果你选择的是万网注册的域名，可以使用其自带的域名解析服务。 进入万网，登陆后进入到个人中心(点击用户名即可)，点击左侧的”云解析”，点击之前所购买的域名，在”解析设置”中，添加如下解析规则: 其中，当记录类型为A时，记录值为服务器的ip地址，这里的服务器地址为存放Github page的地址，你可以通过命令行输入 ping github.io 得到。 DNS域名解析设置需要一定时间，之后你可以通过ping自己的域名地址来查看是否解析成功。 在Github对应的repository中添加CNAME文件即在 MyGithub/MyGithub.github.io 中加入名为”CNAME”的文件，文件内容为你的域名地址，如 www.××××××.com 保存即可。 CNAME文件设置的目的是，通过访问 MyGithub.github.io 可以跳转到你所注册的域名上。 为了方便本地文件deploy的时候，CNAME文件不发生丢失，可以在本地网站根目录下的source文件夹下，添加以上的CNAME文件。以后每次deploy的时候，CNAME文件不会发生丢失。 通过以上的设置，相信你已经可以通过注册域名来访问一个默认的hexo主题页面了。之后的工作就在于，(1)如何对主题进行个性化设置及；(2)发表博文以充实网站内容。这里有关于next主题的个性化设置说明。 本文结束，感谢欣赏。 欢迎转载，请注明本文的链接地址： http://www.jeyzhang.com/hexo-github-blog-building.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Github Page</tag>
        <tag>Hexo</tag>
        <tag>Personal Website</tag>
      </tags>
  </entry>
</search>
