<!doctype html>
<html class="theme-next   use-motion ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Attention,Deep Learning,LSTM,Machine Learning,RNN," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description" content="导读目前采用编码器-解码器 (Encode-Decode) 结构的模型非常热门，是因为它在许多领域较其他的传统模型方法都取得了更好的结果。这种结构的模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，该模型能够学习出对应合理的向量表示。然而，这种模型存在的问题在于：当输入序列非常长时，模型难以学到合理的向量表示。
在这篇博文中，我们将探索加入LSTM/RNN模型中的atte">
<meta property="og:type" content="article">
<meta property="og:title" content="理解LSTM/RNN中的Attention机制">
<meta property="og:url" content="http://www.jeyzhang.com/understand-attention-in-rnn.html">
<meta property="og:site_name" content="Jey Zhang">
<meta property="og:description" content="导读目前采用编码器-解码器 (Encode-Decode) 结构的模型非常热门，是因为它在许多领域较其他的传统模型方法都取得了更好的结果。这种结构的模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，该模型能够学习出对应合理的向量表示。然而，这种模型存在的问题在于：当输入序列非常长时，模型难以学到合理的向量表示。
在这篇博文中，我们将探索加入LSTM/RNN模型中的atte">
<meta property="og:image" content="http://i.imgur.com/V9HYowa.png">
<meta property="og:image" content="http://i.imgur.com/qycpffZ.png">
<meta property="og:image" content="http://i.imgur.com/B70hRCE.png">
<meta property="og:image" content="http://i.imgur.com/BTCD2NH.png">
<meta property="og:image" content="http://i.imgur.com/Kh3gwhV.png">
<meta property="og:image" content="http://i.imgur.com/tTBAnzw.png">
<meta property="og:image" content="http://i.imgur.com/CTuJ7Ln.jpg">
<meta property="og:updated_time" content="2017-07-05T06:13:10.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="理解LSTM/RNN中的Attention机制">
<meta name="twitter:description" content="导读目前采用编码器-解码器 (Encode-Decode) 结构的模型非常热门，是因为它在许多领域较其他的传统模型方法都取得了更好的结果。这种结构的模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，该模型能够学习出对应合理的向量表示。然而，这种模型存在的问题在于：当输入序列非常长时，模型难以学到合理的向量表示。
在这篇博文中，我们将探索加入LSTM/RNN模型中的atte">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post',
    motion: true
  };
</script>

  <title> 理解LSTM/RNN中的Attention机制 | Jey Zhang </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-71292341-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?eed1a5ff91ce000d3cbee31156f82f2c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Jey Zhang</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Life is Now.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            About
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'LwCA4Pqyomh6kHjA4fV9','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                理解LSTM/RNN中的Attention机制
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            Posted on
            <time itemprop="dateCreated" datetime="2017-07-03T18:59:56+08:00" content="2017-07-03">
              2017-07-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; In
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/understand-attention-in-rnn.html#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="understand-attention-in-rnn.html" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          
		  
		  
			 &nbsp; | &nbsp;
			 <span id="/understand-attention-in-rnn.html"class="leancloud_visitors"  data-flag-title="理解LSTM/RNN中的Attention机制">
             &nbsp;Views
            </span>
		  
		
        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h3 id="导读">导读</h3><p>目前采用编码器-解码器 (Encode-Decode) 结构的模型非常热门，是因为它在许多领域较其他的传统模型方法都取得了更好的结果。这种结构的模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，该模型能够学习出对应合理的向量表示。然而，这种模型存在的问题在于：<strong>当输入序列非常长时，模型难以学到合理的向量表示</strong>。</p>
<p>在这篇博文中，我们将探索加入LSTM/RNN模型中的attention机制是如何克服传统编码器-解码器结构存在的问题的。</p>
<p>通过阅读这篇博文，你将会学习到：</p>
<ul>
<li>传统编码器-解码器结构存在的问题及如何将输入序列编码成固定的向量表示；</li>
<li>Attention机制是如何克服上述问题的，以及在模型输出时是如何考虑输出与输入序列的每一项关系的；</li>
<li>基于attention机制的LSTM/RNN模型的5个应用领域：机器翻译、图片描述、语义蕴涵、语音识别和文本摘要。</li>
</ul>
<p>让我们开始学习吧。</p>
<h3 id="长输入序列带来的问题">长输入序列带来的问题</h3><p>使用传统编码器-解码器的RNN模型先用一些LSTM单元来对输入序列进行学习，编码为固定长度的向量表示；然后再用一些LSTM单元来读取这种向量表示并解码为输出序列。</p>
<p>采用这种结构的模型在许多比较难的序列预测问题（如文本翻译）上都取得了最好的结果，因此迅速成为了目前的主流方法。</p>
<p>例如：</p>
<ul>
<li><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks, 2014</a></li>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014</a></li>
</ul>
<p>这种结构在很多其他的领域上也取得了不错的结果。然而，它存在一个问题在于：<strong>输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示</strong>。</p>
<p>这个问题限制了模型的性能，尤其是<strong>当输入序列比较长时，模型的性能会变得很差</strong>（在文本翻译任务上表现为待翻译的原始文本长度过长时翻译质量较差）。</p>
<blockquote>
<p>“一个潜在的问题是，采用编码器-解码器结构的神经网络模型需要将输入序列中的必要信息表示为一个固定长度的向量，而当输入序列很长时则难以保留全部的必要信息（因为太多），尤其是当输入序列的长度比训练数据集中的更长时。”</p>
<p>— Dzmitry Bahdanau, et al., <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural machine translation by jointly learning to align and translate, 2015</a></p>
</blockquote>
<h3 id="使用attention机制">使用attention机制</h3><p>Attention机制的基本思想是，<strong>打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制</strong>。</p>
<p>Attention机制的实现是<strong>通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联</strong>。</p>
<p>换一个角度而言，输出序列中的每一项的生成概率取决于在输入序列中选择了哪些项。</p>
<blockquote>
<p>“在文本翻译任务上，使用attention机制的模型每生成一个词时都会在输入序列中找出一个与之最相关的词集合。之后模型根据当前的上下文向量 (context vectors) 和所有之前生成出的词来预测下一个目标词。</p>
<p>… 它将输入序列转化为一堆向量的序列并自适应地从中选择一个子集来解码出目标翻译文本。这感觉上像是用于文本翻译的神经网络模型需要“压缩”输入文本中的所有信息为一个固定长度的向量，不论输入文本的长短。”</p>
<p>— Dzmitry Bahdanau, et al., <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural machine translation by jointly learning to align and translate, 2015</a></p>
</blockquote>
<p>虽然模型使用attention机制之后会增加计算量，但是性能水平能够得到提升。另外，使用attention机制便于理解在模型输出过程中输入序列中的信息是如何影响最后生成序列的。这有助于我们更好地理解模型的内部运作机制以及对一些特定的输入-输出进行debug。</p>
<blockquote>
<p>“论文提出的方法能够直观地观察到生成序列中的每个词与输入序列中一些词的对齐关系，这可以通过对标注 (annotations) 权重参数可视化来实现…每个图中矩阵的每一行表示与标注相关联的权重。由此我们可以看出在生成目标词时，源句子中的位置信息会被认为更重要。”</p>
<p>— Dzmitry Bahdanau, et al., <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural machine translation by jointly learning to align and translate, 2015</a></p>
</blockquote>
<h3 id="大型图片带来的问题">大型图片带来的问题</h3><p>被广泛应用于计算机视觉领域的卷积神经网络模型同样存在类似的问题： 对于特别大的图片输入，模型学习起来比较困难。</p>
<p>由此，一种启发式的方法是将在模型做预测之前先对大型图片进行某种近似的表示。</p>
<blockquote>
<p>“人类的感知有一个重要的特性是不会立即处理外界的全部输入，相反的，人类会将注意力专注于所选择的部分来得到所需要的信息，然后结合不同时间段的局部信息来建立一个内部的场景表示，从而引导眼球的移动及做出决策。”</p>
<p>— <a href="https://arxiv.org/abs/1406.6247" target="_blank" rel="external">Recurrent Models of Visual Attention, 2014</a></p>
</blockquote>
<p>这种启发式方法某种程度上也可以认为是考虑了attention，但在这篇博文中，这种方法并不认为是基于attention机制的。</p>
<p>基于attention机制的相关论文如下：</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.6247" target="_blank" rel="external">Recurrent Models of Visual Attention, 2014</a></li>
<li><a href="https://arxiv.org/abs/1502.04623" target="_blank" rel="external">DRAW: A Recurrent Neural Network For Image Generation, 2014</a></li>
<li><a href="https://arxiv.org/abs/1412.7755" target="_blank" rel="external">Multiple Object Recognition with Visual Attention, 2014</a></li>
</ul>
<h3 id="基于attention模型的应用实例">基于attention模型的应用实例</h3><p>这部分将列举几个具体的应用实例，介绍attention机制是如何用在LSTM/RNN模型来进行序列预测的。</p>
<h4 id="1-_Attention在文本翻译任务上的应用">1. Attention在文本翻译任务上的应用</h4><p>文本翻译这个实例在前面已经提过了。</p>
<p>给定一个法语的句子作为输入序列，需要输出翻译为英语的句子。Attention机制被用在输出输出序列中的每个词时会专注考虑输入序列中的一些被认为比较重要的词。</p>
<blockquote>
<p>我们对原始的编码器-解码器模型进行了改进，使其有一个模型来对输入内容进行搜索，也就是说在生成目标词时会有一个编码器来做这个事情。这打破了之前的模型是基于将整个输入序列强行编码为一个固定长度向量的限制，同时也让模型在生成下一个目标词时重点考虑输入中相关的信息。</p>
<p>— Dzmitry Bahdanau, et al., <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural machine translation by jointly learning to align and translate, 2015</a></p>
</blockquote>
<p><img src="http://i.imgur.com/V9HYowa.png" alt=""></p>
<p><em>Attention在文本翻译任务（输入为法语文本序列，输出为英语文本序列）上的可视化（图片来源于Dzmitry Bahdanau, et al., <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural machine translation by jointly learning to align and translate, 2015</a>）</em></p>
<h4 id="2-_Attention在图片描述上的应用">2. Attention在图片描述上的应用</h4><p>与之前启发式方法不同的是，基于序列生成的attention机制可以应用在计算机视觉相关的任务上，帮助卷积神经网络重点关注图片的一些局部信息来生成相应的序列，典型的任务就是对一张图片进行文本描述。</p>
<p>给定一张图片作为输入，输出对应的英文文本描述。Attention机制被用在输出输出序列的每个词时会专注考虑图片中不同的局部信息。</p>
<blockquote>
<p>我们提出了一种基于attention的方法，该方法在3个标准数据集上都取得了最佳的结果……同时展现了attention机制能够更好地帮助我们理解模型地生成过程，模型学习到的对齐关系与人类的直观认知非常的接近（如下图）。</p>
<p>— <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2016</a></p>
</blockquote>
<p><img src="http://i.imgur.com/qycpffZ.png" alt=""></p>
<p><em>Attention在图片描述任务（输入为图片，输出为描述的文本）上的可视化（图片来源于Attend and Tell: Neural Image Caption Generation with Visual Attention, 2016）</em></p>
<h4 id="3-_Attention在语义蕴涵_(Entailment)_中的应用">3. Attention在语义蕴涵 (Entailment) 中的应用</h4><p>给定一个用英文描述的前提和假设作为输入，输出假设与前提是否矛盾、是否相关或者是否成立。</p>
<p>举个例子：</p>
<p><strong>前提</strong>：在一个婚礼派对上拍照</p>
<p><strong>假设</strong>：有人结婚了</p>
<p>该例子中的假设是成立的。</p>
<p>Attention机制被用于关联假设和前提描述文本之间词与词的关系。</p>
<blockquote>
<p>我们提出了一种基于LSTM的神经网络模型，和把每个输入文本都独立编码为一个语义向量的模型不同的是，该模型同时读取前提和假设两个描述的文本序列并判断假设是否成立。我们在模型中加入了attention机制来找出假设和前提文本中词/短语之间的对齐关系。……加入attention机制能够使模型在实验结果上有2.6个点的提升，这是目前数据集上取得的最好结果…</p>
<p>— <a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="external">Reasoning about Entailment with Neural Attention, 2016</a></p>
</blockquote>
<p><img src="http://i.imgur.com/B70hRCE.png" alt=""></p>
<p><em>Attention在语义蕴涵任务（输入是前提文本，输出是假设文本）上的可视化（图片来源于Reasoning about Entailment with Neural Attention, 2016）</em></p>
<h4 id="4-_Attention在语音识别上的应用">4. Attention在语音识别上的应用</h4><p>给定一个英文的语音片段作为输入，输出对应的音素序列。</p>
<p>Attention机制被用于对输出序列的每个音素和输入语音序列中一些特定帧进行关联。</p>
<blockquote>
<p>…一种基于attention机制的端到端可训练的语音识别模型，能够结合文本内容和位置信息来选择输入序列中下一个进行编码的位置。该模型有一个优点是能够识别长度比训练数据长得多的语音输入。</p>
<p>— <a href="https://arxiv.org/abs/1506.07503" target="_blank" rel="external">Attention-Based Models for Speech Recognition, 2015.</a></p>
</blockquote>
<p><img src="http://i.imgur.com/BTCD2NH.png" alt=""></p>
<p><em>Attention在语音识别任务（输入是音帧，输出是音素的位置）上的可视化（图片来源于Attention-Based Models for Speech Recognition, 2015）</em></p>
<h4 id="5-_Attention在文本摘要上的应用">5. Attention在文本摘要上的应用</h4><p>给定一篇英文文章作为输入序列，输出一个对应的摘要序列。</p>
<p>Attention机制被用于关联输出摘要中的每个词和输入中的一些特定词。</p>
<blockquote>
<p>… 在最近神经网络翻译模型的发展基础之上，提出了一个用于生成摘要任务的基于attention的神经网络模型。通过将这个概率模型与一个生成式方法相结合来生成出准确的摘要。</p>
<p>— <a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization, 2015</a></p>
</blockquote>
<p><img src="http://i.imgur.com/Kh3gwhV.png" alt=""></p>
<p><em>Attention在文本摘要任务（输入为文章，输出为文本摘要）上的可视化（图片来源于A Neural Attention Model for Abstractive Sentence Summarization, 2015）</em></p>
<h3 id="进一步的阅读">进一步的阅读</h3><p>如果你想进一步地学习如何在LSTM/RNN模型中加入attention机制，可阅读以下论文：</p>
<ul>
<li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="external">Attention and memory in deep learning and NLP</a></li>
<li><a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" target="_blank" rel="external">Attention Mechanism</a></li>
<li><a href="http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html" target="_blank" rel="external">Survey on Attention-based Models Applied in NLP</a></li>
<li><a href="https://www.quora.com/What-is-exactly-the-attention-mechanism-introduced-to-RNN-recurrent-neural-network-It-would-be-nice-if-you-could-make-it-easy-to-understand" target="_blank" rel="external">What is exactly the attention mechanism introduced to RNN?</a> （来自Quora）</li>
<li><a href="https://www.quora.com/What-is-Attention-Mechanism-in-Neural-Networks" target="_blank" rel="external">What is Attention Mechanism in Neural Networks?</a></li>
</ul>
<p>目前Keras官方还没有单独将attention模型的代码开源，下面有一些第三方的实现：</p>
<ul>
<li><a href="http://ben.bolte.cc/blog/2016/language.html" target="_blank" rel="external">Deep Language Modeling for Question Answering using Keras</a></li>
<li><a href="https://github.com/fchollet/keras/issues/2067" target="_blank" rel="external">Attention Model Available!</a></li>
<li><a href="https://github.com/philipperemy/keras-attention-mechanism" target="_blank" rel="external">Keras Attention Mechanism</a></li>
<li><a href="https://github.com/fchollet/keras/issues/1472" target="_blank" rel="external">Attention and Augmented Recurrent Neural Networks</a></li>
<li><a href="https://github.com/fchollet/keras/issues/4962" target="_blank" rel="external">How to add Attention on top of a Recurrent Layer (Text Classification)</a></li>
<li><a href="https://github.com/fchollet/keras/issues/1472" target="_blank" rel="external">Attention Mechanism Implementation Issue</a></li>
<li><a href="https://github.com/fchollet/keras/issues/2612" target="_blank" rel="external">Implementing simple neural attention model (for padded inputs)</a></li>
<li><a href="https://github.com/fchollet/keras/issues/1094" target="_blank" rel="external">Attention layer requires another PR</a></li>
<li><a href="https://github.com/farizrahman4u/seq2seq" target="_blank" rel="external">seq2seq library</a></li>
</ul>
<h3 id="总结">总结</h3><p>通过这篇博文，你应该学习到了attention机制是如何应用在LSTM/RNN模型中来解决序列预测存在的问题。</p>
<p>具体而言，采用传统编码器-解码器结构的LSTM/RNN模型存在一个问题：不论输入长短都将其编码成一个固定长度的向量表示，这使模型对于长输入序列的学习效果很差（解码效果很差）。而attention机制则克服了上述问题，原理是在模型输出时会选择性地专注考虑输入中的对应相关的信息。使用attention机制的方法被广泛应用在各种序列预测任务上，包括文本翻译、语音识别等。</p>
<hr>
<p>本文结束，感谢欣赏。</p>
<p>感谢原作者<strong><a href="http://machinelearningmastery.com/author/jasonb/" target="_blank" rel="external">Jason Brownlee</a></strong>。原文链接见：<strong><a href="http://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/" target="_blank" rel="external">Attention in Long Short-Term Memory Recurrent Neural Networks</a></strong></p>
<p>同时，本译文稍作修改被刊登在公众号：<strong>AI科技大本营</strong>上，文章链接见<strong><a href="http://mp.weixin.qq.com/s/0SWcAAiuN3BYtStDZXyAXg" target="_blank" rel="external">一文读懂Attention：Facebook曾拿CNN秒杀谷歌，现如今谷歌拿它秒杀所有人</a></strong>。欢迎关注这个公众号（微信搜索<strong>rgznai100</strong>），上面有较多机器学习/深度学习相关的资源（尽管文章稍有些标题党=。=）。</p>
<hr>
<p><strong>文章写得不错？打赏一个呗:)</strong></p>
<p><img src="http://i.imgur.com/tTBAnzw.png" alt=""></p>
<p><strong>近期博主准备筹建NLP方面的技术群，欢迎感兴趣的小伙伴加我入群交流:)，加好友请备注：”博客”。</strong></p>
<p><img src="http://i.imgur.com/CTuJ7Ln.jpg" alt=""></p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Attention/" rel="tag">#Attention</a>
          
            <a href="/tags/Deep-Learning/" rel="tag">#Deep Learning</a>
          
            <a href="/tags/LSTM/" rel="tag">#LSTM</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
            <a href="/tags/RNN/" rel="tag">#RNN</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/deep-learning-for-chatbots-2.html" rel="next" title="聊天机器人中的深度学习技术之二：基于检索模型的实现">
                <i class="fa fa-chevron-left"></i> 聊天机器人中的深度学习技术之二：基于检索模型的实现
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/text-classification-in-action.html" rel="prev" title="文本分类实战系列（一）：特征工程">
                文本分类实战系列（一）：特征工程 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


        </div>

        


        
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      </div>
    
  </div>


      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/upload/image/avatar.png" alt="Jey Zhang" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Jey Zhang</p>
        </div>
        <p class="site-description motion-element" itemprop="description">Life is Now.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">8</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">50</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://www.facebook.com/jeyzhang" target="_blank">
                  
                    <i class="fa fa-globe"></i> facebook
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/jerrychang0402" target="_blank">
                  
                    <i class="fa fa-globe"></i> weibo
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/zhangjieup" target="_blank">
                  
                    <i class="fa fa-globe"></i> zhihu
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/jeyzhang" target="_blank">
                  
                    <i class="fa fa-globe"></i> github
                  
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
            <p class="site-author-name">Links</p>
            
              <span class="links-of-author-item">
                <a href="http://www.yunaitong.cn" target="_blank">Tong</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.wildml.com/" target="_blank">WILDML</a>
              </span>
            
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator">
            <i class="fa fa-angle-double-up"></i>
          </div>
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#导读"><span class="nav-number">1.</span> <span class="nav-text">导读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#长输入序列带来的问题"><span class="nav-number">2.</span> <span class="nav-text">长输入序列带来的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用attention机制"><span class="nav-number">3.</span> <span class="nav-text">使用attention机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#大型图片带来的问题"><span class="nav-number">4.</span> <span class="nav-text">大型图片带来的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于attention模型的应用实例"><span class="nav-number">5.</span> <span class="nav-text">基于attention模型的应用实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-_Attention在文本翻译任务上的应用"><span class="nav-number">5.1.</span> <span class="nav-text">1. Attention在文本翻译任务上的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-_Attention在图片描述上的应用"><span class="nav-number">5.2.</span> <span class="nav-text">2. Attention在图片描述上的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-_Attention在语义蕴涵_(Entailment)_中的应用"><span class="nav-number">5.3.</span> <span class="nav-text">3. Attention在语义蕴涵 (Entailment) 中的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-_Attention在语音识别上的应用"><span class="nav-number">5.4.</span> <span class="nav-text">4. Attention在语音识别上的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-_Attention在文本摘要上的应用"><span class="nav-number">5.5.</span> <span class="nav-text">5. Attention在文本摘要上的应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#进一步的阅读"><span class="nav-number">6.</span> <span class="nav-text">进一步的阅读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator">
            <i class="fa fa-angle-double-down"></i>
          </div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jey Zhang</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>

<span id="busuanzi_container_site_pv">
  &nbsp; | &nbsp;Total visited <span id="busuanzi_value_site_pv"></span> times.
</span>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
    
    

  

    <script type="text/javascript">
      var disqus_shortname = 'jeyzhang';
      var disqus_identifier = 'understand-attention-in-rnn.html';
      var disqus_title = '理解LSTM/RNN中的Attention机制';
      var disqus_url = 'http://www.jeyzhang.com/understand-attention-in-rnn.html';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  


  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    motionMiddleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');
      if (CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    };
  });
</script>



  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  

  
  
  
  
  	 <!-- custom analytics part create by xiamo -->
<script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
<script>AV.initialize("4KyzRuAgl6zwWXQK1UfxmIh0-gzGzoHsz", "YeXrCFBwTG1qRdqS2rUJ7XgJ");</script>
<script>
function showTime(Counter) {
	var query = new AV.Query(Counter);
	$(".leancloud_visitors").each(function() {
		var url = $(this).attr("id").trim();
		query.equalTo("url", url);
		query.find({
			success: function(results) {
				if (results.length == 0) {
					var content = '0 ' + $(document.getElementById(url)).text();
					$(document.getElementById(url)).text(content);
					return;
				}
				for (var i = 0; i < results.length; i++) {
					var object = results[i];
					var content = object.get('time') + ' ' + $(document.getElementById(url)).text();
					$(document.getElementById(url)).text(content);
				}
			},
			error: function(object, error) {
				console.log("Error: " + error.code + " " + error.message);
			}
		});

	});
}

function addCount(Counter) {
	var Counter = AV.Object.extend("Counter");
	url = $(".leancloud_visitors").attr('id').trim();
	title = $(".leancloud_visitors").attr('data-flag-title').trim();
	var query = new AV.Query(Counter);
	query.equalTo("url", url);
	query.find({
		success: function(results) {
			if (results.length > 0) {
				var counter = results[0];
				counter.fetchWhenSave(true);
				counter.increment("time");
				counter.save(null, {
					success: function(counter) {
						var content =  counter.get('time') + ' ' + $(document.getElementById(url)).text();
						$(document.getElementById(url)).text(content);
					},
					error: function(counter, error) {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
					}
				});
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
				newcounter.set("time", 1);
				newcounter.save(null, {
					success: function(newcounter) {
					    console.log("newcounter.get('time')="+newcounter.get('time'));
						var content = newcounter.get('time') + ' ' + $(document.getElementById(url)).text();
						$(document.getElementById(url)).text(content);
					},
					error: function(newcounter, error) {
						console.log('Failed to create');
					}
				});
			}
		},
		error: function(error) {
			console.log('Error:' + error.code + " " + error.message);
		}
	});
}
$(function() {
	var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else if ($('.post-title-link').length > 1) {
		showTime(Counter);
	}
}); 
</script>
  
  
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
